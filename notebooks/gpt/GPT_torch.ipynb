{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8079d2e-dfd2-4cd4-bc7c-9691fe354900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rusenttokenize\n",
    "# !pip install tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a071746-6010-412c-8e38-72553da0d780",
   "metadata": {},
   "source": [
    "В этом семинаре мы попробуем натренировать простую версию GPT. Код в очень большой части основан на вот этот туториале от Andrey Karpathy - https://www.youtube.com/watch?v=kCc8FmEb1nY . Я добавил более объемный датасет, BPE токенизацию, а также некоторые дополнительные пояснения, но основной код практически точно такой же, потому что сложно придумать что-то лучше.\n",
    "\n",
    "Помимо собственно разбора GPT модели, в этом семинаре также разбирается pytorch. \n",
    "Но прежде чем переходить к этому, давайте загрузим данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8079de80-5156-4252-86cb-f2ccd1af2fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "byT5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a048732b-470d-433b-ad3d-57505ad08075",
   "metadata": {},
   "source": [
    "## Данные и токенизация\n",
    "\n",
    "Для обучения GPT нужен текст. Чем больше, тем лучше. В туториале использовался просто корпус текстов Шекспира, но я решил взять чуть более реалистичный и объемный текст - новостные тексты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "730cc70e-2db8-4f7b-94e5-c787a0005fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rusenttokenize import ru_sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eed8939-d01a-456d-996c-9101ff9aef3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://github.com/mannefedov/compling_nlp_hse_course/raw/refs/heads/master/data/lenta_40k.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2df3b0e0-f454-4142-8640-81b78f47c142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Россия должна сотрудничать с Всемирным антидоп...</td>\n",
       "      <td>Спорт</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Уголовный суд Кувейта 28 июня освободил под за...</td>\n",
       "      <td>Мир</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Французский журнал Charlie Hebdo опубликовал н...</td>\n",
       "      <td>Интернет и СМИ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>В Петербурге в доме № 53 по улице Лени Голиков...</td>\n",
       "      <td>Россия</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>В московском аэропорту \"Домодедово\" задержан г...</td>\n",
       "      <td>Россия</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text           topic\n",
       "0  Россия должна сотрудничать с Всемирным антидоп...           Спорт\n",
       "1  Уголовный суд Кувейта 28 июня освободил под за...             Мир\n",
       "2  Французский журнал Charlie Hebdo опубликовал н...  Интернет и СМИ\n",
       "3  В Петербурге в доме № 53 по улице Лени Голиков...          Россия\n",
       "4  В московском аэропорту \"Домодедово\" задержан г...          Россия"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cca75f-26f8-4b2a-928e-dd5ecb0e3a51",
   "metadata": {},
   "source": [
    "Нам понадобятся только тексты. Также трансформеры очень чувствительны к длине текста, поэтому для простоты разделим все тексты на предложения и будем считать каждое предложение одним текстом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab05863b-2763-4f56-9d68-ecdf7817d673",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for text in data.text.values:\n",
    "    sentences.extend(ru_sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "869eb55d-6618-468e-8489-c71f9afa914e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "489727"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# почти 500 тыс предложений\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0921762d-81ff-467b-82b2-a8c993d6ac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cохраним в отдельный файл чтобы больше не тратить время на токенизацию,\n",
    "# также файл понадобится дальше для обучения токенизатора\n",
    "f = open('corpus.txt', 'w')\n",
    "for sent in sentences:\n",
    "    f.write(sent + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbe17b21-d744-41ee-9440-e45f88d16bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = open('corpus.txt').read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2af103e-460a-452d-9ae1-44e3f2402e73",
   "metadata": {},
   "source": [
    "Теперь нам нужно обучить токенизатор. В туториале Karpathy обучал GPT на символах, но обычно используется subword токенизация. Чтобы сделать эту модель чуть менее игрушечной, давайте добавим такую токенизацию. На саму модель это никак не повлияет, так как она в любом случае ожидает список индексов как input. С символьной токенизацией такие списки будут очень длинным, а subword токенизация группирует символы в нграммы (или даже целые слова) и таким образом длина последовательности сокращается.\n",
    "\n",
    "Мы не будем писать алгоритм токенизации с нуля, а воспользуемся готовым решением от huggingface - библиотекой tokenizers. Она написана на Rust и поэтому достаточно быстрая. \n",
    "Токенизация будет основана на алгоритме Byte-Pair-Encoding. В нем строки сначала кодируются как байты, которые представляют индекс символа в таблице Юникода. И в процессе обучения отдельные байты группируются по частотности в нграммы до тех пор, пока размер словаря не достигнет заданого лимита. \n",
    "По умолчанию это значение - 30 тысяч. Также в словаре также обычно добавляют какие-то специальные токены, которых нет в текстах, но которые будут добавляться, чтобы передать в модели какие-то дополнительные параметры. Мы добавим три токена - паддинг, токен начала текста, токена конца текста. Паддинг будет использоваться чтобы сравнять все тексты до одной длины при передаче в модель, но мы не хотим, чтобы эти токены как-то влияли на обучение, поэтому их нужно будет замаскировать. А BOS и EOS мы уже использовали в простых языковых моделях. Тут они нужны для тех же целей. В реальных моделях также в специальные токены входят токены разделяющие части промпта - системное сообщение, сообщение пользователя, сообщение модели и т.п. Про них мы поговорим в следующих семинарах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a21657f2-d4d9-4ffd-beba-d26fc240cd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers import decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f31d7555-fc2c-4d78-8e1b-0740f5ae6d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE()) \n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(special_tokens=[\"[PAD]\", \"[BOS]\", \"[EOS]\"], end_of_word_suffix='</w>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8c0b5bd5-6d8e-46cf-bdd0-1d6d4c6e274f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# токенайзер обучается на файле а не на питоновских списках\n",
    "tokenizer.train(files=[\"corpus.txt\"], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fdc103d1-12af-4cd2-8f83-08f448f4da15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраним токенизатор\n",
    "tokenizer.save('tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "55469115-3a7a-4f64-9261-d0b6e0b8fde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# при перезапуске можно просто перезагрузить готовый токенизатор\n",
    "# также он понадобится если мы решим сохранить модель\n",
    "tokenizer = Tokenizer.from_file(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "64a16f80-2a4c-4e1e-a792-9e8638e1103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.BPEDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7a8772a7-3ce8-4580-b7fe-7fd0ab00a3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "579e5401-dd0c-43a8-a07a-d549fd19546e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684b73b1-d06e-4e80-9084-0ce9c578dcaa",
   "metadata": {},
   "source": [
    "Посмотрим что получается в результате токенизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8928e36f-6212-4b9d-9538-e30e14fe5dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[867, 995, 489, 720, 686, 3777, 1469, 808, 9357]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Какой-то напечатанный текст').ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e3abf3c0-1bdd-4b01-b324-846f6025ef0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Какой - то напечатанный текст'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.encode('Какой-то напечатанный текст').tokens\n",
    "tokenizer.decoder.decode(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce2943f-de0d-432f-8225-fc6e6aab622b",
   "metadata": {},
   "source": [
    "Напишем еще функцию которая будет подставлять BOS и EOS токены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "222326c4-25e9-4326-a968-971830028a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, tokenizer):\n",
    "    return [tokenizer.token_to_id('[BOS]')] + tokenizer.encode(text).ids + [tokenizer.token_to_id('[EOS]')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624a4f46-8c5b-432d-9ea3-52b4707cd56c",
   "metadata": {},
   "source": [
    "Индекс паддинг токена пригодится позже для маскинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fa00ab64-24fa-4b8d-9fa6-e465c01377ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = tokenizer.token_to_id('[PAD]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c0adae-2bea-482e-82f1-f00af092c01c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebffabc4-1301-41a9-a79a-f56756aec49e",
   "metadata": {},
   "source": [
    "Теперь токенизируем все предложения и создадим датасет, который будет передавать в модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "210f8c8e-75aa-4e31-8e36-cc2d1099518c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ce4b703a-2229-451d-b5eb-6b236d17addd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, sentences, tokenizer, max_len=32):\n",
    "        # каждое предложение преобразуется в последовательность индексов \n",
    "        # а списки преобразуются в тензоры\n",
    "        self.encoded_texts = [torch.LongTensor(encode(sent, tokenizer)[-max_len:]) for sent in sentences]\n",
    "        # чтобы составить один общий обучающий тензор нужно сравнять длины последовательностей отдельных текстов\n",
    "        # в торче не такая удобная функция паддинга, поэтому транкация (отрезание лишнего) происходит уже выше\n",
    "        self.X = torch.nn.utils.rnn.pad_sequence(self.encoded_texts, padding_value=PAD_IDX, batch_first=True)\n",
    "        self.length = len(self.encoded_texts)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # обучающий пример для GPT составляется из одного текста\n",
    "        # x - это все токены кроме последнего\n",
    "        # y - это все токены кроме первого\n",
    "        # другими словами, y это x со сдвигом вправо\n",
    "        # каждый отдельный элемент в y - следующий токен для соответствующего элемента в x\n",
    "        # tokens = [1,2,3,4,5,0]\n",
    "        # x = [1,2,3,4,0]\n",
    "        # y = [2,3,4,5,0]\n",
    "\n",
    "        # 1 -> 2\n",
    "        # 1,2 -> 3\n",
    "        # 1,2,3 -> 4 \n",
    "        # 1,2,3,4 -> 5\n",
    "        # teacher forcing \n",
    "        \n",
    "        x = self.X[index][:-1]\n",
    "        y = self.X[index][1:]\n",
    "        \n",
    "        # чтобы не учитывать паддинг нам нужно создать маску\n",
    "        mask = x!=PAD_IDX\n",
    "\n",
    "        return x, y, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2018b73c-f17a-48ba-a56f-8a5fc363fc29",
   "metadata": {},
   "source": [
    "Разделим данные на обучающие и валидационные (90% и 10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "52547eac-519a-498a-84ae-3f97dd5e0a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(sentences)) # first 90% will be train, rest val\n",
    "sentences_train = sentences[:n]\n",
    "sentences_val = sentences[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6904ab1e-44b5-44f7-b073-dc53227573e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "622c6e30-6c24-4ee8-abad-b381e7958426",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = Dataset(sentences_train, tokenizer, MAX_LEN)\n",
    "val_set = Dataset(sentences_val, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d924ff1b-039e-4a21-9183-a11dedd8390d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    1,  1911,  2939, 14713,   400,  5442,   833, 13328, 26169, 15291,\n",
       "           541, 16131,  1126,  1389,  1102, 20933,  2983,  5198, 12060,   691,\n",
       "          9143, 10699,  2376,   829,   398, 11044,   489,   893,   489, 13170,\n",
       "           477,     2,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0]),\n",
       " tensor([ 1911,  2939, 14713,   400,  5442,   833, 13328, 26169, 15291,   541,\n",
       "         16131,  1126,  1389,  1102, 20933,  2983,  5198, 12060,   691,  9143,\n",
       "         10699,  2376,   829,   398, 11044,   489,   893,   489, 13170,   477,\n",
       "             2,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0]),\n",
       " tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "96703f1e-6ab4-4b57-8084-8ce50668a8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = torch.utils.data.DataLoader(training_set, batch_size=200, shuffle=True, )\n",
    "val_generator = torch.utils.data.DataLoader(training_set, batch_size=200, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b693c-e415-417a-b1ec-22810d3198ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35cb8123-b7a8-4146-8168-83e05ad73d63",
   "metadata": {},
   "source": [
    "Теперь можно переходить к обучению. Сначала давайте посмотрим на отдельные инструменты pytorch, которые будут использовать в моделях\n",
    "\n",
    "#### torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6b9c361-5816-4c80-af90-9298b891a0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# в коде часто встречается вот такой импорт\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd441795-65cd-423c-ab65-facd0b2f8c8a",
   "metadata": {},
   "source": [
    "Торч во многом похож на numpy и в нем есть те же стандартные функции "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce9899af-9cc2-47fc-a2db-b483406ac79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создает тензор заданного размера и заполняет его нулями\n",
    "torch.zeros((2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dafd7975-6707-4bf2-a009-2e7579d60a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.],\n",
       "         [1.],\n",
       "         [1.]],\n",
       "\n",
       "        [[1.],\n",
       "         [1.],\n",
       "         [1.]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создает тензор заданного размера и заполняет его единицами\n",
    "torch.ones((2, 3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bea528b0-de82-4d89-bc91-c524eb4d4767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8246, 0.7336]],\n",
       "\n",
       "        [[0.0444, 0.0924]],\n",
       "\n",
       "        [[0.5371, 0.2244]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создает тензор заданного размера и заполняет его случайными числами 0-1\n",
    "torch.rand((3, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "28ac28d6-0dcb-4a57-a89a-5fe6a2a01bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 2])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# размерность можно получить также как в numpy\n",
    "t = torch.rand((3, 1, 2))\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "27221fc1-3616-462b-8391-cb464cc5fff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4824, 0.6259, 0.3302, 0.5482, 0.6099, 0.4443])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# чтобы изменить размерность можно использовать .view\n",
    "t.view(3*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17ff6995-5c89-4ee4-95c8-5037bf9d8d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8901, 0.6534, 0.3813, 0.0028, 0.2018, 0.4946])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.view(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b824238b-0f60-4ebc-9bc4-7c4182e282e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[3, 4]' is invalid for input of size 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# изменение размерности не добавляет и не удаляет данные из изначального тензора\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# поэтому размеры должны совпадать\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[3, 4]' is invalid for input of size 6"
     ]
    }
   ],
   "source": [
    "# изменение размерности не добавляет и не удаляет данные из изначального тензора\n",
    "# поэтому размеры должны совпадать\n",
    "t.view(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "50381f52-3b1a-4810-8d99-43385f4d8dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 1])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# если размерности нужно поменять местами то для этого используется transpose\n",
    "# размерность dim1 меняется местами с размерностью dim2\n",
    "t.transpose(-2, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f0569260-a9d9-430b-a496-124608787fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# как и в numpy у тензоров есть специальный атрибут .T который возвращает транспонированный тензор\n",
    "# но при обучении моделей мы обычно работает с тензорами, где первая размерность это batch_size и ее нужно оставить на своей месте\n",
    "# поэтому используется transpose c указанием конкретных размерностей а batch размерность остается на своем месте\n",
    "t.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128f3ae5-b058-4663-80ba-1c03a00a06b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "880bb83f-bf67-419f-b20f-a6945fef6007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 3]), torch.Size([4, 3, 3]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# с помощью транспонирования можно рассчитать dot product (=близость) между\n",
    "# векторами в последовательности в батче\n",
    "# t размерности (4,3,2) и напрямую умножить его на себя нельзя потому что размерности 1,2 и 1,2 не подходят (нужно чтобы внутрение сходились)\n",
    "# transpose(-2, -1) или transpose(2, 1) что то же самое \n",
    "# позволят рассчитать dot product чем простое умножение @\n",
    "# оно тоже уже умеет работать с батчами поэтому первая разверность не изменится\n",
    "# в результате размерность будет 4,3,3\n",
    "# то есть для каждого примера в батче размером 4\n",
    "# между всеми элементами последовательности размером 3\n",
    "# будет рассчита dot product между отдельными векторами размерности 2\n",
    "# близость считается между парами в последовательности поэтому в результате для каждого будет 3 близости\n",
    "t = torch.rand((4, 3, 2))\n",
    "(t @ t.transpose(-2, -1)).shape, (t @ t.transpose(2, 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2755d7dc-31c7-4420-b818-9fecdbae1605",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec448a4b-a210-4d86-b36e-23050c666aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.8008, 0.9708]],\n",
       " \n",
       "         [[0.1924, 0.2296]],\n",
       " \n",
       "         [[0.1065, 0.2852]],\n",
       " \n",
       "         [[0.8307, 0.0645]]]),\n",
       " tensor([[0.8008, 0.9708],\n",
       "         [0.1924, 0.2296],\n",
       "         [0.1065, 0.2852],\n",
       "         [0.8307, 0.0645]]),\n",
       " torch.Size([4, 2]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# если одна из размерностей единичная то ее можно схлопнуть \n",
    "t = torch.rand((4, 1, 2))\n",
    "t, t.squeeze(1), t.squeeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fefe5056-dca7-4a0e-92ae-6425669bf893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.8008, 0.9708]],\n",
       " \n",
       "         [[0.1924, 0.2296]],\n",
       " \n",
       "         [[0.1065, 0.2852]],\n",
       " \n",
       "         [[0.8307, 0.0645]]]),\n",
       " tensor([[[[0.8008],\n",
       "           [0.9708]]],\n",
       " \n",
       " \n",
       "         [[[0.1924],\n",
       "           [0.2296]]],\n",
       " \n",
       " \n",
       "         [[[0.1065],\n",
       "           [0.2852]]],\n",
       " \n",
       " \n",
       "         [[[0.8307],\n",
       "           [0.0645]]]]),\n",
       " torch.Size([4, 1, 2, 1]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# также можно добавить единичную размерность \n",
    "t, t.unsqueeze(3), t.unsqueeze(3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518de6e9-6b93-4bbd-8640-6681d38e9c57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6a9584d2-afc0-4553-a579-237254e6732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.rand((1,3,4))\n",
    "t2 = torch.rand((1,3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8018059c-d96d-4d76-8d45-62a9ab58ed8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 4])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cat конкатенирует тензоры по заданной размерности (по умолчанию первой - то есть батч размерности)\n",
    "torch.cat([t1, t2], 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e49c717-9d93-4cf5-93a0-87b0865daea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "750903ca-ba86-414e-be47-b427853c0d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 1., 1.]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# для создания масок для attention понадобится функция tril которая зануляет элементы выше диагонали \n",
    "torch.tril(torch.ones((2,3,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7da5d949-fc75-4c17-805a-6a448f4c0099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c помощью tril и функции masked_fill можно заменять значения в тензоре по диагональному паттерну"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9b40a33b-bf99-43e9-a1e0-6e5aa8fc0cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7635, 0.4642, 0.7521],\n",
       "         [0.4594, 0.7915, 0.5790],\n",
       "         [0.0320, 0.3731, 0.9466]],\n",
       "\n",
       "        [[0.6682, 0.3126, 0.8954],\n",
       "         [0.9256, 0.7902, 0.4135],\n",
       "         [0.1221, 0.3147, 0.1836]],\n",
       "\n",
       "        [[0.2292, 0.6574, 0.7839],\n",
       "         [0.2278, 0.7917, 0.1904],\n",
       "         [0.9878, 0.6356, 0.5606]],\n",
       "\n",
       "        [[0.7482, 0.4246, 0.8443],\n",
       "         [0.9940, 0.1193, 0.7985],\n",
       "         [0.0143, 0.4395, 0.3698]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.rand((4, 3, 3))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1129b831-ceee-4a8b-a6f5-411c25b138ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7635,   -inf,   -inf],\n",
       "         [0.4594, 0.7915,   -inf],\n",
       "         [0.0320, 0.3731, 0.9466]],\n",
       "\n",
       "        [[0.6682,   -inf,   -inf],\n",
       "         [0.9256, 0.7902,   -inf],\n",
       "         [0.1221, 0.3147, 0.1836]],\n",
       "\n",
       "        [[0.2292,   -inf,   -inf],\n",
       "         [0.2278, 0.7917,   -inf],\n",
       "         [0.9878, 0.6356, 0.5606]],\n",
       "\n",
       "        [[0.7482,   -inf,   -inf],\n",
       "         [0.9940, 0.1193,   -inf],\n",
       "         [0.0143, 0.4395, 0.3698]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# в тезоре t значения которые соответствуют нулю в треугольной маске заменяются на минус бесконечность\n",
    "t.masked_fill(torch.tril(torch.ones((3,3)))==0, float(\"-inf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f99660e6-5a50-46a5-9a95-3ec1a7cb81b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# с помощью такой операции в GPT реализован механизм внимания где каждый токен общается только с токенами до него"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ad05e6-dcd5-446d-b5a9-2aa5eeb5956d",
   "metadata": {},
   "source": [
    "### deep learning layers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9d530e09-718a-4d9d-b0af-28f8c995f1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding слой сопоставляет вектор индексу\n",
    "# первый аргумент - размерность словаря\n",
    "# второй - размерность вектора\n",
    "embed = nn.Embedding(10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f79157b0-d0a9-4905-8638-f253072515b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# два текста по 4 токена в каждом\n",
    "t = torch.LongTensor([[1,3,4,5], [3,4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4c96ba82-b2b6-4ba8-a01c-5403f7c6a07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "714ae7e7-54cb-4e95-8d00-445f1151e4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 20])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# в результате каждому токену сопоставляется вектор 20\n",
    "embed(t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71d51f1-c181-4e56-8b00-a7715e891436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "31d51b4b-bf34-4a6c-9560-ef639ed707bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# полносвязный слой или линейной преобразование\n",
    "# первый аргумент изначальная размерность\n",
    "# второй - выходная размерность\n",
    "linear = nn.Linear(2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "eb8055bf-af97-44ca-931e-a8f3375ec0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# изначально у нас есть batch с двумя примерами где каждый состоит из трех токенов и у каждого токена вектор 2\n",
    "t = torch.rand((2, 3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e3ef39b9-81e3-47e5-abb5-016c2154d695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# применив полносвязный слой мы получим то же самое только размерность векторов теперь 10\n",
    "linear(t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "66a7f391-a159-430f-a500-9649f8769144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция активации которая зануляет отрицательные значения\n",
    "relu = nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c8e2c0f9-75d1-4532-9eb4-eece1b479c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.5116, -0.5753,  0.1465,  0.1994,  0.5395, -0.0600, -0.0829,\n",
       "            0.0278, -0.5628, -0.5003],\n",
       "          [-0.5552, -1.0051,  0.1534,  0.2507,  0.2597, -0.3272, -0.2982,\n",
       "           -0.0311, -0.2502, -0.2841],\n",
       "          [-0.5205, -0.9500,  0.2155,  0.4156,  0.2760, -0.3582, -0.4242,\n",
       "            0.1563, -0.2666, -0.1669]],\n",
       " \n",
       "         [[-0.5201, -0.6877,  0.1545,  0.2296,  0.4644, -0.1363, -0.1543,\n",
       "            0.0301, -0.4787, -0.4296],\n",
       "          [-0.4676, -0.5273,  0.2303,  0.4240,  0.5444, -0.1179, -0.2653,\n",
       "            0.2761, -0.5658, -0.3297],\n",
       "          [-0.5000, -0.8396,  0.2337,  0.4569,  0.3417, -0.3102, -0.4177,\n",
       "            0.2286, -0.3394, -0.1764]]], grad_fn=<ViewBackward0>),\n",
       " tensor([[[0.0000, 0.0000, 0.1465, 0.1994, 0.5395, 0.0000, 0.0000, 0.0278,\n",
       "           0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.1534, 0.2507, 0.2597, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.2155, 0.4156, 0.2760, 0.0000, 0.0000, 0.1563,\n",
       "           0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.1545, 0.2296, 0.4644, 0.0000, 0.0000, 0.0301,\n",
       "           0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.2303, 0.4240, 0.5444, 0.0000, 0.0000, 0.2761,\n",
       "           0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.2337, 0.4569, 0.3417, 0.0000, 0.0000, 0.2286,\n",
       "           0.0000, 0.0000]]], grad_fn=<ReluBackward0>))"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(t), relu(linear(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c88bfd5b-2ad3-4d3e-bf01-05c1d56c3a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.5116, -0.5753,  0.1465,  0.1994,  0.5395, -0.0600, -0.0829,\n",
       "            0.0278, -0.5628, -0.5003],\n",
       "          [-0.5552, -1.0051,  0.1534,  0.2507,  0.2597, -0.3272, -0.2982,\n",
       "           -0.0311, -0.2502, -0.2841],\n",
       "          [-0.5205, -0.9500,  0.2155,  0.4156,  0.2760, -0.3582, -0.4242,\n",
       "            0.1563, -0.2666, -0.1669]],\n",
       " \n",
       "         [[-0.5201, -0.6877,  0.1545,  0.2296,  0.4644, -0.1363, -0.1543,\n",
       "            0.0301, -0.4787, -0.4296],\n",
       "          [-0.4676, -0.5273,  0.2303,  0.4240,  0.5444, -0.1179, -0.2653,\n",
       "            0.2761, -0.5658, -0.3297],\n",
       "          [-0.5000, -0.8396,  0.2337,  0.4569,  0.3417, -0.3102, -0.4177,\n",
       "            0.2286, -0.3394, -0.1764]]], grad_fn=<ViewBackward0>),\n",
       " tensor([[[0.0643, 0.0603, 0.1242, 0.1309, 0.1840, 0.1010, 0.0987, 0.1103,\n",
       "           0.0611, 0.0650],\n",
       "          [0.0663, 0.0423, 0.1348, 0.1485, 0.1499, 0.0833, 0.0858, 0.1121,\n",
       "           0.0900, 0.0870],\n",
       "          [0.0647, 0.0421, 0.1350, 0.1649, 0.1434, 0.0761, 0.0712, 0.1272,\n",
       "           0.0834, 0.0921]],\n",
       " \n",
       "         [[0.0650, 0.0550, 0.1276, 0.1376, 0.1740, 0.0954, 0.0937, 0.1127,\n",
       "           0.0678, 0.0712],\n",
       "          [0.0627, 0.0591, 0.1260, 0.1530, 0.1726, 0.0890, 0.0768, 0.1320,\n",
       "           0.0569, 0.0720],\n",
       "          [0.0639, 0.0455, 0.1332, 0.1664, 0.1483, 0.0773, 0.0694, 0.1325,\n",
       "           0.0751, 0.0884]]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax - функция активации которая нормализует значения в векторе так что они лежат в интервале от 0-1 и суммируются в 1\n",
    "linear(t), F.softmax(linear(t), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4fc17f33-013f-4b62-b5b6-fc9a6ed1a75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LayerNorm нормализует значения в векторах так что среднее равно 0 а стандартное отклонение 1\n",
    "ln = nn.LayerNorm(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "38afd361-193b-447f-a8a7-7757398ef775",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = linear(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0d63588b-636f-4536-b5b5-25c9300bcc7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.1457, grad_fn=<MeanBackward0>),\n",
       " tensor(0.3879, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.mean(), out.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "2a9c9ead-afd7-4722-af4b-862765d01f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-7.9473e-09, grad_fn=<MeanBackward0>),\n",
       " tensor(1.0084, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln(out).mean(), ln(out).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ff506604-d2cd-4201-a0ce-414ae43c7891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.5116, -0.5753,  0.1465,  0.1994,  0.5395, -0.0600, -0.0829,\n",
       "            0.0278, -0.5628, -0.5003],\n",
       "          [-0.5552, -1.0051,  0.1534,  0.2507,  0.2597, -0.3272, -0.2982,\n",
       "           -0.0311, -0.2502, -0.2841],\n",
       "          [-0.5205, -0.9500,  0.2155,  0.4156,  0.2760, -0.3582, -0.4242,\n",
       "            0.1563, -0.2666, -0.1669]],\n",
       " \n",
       "         [[-0.5201, -0.6877,  0.1545,  0.2296,  0.4644, -0.1363, -0.1543,\n",
       "            0.0301, -0.4787, -0.4296],\n",
       "          [-0.4676, -0.5273,  0.2303,  0.4240,  0.5444, -0.1179, -0.2653,\n",
       "            0.2761, -0.5658, -0.3297],\n",
       "          [-0.5000, -0.8396,  0.2337,  0.4569,  0.3417, -0.3102, -0.4177,\n",
       "            0.2286, -0.3394, -0.1764]]], grad_fn=<ViewBackward0>),\n",
       " tensor([[[-1.0233, -1.1978,  0.7791,  0.9240,  1.8556,  0.2135,  0.1508,\n",
       "            0.4542, -1.1635, -0.9925],\n",
       "          [-0.9336, -2.1461,  0.9759,  1.2380,  1.2624, -0.3192, -0.2410,\n",
       "            0.4787, -0.1118, -0.2032],\n",
       "          [-0.8853, -1.9469,  0.9337,  1.4283,  1.0833, -0.4841, -0.6473,\n",
       "            0.7875, -0.2578, -0.0113]],\n",
       " \n",
       "         [[-1.0351, -1.5074,  0.8659,  1.0777,  1.7393,  0.0465, -0.0042,\n",
       "            0.5154, -0.9183, -0.7799],\n",
       "          [-0.9843, -1.1357,  0.7873,  1.2793,  1.5849, -0.0964, -0.4708,\n",
       "            0.9038, -1.2337, -0.6343],\n",
       "          [-0.9111, -1.7522,  0.9065,  1.4594,  1.1741, -0.4410, -0.7071,\n",
       "            0.8939, -0.5131, -0.1094]]], grad_fn=<NativeLayerNormBackward0>))"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, ln(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "83ffa21a-8b9e-4cf2-a698-c73a9511a6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout зануляет случайные значения в векторах\n",
    "# параметр задает вероятность зануления\n",
    "dropout = nn.Dropout(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "325e0f4f-46e5-4333-bc48-428126f608d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0232, -1.1506,  0.0000,  0.0000,  0.0000, -0.1201, -0.1659,\n",
       "           0.0000, -1.1255, -1.0007],\n",
       "         [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
       "          -0.0000, -0.0000, -0.0000],\n",
       "         [-1.0410, -0.0000,  0.4310,  0.8312,  0.5521, -0.7164, -0.0000,\n",
       "           0.0000, -0.5332, -0.3338]],\n",
       "\n",
       "        [[-1.0403, -1.3754,  0.3089,  0.0000,  0.9288, -0.0000, -0.0000,\n",
       "           0.0602, -0.0000, -0.8591],\n",
       "         [-0.0000, -0.0000,  0.0000,  0.8481,  0.0000, -0.0000, -0.0000,\n",
       "           0.5523, -1.1317, -0.0000],\n",
       "         [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.6205, -0.0000,\n",
       "           0.0000, -0.0000, -0.0000]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(linear(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "4737440d-0499-4612-997d-78eafaceac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential позволяет соединить несколько слоев в одно последовательное преобразование\n",
    "net = nn.Sequential(\n",
    "            nn.Linear(2, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 2),\n",
    "            nn.Dropout(0.5),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "46d58324-7417-4e2e-aa8b-e23f9cccee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = net(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "cb6dbc45-cec3-48c2-893c-5b09c12425df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f848c5b5-ece6-4b27-be37-47fb2faa2b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2389,  0.0000],\n",
       "         [-0.0000,  0.8493],\n",
       "         [-0.4012,  0.0000]],\n",
       "\n",
       "        [[-0.0000,  0.7760],\n",
       "         [-0.2761,  0.8136],\n",
       "         [-0.3664,  0.8802]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2ce406-6acf-48de-8b7e-4918c6d16722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16b10878-286a-4294-8546-54b822d3a66d",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e8f9ad-f7f5-41f3-acc9-234a2e509b23",
   "metadata": {},
   "source": [
    "Теперь давайте разберем код Karpath в котором он собирает GPT и обучим ее на наших данных.\n",
    "\n",
    "\n",
    "GPT это трансформерная модель. Она реализована также как это описано в статье Attention is all you need, за исключением того, что в GPT есть только decoder (правый столб). Статья AIAYN изначально про машинный перевод и поэтому в ней используется encoder-decoder архитектура. \n",
    "Декодером GPT делает то, что в ней используется causual attention, где каждый токен общается только с предыдущими. В encoder все токены взаимодействуют со всеми.\n",
    "Также небольшое отличие состоит в порядке применения layerNorm (сейчас его перенесли до MHA и до FF)\n",
    "\n",
    "\n",
    "\n",
    "![](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F0235fd2f-26f4-47ff-b95e-eddf6a4593b0_782x1152.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "67b7747d-1823-4baa-9960-7e3a3d5428c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "92250369-8e18-4fea-87bd-002441a67ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "block_size = MAX_LEN # what is the maximum context length for predictions?\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "n_embd = 64 # размерность эмбеддингов и векторов внутри трансформера\n",
    "#ffn_hid_dim = n_embd * 4\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "abdaaf64-9e4f-4b17-bd09-14e822b67546",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        if mask is not None:\n",
    "            wei.masked_fill(~mask.unsqueeze(1), float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        out = torch.cat([h(x, mask) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # print(inp)\n",
    "        x, mask = inp\n",
    "        x = x + self.sa(self.ln1(x), mask)\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return (x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c1bedbe1-9c19-420f-aa3b-a801ef3b9102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None, mask=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x, mask = self.blocks((x, mask)) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets, ignore_index=PAD_IDX)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, stop_token):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)# (B, 1)\n",
    "            if idx_next == stop_token:\n",
    "                break\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6d41a0d8-9c0a-406c-bcf3-75a45227c173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.073392 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "70cae839-8596-4316-9018-a9918cf95aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8d46ac9a-80b8-41fe-91ac-833010507844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910f9a29-e78a-4d22-9354-985c48748e1f",
   "metadata": {},
   "source": [
    "Код обучения просто передает в модель батчи из датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6d011245-1be5-4169-abff-f6b4690cb3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, print_every=10):\n",
    "    epoch_loss = []\n",
    "    model.train()  \n",
    "\n",
    "    for i, (xs, ys, mask) in enumerate(iterator):\n",
    "        optimizer.zero_grad()   \n",
    "        logits, loss = model(xs.to(device), ys.to(device), mask.to('cuda')) \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "        if not (i+1) % print_every:\n",
    "            print(f'Loss: {torch.Tensor(epoch_loss).mean(-1)}')\n",
    "        \n",
    "    return torch.Tensor(epoch_loss).mean(-1)\n",
    "\n",
    "def evaluate(model, iterator):\n",
    "    epoch_loss = []\n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        for xs, ys, mask in iterator:   \n",
    "            logits, loss = model(xs.to(device), ys.to(device), mask.to('cuda'))     \n",
    "            epoch_loss.append(loss.item())  \n",
    "            \n",
    "    return torch.Tensor(epoch_loss).mean(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b45064-e0a7-4c89-80a8-19cc86ed76b2",
   "metadata": {},
   "source": [
    "При обучении после каждой эпохи генерируется текст чтобы видеть прогресс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "adc2e1de-b36d-4a18-96a6-2f0f371995ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Loss: 8.584026336669922\n",
      "Loss: 8.17166519165039\n",
      "Loss: 7.972280502319336\n",
      "Loss: 7.828829288482666\n",
      "Loss: 7.711452007293701\n",
      "Loss: 7.608393669128418\n",
      "Loss: 7.514884948730469\n",
      "Loss: 7.4282732009887695\n",
      "Loss: 7.348236083984375\n",
      "Loss: 7.2724809646606445\n",
      "Loss: 7.200066089630127\n",
      "Loss: 7.132760524749756\n",
      "Loss: 7.069334506988525\n",
      "Loss: 7.009912967681885\n",
      "Loss: 6.954019546508789\n",
      "Loss: 6.90191650390625\n",
      "Loss: 6.8515167236328125\n",
      "Loss: 6.804726600646973\n",
      "Loss: 6.7599310874938965\n",
      "Loss: 6.717349052429199\n",
      "Loss: 6.676753997802734\n",
      "Loss: 6.63861608505249\n",
      "Eval -  5.76952600479126\n",
      "По ее словам , Шинязации Кураничекой была совершена четным главой отдела пока не исключил\n",
      "Возможный банк Гордон о том , что и его семья никогда позже разлизов для него Вера Маллер Невведения двусторонинженер Сахалинка\n",
      "Экупт вслед к власти на границе к нормаотебиенской , рассмотрит на утверждение , арестовала от все утверщения в галерее \" 5 \", передает ТАСС\n",
      "1\n",
      "Loss: 5.738876819610596\n",
      "Loss: 5.72067928314209\n",
      "Loss: 5.708089828491211\n",
      "Loss: 5.695284366607666\n",
      "Loss: 5.681899070739746\n",
      "Loss: 5.668886184692383\n",
      "Loss: 5.654829978942871\n",
      "Loss: 5.642187595367432\n",
      "Loss: 5.630497455596924\n",
      "Loss: 5.619437217712402\n",
      "Loss: 5.6077656745910645\n",
      "Loss: 5.5960798263549805\n",
      "Loss: 5.585582256317139\n",
      "Loss: 5.574732780456543\n",
      "Loss: 5.563994884490967\n",
      "Loss: 5.553378105163574\n",
      "Loss: 5.543426990509033\n",
      "Loss: 5.533946514129639\n",
      "Loss: 5.524518966674805\n",
      "Loss: 5.514758586883545\n",
      "Loss: 5.505795955657959\n",
      "Loss: 5.497215747833252\n",
      "Eval -  5.242829322814941\n",
      "\" Я готов лишь ее , поэтому африканская только поэтому прогнозировали такие меры\n",
      "Статья будет расследовать циквую опасность соответствующего ангума со стороны Нававался и провольтах в свете , которые сышедшего на замену их цену на вооружение и профессионалиссивно\n",
      "Об этом сообщает Postiminect\n",
      "2\n",
      "Loss: 5.247035980224609\n",
      "Loss: 5.23923397064209\n",
      "Loss: 5.237048149108887\n",
      "Loss: 5.233165740966797\n",
      "Loss: 5.228250980377197\n",
      "Loss: 5.2233734130859375\n",
      "Loss: 5.2171831130981445\n",
      "Loss: 5.214299201965332\n",
      "Loss: 5.2096028327941895\n",
      "Loss: 5.206192970275879\n",
      "Loss: 5.202394485473633\n",
      "Loss: 5.199341773986816\n",
      "Loss: 5.195230007171631\n",
      "Loss: 5.191343307495117\n",
      "Loss: 5.187573432922363\n",
      "Loss: 5.183582305908203\n",
      "Loss: 5.180008411407471\n",
      "Loss: 5.176240921020508\n",
      "Loss: 5.172390460968018\n",
      "Loss: 5.168587684631348\n",
      "Loss: 5.16480016708374\n",
      "Loss: 5.1618218421936035\n",
      "Eval -  5.013362884521484\n",
      "Как отмечается , он основал к бывшему берегам и написал Dead Fee своей певицы спасла\n",
      "Дочь пользователи из дома\n",
      "На данный момент подпрезидентский бюджет , сделано в октябре 2009 года 14 марта , сделал публикация расторгнуть запрет на воздушное проподготовку ежегодными\n",
      "3\n",
      "Loss: 5.018315315246582\n",
      "Loss: 5.014183521270752\n",
      "Loss: 5.012704849243164\n",
      "Loss: 5.0127458572387695\n",
      "Loss: 5.011291027069092\n",
      "Loss: 5.0096588134765625\n",
      "Loss: 5.009410858154297\n",
      "Loss: 5.009127140045166\n",
      "Loss: 5.00661563873291\n",
      "Loss: 5.005575180053711\n",
      "Loss: 5.0039286613464355\n",
      "Loss: 5.0022759437561035\n",
      "Loss: 5.000068187713623\n",
      "Loss: 4.998203754425049\n",
      "Loss: 4.996253490447998\n",
      "Loss: 4.994476795196533\n",
      "Loss: 4.992412090301514\n",
      "Loss: 4.990657329559326\n",
      "Loss: 4.9887871742248535\n",
      "Loss: 4.986923694610596\n",
      "Loss: 4.985323905944824\n",
      "Loss: 4.983425140380859\n",
      "Eval -  4.871294021606445\n",
      "Соскаванное сквозь компанию Othskro уточняет , что сборы будет работать лишь в состоянии урагана в Японии ( Amzeenon\n",
      "Беннлисослав Вольфван , в результате сильного ими функции директоров Hallgentxh Hall riliner опроверг подписи долгов\n",
      "Известно , что лишь после этого заведения веденной бригады не закроются от земли имени Горького со предметом того , чтобы успеть разданное\n",
      "4\n",
      "Loss: 4.871158599853516\n",
      "Loss: 4.8762383460998535\n",
      "Loss: 4.875579833984375\n",
      "Loss: 4.876194000244141\n",
      "Loss: 4.878403186798096\n",
      "Loss: 4.881315231323242\n",
      "Loss: 4.881041049957275\n",
      "Loss: 4.879766941070557\n",
      "Loss: 4.879831790924072\n",
      "Loss: 4.880425453186035\n",
      "Loss: 4.880573272705078\n",
      "Loss: 4.879690170288086\n",
      "Loss: 4.879103183746338\n",
      "Loss: 4.878217697143555\n",
      "Loss: 4.877015590667725\n",
      "Loss: 4.876431941986084\n",
      "Loss: 4.875754356384277\n",
      "Loss: 4.874831199645996\n",
      "Loss: 4.873321533203125\n",
      "Loss: 4.872452259063721\n",
      "Loss: 4.871353626251221\n",
      "Loss: 4.870250701904297\n",
      "Eval -  4.782349586486816\n",
      "Торги по оценке крупнейшей в России металлургический контингент нередко стали выполнением борьбы с банками на два миллиона рублей у Google от претендентами политика\n",
      "В ведущих агентства входят AxOffg , в которую утверждается , что ежегодное влияние на ограниченном аренде пойдет инименный день спешно на коррологов мира по задателю ноутбуков НХЛ\n",
      "По мнению корвета Александра Лалла ( Мадрида , ее причиной кризиса в новом море предлагалось повысятся для \" Русское \" продуктов уже достигла минимальных услуг\n",
      "5\n",
      "Loss: 4.7729644775390625\n",
      "Loss: 4.773868560791016\n",
      "Loss: 4.779287338256836\n",
      "Loss: 4.783237457275391\n",
      "Loss: 4.786911964416504\n",
      "Loss: 4.788356781005859\n",
      "Loss: 4.7904229164123535\n",
      "Loss: 4.790971755981445\n",
      "Loss: 4.792738914489746\n",
      "Loss: 4.793971061706543\n",
      "Loss: 4.79409122467041\n",
      "Loss: 4.793756484985352\n",
      "Loss: 4.7943315505981445\n",
      "Loss: 4.793623924255371\n",
      "Loss: 4.793181419372559\n",
      "Loss: 4.793118000030518\n",
      "Loss: 4.792612552642822\n",
      "Loss: 4.791801929473877\n",
      "Loss: 4.791369915008545\n",
      "Loss: 4.791112899780273\n",
      "Loss: 4.7909255027771\n",
      "Loss: 4.790688991546631\n",
      "Eval -  4.7070536613464355\n",
      "Следующий фильм « 18 » Кафрул ( Euroecthehwood\n",
      "Таким образом у \" ОГ \" было более известная испанец , а неподвигу Диее управления\n",
      "22 - летний Укана разрушил другим странам Швейцарии ' s Creed\n",
      "6\n",
      "Loss: 4.714417934417725\n",
      "Loss: 4.724402904510498\n",
      "Loss: 4.720452308654785\n",
      "Loss: 4.720766544342041\n",
      "Loss: 4.721383094787598\n",
      "Loss: 4.723095893859863\n",
      "Loss: 4.724578857421875\n",
      "Loss: 4.725257396697998\n",
      "Loss: 4.726857662200928\n",
      "Loss: 4.727640628814697\n",
      "Loss: 4.7278733253479\n",
      "Loss: 4.728398323059082\n",
      "Loss: 4.727777481079102\n",
      "Loss: 4.728553295135498\n",
      "Loss: 4.729013442993164\n",
      "Loss: 4.728877067565918\n",
      "Loss: 4.729511737823486\n",
      "Loss: 4.730015754699707\n",
      "Loss: 4.729759693145752\n",
      "Loss: 4.72991418838501\n",
      "Loss: 4.729926109313965\n",
      "Loss: 4.729884147644043\n",
      "Eval -  4.648861408233643\n",
      "Правоохранительные органы по обвинению в организации происшествия пока неизвестны\n",
      "Исполнятую медаль ведет переговоры со странами , представляющих спортивную девятую карточку перед немецким \" Будапешт\n",
      "Как сообщается на сайте газеты , речь идет о своем переносе построек ( проход прессы , время Ли + Lenpoven\n",
      "7\n",
      "Loss: 4.6532793045043945\n",
      "Loss: 4.660558700561523\n",
      "Loss: 4.6632399559021\n",
      "Loss: 4.666407108306885\n",
      "Loss: 4.667403697967529\n",
      "Loss: 4.6696295738220215\n",
      "Loss: 4.672366142272949\n",
      "Loss: 4.674787998199463\n",
      "Loss: 4.675729274749756\n",
      "Loss: 4.6773457527160645\n",
      "Loss: 4.678022384643555\n",
      "Loss: 4.67794942855835\n",
      "Loss: 4.678309917449951\n",
      "Loss: 4.679426670074463\n",
      "Loss: 4.679941177368164\n",
      "Loss: 4.680288314819336\n",
      "Loss: 4.680420875549316\n",
      "Loss: 4.680300235748291\n",
      "Loss: 4.680474281311035\n",
      "Loss: 4.680851936340332\n",
      "Loss: 4.680631160736084\n",
      "Loss: 4.680520057678223\n",
      "Eval -  4.607820987701416\n",
      "Встреча действующего чемпиона IBF за матч против « Челси » с 1 декабря\n",
      "Правоохранительные органы , подозреваемый Поклонская , полностью освободили на Красной площади на месте , скончался на месте нападения на Автоинспекции\n",
      "Последней Samsung \" Серинт \" - чистая прибыль - \" страна Easte ' s и старается плохим и глукится уровень возможности для всех видов товаров через основном\n",
      "8\n",
      "Loss: 4.610116958618164\n",
      "Loss: 4.616552829742432\n",
      "Loss: 4.61916971206665\n",
      "Loss: 4.622060775756836\n",
      "Loss: 4.625572204589844\n",
      "Loss: 4.628335952758789\n",
      "Loss: 4.629365921020508\n",
      "Loss: 4.630478382110596\n",
      "Loss: 4.630887508392334\n",
      "Loss: 4.631759166717529\n",
      "Loss: 4.632584095001221\n",
      "Loss: 4.63399076461792\n",
      "Loss: 4.634325981140137\n",
      "Loss: 4.635126113891602\n",
      "Loss: 4.635700225830078\n",
      "Loss: 4.636230945587158\n",
      "Loss: 4.637105464935303\n",
      "Loss: 4.637351989746094\n",
      "Loss: 4.6378679275512695\n",
      "Loss: 4.6385087966918945\n",
      "Loss: 4.6390910148620605\n",
      "Loss: 4.639528751373291\n",
      "Eval -  4.570179462432861\n",
      "Оставшиеся 5 лет также были расположены там сотрудников ЦРУ\n",
      "Об этом говорится в исследовании консалтинговой компании Media Group в Гонконге\n",
      "Акционерный генерал Джон Кирби Вивиашвили также сыграл Сергея Ельцин , перед ураном « Марком революции и кратными ошибок подошли к нему » и провели оссбросы\n",
      "9\n",
      "Loss: 4.582310676574707\n",
      "Loss: 4.582547664642334\n",
      "Loss: 4.5856428146362305\n",
      "Loss: 4.584713935852051\n",
      "Loss: 4.58817720413208\n",
      "Loss: 4.588860034942627\n",
      "Loss: 4.590743064880371\n",
      "Loss: 4.591935157775879\n",
      "Loss: 4.593635082244873\n",
      "Loss: 4.596286296844482\n",
      "Loss: 4.597182273864746\n",
      "Loss: 4.598222732543945\n",
      "Loss: 4.598869323730469\n",
      "Loss: 4.599891662597656\n",
      "Loss: 4.601235866546631\n",
      "Loss: 4.601823806762695\n",
      "Loss: 4.6032843589782715\n",
      "Loss: 4.603695869445801\n",
      "Loss: 4.603838920593262\n",
      "Loss: 4.603752613067627\n",
      "Loss: 4.603706359863281\n",
      "Loss: 4.6043171882629395\n",
      "Eval -  4.53765344619751\n",
      "Напомним , что ранее начались действия на территории Цхинвали 10 - километрория в регионе\n",
      "Прямую трансляцию матча команды Вратари клуба сыграл со счетом 1 : 2\n",
      "\" Мы не считаем никакого дисциплины , и назвать - от начальника серию членов ЦИК осволомного монастыря\n",
      "10\n",
      "Loss: 4.538386821746826\n",
      "Loss: 4.552356719970703\n",
      "Loss: 4.552554607391357\n",
      "Loss: 4.55417013168335\n",
      "Loss: 4.554865837097168\n",
      "Loss: 4.5577712059021\n",
      "Loss: 4.560391426086426\n",
      "Loss: 4.562150478363037\n",
      "Loss: 4.563690185546875\n",
      "Loss: 4.564205646514893\n",
      "Loss: 4.564847946166992\n",
      "Loss: 4.5654377937316895\n",
      "Loss: 4.567165374755859\n",
      "Loss: 4.567822456359863\n",
      "Loss: 4.568913459777832\n",
      "Loss: 4.56959342956543\n",
      "Loss: 4.569753646850586\n",
      "Loss: 4.571109294891357\n",
      "Loss: 4.571706295013428\n",
      "Loss: 4.572110176086426\n",
      "Loss: 4.572749137878418\n",
      "Loss: 4.5734968185424805\n",
      "Eval -  4.508671760559082\n",
      "Оставшиеся несколько целей уличного сообщества считаются разведакции в совершении отступления в Москве\n",
      "В своем суперкурортном пропоказаний его агергал радикальное приложение мобильных машин , двигавшееся количество целей и двигателя , повлиявших в чисталее угля в результате авиакатастрофы , никаких довольно масштабной гибелью дорожно - транспортных предприятий , эффективность этих многоцелевых огневок\n",
      "Его сделано в отношении осужденного\n",
      "11\n",
      "Loss: 4.511892795562744\n",
      "Loss: 4.514524459838867\n",
      "Loss: 4.518864154815674\n",
      "Loss: 4.5228424072265625\n",
      "Loss: 4.524482250213623\n",
      "Loss: 4.5265421867370605\n",
      "Loss: 4.528122425079346\n",
      "Loss: 4.530412673950195\n",
      "Loss: 4.533614158630371\n",
      "Loss: 4.534255504608154\n",
      "Loss: 4.5357465744018555\n",
      "Loss: 4.537935256958008\n",
      "Loss: 4.538250923156738\n",
      "Loss: 4.539211273193359\n",
      "Loss: 4.539322376251221\n",
      "Loss: 4.539398670196533\n",
      "Loss: 4.5408735275268555\n",
      "Loss: 4.542569637298584\n",
      "Loss: 4.543613433837891\n",
      "Loss: 4.544571399688721\n",
      "Loss: 4.5450239181518555\n",
      "Loss: 4.5453972816467285\n",
      "Eval -  4.473221302032471\n",
      "По мнению Бонда , перестановка приступил к истории Италии , снижение производства дальним доллара и чилийской зимой\n",
      "Огонь применяла коридор ногу со своей матерью в чемпионате Испании\n",
      "Тем не менее , ни к этому , смерть не пользуются ожидаемым , в чем у нас нет доказательств ни острым в одном из правоохранительных органах\n",
      "12\n",
      "Loss: 4.496516704559326\n",
      "Loss: 4.4918293952941895\n",
      "Loss: 4.4959797859191895\n",
      "Loss: 4.500423908233643\n",
      "Loss: 4.502549171447754\n",
      "Loss: 4.502803325653076\n",
      "Loss: 4.505421161651611\n",
      "Loss: 4.505794525146484\n",
      "Loss: 4.508107662200928\n",
      "Loss: 4.509782314300537\n",
      "Loss: 4.510804653167725\n",
      "Loss: 4.512543678283691\n",
      "Loss: 4.513936519622803\n",
      "Loss: 4.514830112457275\n",
      "Loss: 4.5155839920043945\n",
      "Loss: 4.516423225402832\n",
      "Loss: 4.517459869384766\n",
      "Loss: 4.517992973327637\n",
      "Loss: 4.518710136413574\n",
      "Loss: 4.518947601318359\n",
      "Loss: 4.519742965698242\n",
      "Loss: 4.520245552062988\n",
      "Eval -  4.459163188934326\n",
      "Обе стороны отказываются ставить на слушания динавских Генштаба , когда считал выполнение развалины тем , что а массовые акции заняли \" в пределах\n",
      "В МВД выступило \" ралль \" ЕС , обладателям кандидатуры члена Совета Федерации футбола с участием болельщиков \" Трома\n",
      "Россияне устраивают игры пени напитки , используя ситуацию , подготовить решение попросить кредита страны в Китай\n",
      "13\n",
      "Loss: 4.474480152130127\n",
      "Loss: 4.472332954406738\n",
      "Loss: 4.475277423858643\n",
      "Loss: 4.476153373718262\n",
      "Loss: 4.47629976272583\n",
      "Loss: 4.4794416427612305\n",
      "Loss: 4.4821977615356445\n",
      "Loss: 4.483991622924805\n",
      "Loss: 4.48640251159668\n",
      "Loss: 4.487923622131348\n",
      "Loss: 4.488492012023926\n",
      "Loss: 4.489560604095459\n",
      "Loss: 4.490434646606445\n",
      "Loss: 4.4910478591918945\n",
      "Loss: 4.491859436035156\n",
      "Loss: 4.4929046630859375\n",
      "Loss: 4.493962287902832\n",
      "Loss: 4.494595050811768\n",
      "Loss: 4.4955973625183105\n",
      "Loss: 4.495939254760742\n",
      "Loss: 4.496623516082764\n",
      "Loss: 4.497419834136963\n",
      "Eval -  4.43390417098999\n",
      "1 августа стало известно , что большинство нефтяных скважин был нанесен этими столкновениями\n",
      "Самолеты BP вызывают ракеты баллистической ракеты , увеличив примерно 12 - 40 лошадиных сил небольшой орбиты , сообщает Defoil\n",
      "Моя предлагают ей принимать лед ремни в придан совое промывать давами », — заявил он\n",
      "14\n",
      "Loss: 4.447494029998779\n",
      "Loss: 4.449568271636963\n",
      "Loss: 4.452133655548096\n",
      "Loss: 4.453064441680908\n",
      "Loss: 4.453578472137451\n",
      "Loss: 4.4562087059021\n",
      "Loss: 4.458090305328369\n",
      "Loss: 4.4608917236328125\n",
      "Loss: 4.462804794311523\n",
      "Loss: 4.464843273162842\n",
      "Loss: 4.466324806213379\n",
      "Loss: 4.467903137207031\n",
      "Loss: 4.469045639038086\n",
      "Loss: 4.470276355743408\n",
      "Loss: 4.471499919891357\n",
      "Loss: 4.4726080894470215\n",
      "Loss: 4.4732818603515625\n",
      "Loss: 4.474266529083252\n",
      "Loss: 4.4750657081604\n",
      "Loss: 4.4753007888793945\n",
      "Loss: 4.475955486297607\n",
      "Loss: 4.4763665199279785\n",
      "Eval -  4.410568714141846\n",
      "Депутаты Верховной Рады стали подавать слухи о теракт о запрете Сурждения из - за пересылающих беспорядки в полицию из - за независимости\n",
      "По его словам , в настоящее время Большой театр может ускорить развитие народных музыкального отрасли , например , проектирование Российской комитета\n",
      "Последний смог ввести в эксплуатацию одновременно технического препятствует для точки предоставления повышения эффективности связи\n",
      "15\n",
      "Loss: 4.422752380371094\n",
      "Loss: 4.424027919769287\n",
      "Loss: 4.426799297332764\n",
      "Loss: 4.428157806396484\n",
      "Loss: 4.4299235343933105\n",
      "Loss: 4.432135581970215\n",
      "Loss: 4.43656587600708\n",
      "Loss: 4.438058376312256\n",
      "Loss: 4.441082954406738\n",
      "Loss: 4.4424638748168945\n",
      "Loss: 4.443983554840088\n",
      "Loss: 4.445568084716797\n",
      "Loss: 4.446810722351074\n",
      "Loss: 4.448133945465088\n",
      "Loss: 4.4491424560546875\n",
      "Loss: 4.450716972351074\n",
      "Loss: 4.451870918273926\n",
      "Loss: 4.4526472091674805\n",
      "Loss: 4.453871726989746\n",
      "Loss: 4.4546284675598145\n",
      "Loss: 4.455817222595215\n",
      "Loss: 4.4570231437683105\n",
      "Eval -  4.393080711364746\n",
      "Первая спасательная академия традиционно состоит из 40 гостей , восемь других семиазков - четыре из них будут выяснены\n",
      "17 февраля премьер - министр Ирака Кунаа встретился с Пентагоном лидером Римфома с Казуп , недовольна отсутствие позиций власти Великобритании в отношении других лиц о защите от уплаты налогов\n",
      "Такое Ын принадлежит 760 - летним юбилеем США\n",
      "16\n",
      "Loss: 4.397495269775391\n",
      "Loss: 4.40087890625\n",
      "Loss: 4.403144359588623\n",
      "Loss: 4.41090726852417\n",
      "Loss: 4.414369583129883\n",
      "Loss: 4.416013717651367\n",
      "Loss: 4.419463157653809\n",
      "Loss: 4.422529220581055\n",
      "Loss: 4.425120830535889\n",
      "Loss: 4.426973819732666\n",
      "Loss: 4.4286322593688965\n",
      "Loss: 4.4300947189331055\n",
      "Loss: 4.430574893951416\n",
      "Loss: 4.43153715133667\n",
      "Loss: 4.432196617126465\n",
      "Loss: 4.432969093322754\n",
      "Loss: 4.434256553649902\n",
      "Loss: 4.43524694442749\n",
      "Loss: 4.436416149139404\n",
      "Loss: 4.437187194824219\n",
      "Loss: 4.438139915466309\n",
      "Loss: 4.438711166381836\n",
      "Eval -  4.374775409698486\n",
      "Но будем искать прекрасные меры с тем , что это он должен будет попытаться вести модель в любой вагоны , которые оставались даже тактические условия\n",
      "За прошедшие годы Слаутбанк - VII планируется ввести без потребителей кредитов\n",
      "По словам главного тренера \" Югеев \" 3 октября , благодаря требованиям Российского народной республики от Партии регионов будет выступать в \" Барселону \", отмечает издание\n",
      "17\n",
      "Loss: 4.378722667694092\n",
      "Loss: 4.383865833282471\n",
      "Loss: 4.388291358947754\n",
      "Loss: 4.39184045791626\n",
      "Loss: 4.394711017608643\n",
      "Loss: 4.3959856033325195\n",
      "Loss: 4.398737907409668\n",
      "Loss: 4.400832653045654\n",
      "Loss: 4.40343713760376\n",
      "Loss: 4.405550003051758\n",
      "Loss: 4.409034729003906\n",
      "Loss: 4.410486221313477\n",
      "Loss: 4.411304950714111\n",
      "Loss: 4.41294527053833\n",
      "Loss: 4.414531230926514\n",
      "Loss: 4.416126728057861\n",
      "Loss: 4.4174909591674805\n",
      "Loss: 4.418600559234619\n",
      "Loss: 4.419752597808838\n",
      "Loss: 4.420265197753906\n",
      "Loss: 4.421126842498779\n",
      "Loss: 4.421935081481934\n",
      "Eval -  4.360169410705566\n",
      "По мнению журналистов , \" это недоциклона \", членами \" Трудовой России \" является Сирия , которые в ближайшее время готовы другие штаб - подрядчики\n",
      "Теннесси умер в мечети Райана погибшего в 1673 году Обама\n",
      "\" Невским правосудием \" выйдет на экраны в понедельник , 18 августа\n",
      "18\n",
      "Loss: 4.374274730682373\n",
      "Loss: 4.380298614501953\n",
      "Loss: 4.382314682006836\n",
      "Loss: 4.381725311279297\n",
      "Loss: 4.383814334869385\n",
      "Loss: 4.386659145355225\n",
      "Loss: 4.38731050491333\n",
      "Loss: 4.387697219848633\n",
      "Loss: 4.389782905578613\n",
      "Loss: 4.391528129577637\n",
      "Loss: 4.392929553985596\n",
      "Loss: 4.39442253112793\n",
      "Loss: 4.395987510681152\n",
      "Loss: 4.397294044494629\n",
      "Loss: 4.3991923332214355\n",
      "Loss: 4.4002203941345215\n",
      "Loss: 4.401948928833008\n",
      "Loss: 4.403065204620361\n",
      "Loss: 4.404310703277588\n",
      "Loss: 4.405048847198486\n",
      "Loss: 4.40582799911499\n",
      "Loss: 4.406628608703613\n",
      "Eval -  4.347639560699463\n",
      "В России зимой 2004 было совершено создание своего музея \" Оходтале \", прошенные на них , пытаясь увидеть драматический окно , в самом исламе - Роув\n",
      "Двое детей , разогнавших автомобили и нападавших , в аквапарк идет около одного из частных домов , сообщает агентство \" Интерфакс\n",
      "Врачи научились в минувшую пятницу таможенник Майн Джемида Красномироа\n",
      "19\n",
      "Loss: 4.347743988037109\n",
      "Loss: 4.350590705871582\n",
      "Loss: 4.356003284454346\n",
      "Loss: 4.359778881072998\n",
      "Loss: 4.365639686584473\n",
      "Loss: 4.368247032165527\n",
      "Loss: 4.370724201202393\n",
      "Loss: 4.372678756713867\n",
      "Loss: 4.374516010284424\n",
      "Loss: 4.375434875488281\n",
      "Loss: 4.377743721008301\n",
      "Loss: 4.379141807556152\n",
      "Loss: 4.381028652191162\n",
      "Loss: 4.38275146484375\n",
      "Loss: 4.384446144104004\n",
      "Loss: 4.3857574462890625\n",
      "Loss: 4.386566638946533\n",
      "Loss: 4.38751220703125\n",
      "Loss: 4.389057159423828\n",
      "Loss: 4.389961242675781\n",
      "Loss: 4.390963554382324\n",
      "Loss: 4.391908168792725\n",
      "Eval -  4.328693866729736\n",
      "Как сообщил руководителем \" Вобутюйска представитель американских спецслужб \", работой закончился со мая в тайном секторе , но речь пойдет о федеральные значительном сообществе в эфире телеканала FOX + Group\n",
      "Законопроект о том , связанные с « Мемудымией для граждан России » внесено в список кандидатов\n",
      "В пятницу в городе Трамп заявил о той же Абрамовичу , что следует дать себе не за этот харьйский Беслану 8 апреля\n",
      "20\n",
      "Loss: 4.340651512145996\n",
      "Loss: 4.344229221343994\n",
      "Loss: 4.347539901733398\n",
      "Loss: 4.351086616516113\n",
      "Loss: 4.353788375854492\n",
      "Loss: 4.354785919189453\n",
      "Loss: 4.357730865478516\n",
      "Loss: 4.3586955070495605\n",
      "Loss: 4.361661434173584\n",
      "Loss: 4.364029884338379\n",
      "Loss: 4.364944934844971\n",
      "Loss: 4.366695880889893\n",
      "Loss: 4.368417739868164\n",
      "Loss: 4.3695502281188965\n",
      "Loss: 4.370890140533447\n",
      "Loss: 4.372199535369873\n",
      "Loss: 4.3731608390808105\n",
      "Loss: 4.37416934967041\n",
      "Loss: 4.375282287597656\n",
      "Loss: 4.376267433166504\n",
      "Loss: 4.377264976501465\n",
      "Loss: 4.378252983093262\n",
      "Eval -  4.314706802368164\n",
      "Ранее сообщалось , что ФБР со концентрацией чиновников перешла на религиозные вопросы - якобы отослукам и воровцам мератионы за символические сроки\n",
      "Анасенко выразил интервью , что его подопечные пять лет рассчитывали со вознаграждениями возможных публикаций\n",
      "Космический самолет Ан - 7 разбился на борту торгового самолета Ту - 154 , перевозившей контейту горы Ради Ряба на железнодорожном автостоянку в Якутную Анталье , передает РИА Новости\n",
      "21\n",
      "Loss: 4.312203884124756\n",
      "Loss: 4.31856632232666\n",
      "Loss: 4.329218864440918\n",
      "Loss: 4.33450174331665\n",
      "Loss: 4.337906837463379\n",
      "Loss: 4.340435981750488\n",
      "Loss: 4.341947555541992\n",
      "Loss: 4.344295501708984\n",
      "Loss: 4.346793174743652\n",
      "Loss: 4.34852409362793\n",
      "Loss: 4.350546836853027\n",
      "Loss: 4.3521728515625\n",
      "Loss: 4.354031562805176\n",
      "Loss: 4.3554768562316895\n",
      "Loss: 4.356816291809082\n",
      "Loss: 4.3586344718933105\n",
      "Loss: 4.36012601852417\n",
      "Loss: 4.361169338226318\n",
      "Loss: 4.362344741821289\n",
      "Loss: 4.363222599029541\n",
      "Loss: 4.3642144203186035\n",
      "Loss: 4.365264892578125\n",
      "Eval -  4.306529998779297\n",
      "Как сообщает официальный сайт УЕФА , кандидат в Госдуму , по словам самого Тихого , Константин Гронов , в соответствии с законом четвертого июня 2010 года — ведь он задал , что наконец было не стушивалось\n",
      "После этого на данный момент на слушаниях был выписан в местный контроль в пять - отелях протеста\n",
      "Ранее Беловежский журналист отметил , что рассчитывая , что у участников митинга организованного\n",
      "22\n",
      "Loss: 4.304548263549805\n",
      "Loss: 4.3137617111206055\n",
      "Loss: 4.317085266113281\n",
      "Loss: 4.321781635284424\n",
      "Loss: 4.326108932495117\n",
      "Loss: 4.331181049346924\n",
      "Loss: 4.333705425262451\n",
      "Loss: 4.335799217224121\n",
      "Loss: 4.3380961418151855\n",
      "Loss: 4.33960485458374\n",
      "Loss: 4.340340614318848\n",
      "Loss: 4.341638088226318\n",
      "Loss: 4.343226432800293\n",
      "Loss: 4.344420909881592\n",
      "Loss: 4.345620155334473\n",
      "Loss: 4.346681118011475\n",
      "Loss: 4.348108291625977\n",
      "Loss: 4.349391937255859\n",
      "Loss: 4.35051965713501\n",
      "Loss: 4.351743698120117\n",
      "Loss: 4.35274076461792\n",
      "Loss: 4.353273868560791\n",
      "Eval -  4.298069000244141\n",
      "В понедельник вечером Награва Германии сообщает , что его уволили из - за иска \" Гости аль - Буке\n",
      "« Суду , эти сумки , имеющие медицинскую компенсацию , буквы\n",
      "Впервые ролик в аппарате правительства носил ОПГ два режиссера Дарнера — « Траглупленное и мур Тий — абсолютно интересно и », — рассказал один архитектор штаба анимации Марина Захария\n",
      "23\n",
      "Loss: 4.303403854370117\n",
      "Loss: 4.301823139190674\n",
      "Loss: 4.305687427520752\n",
      "Loss: 4.310599327087402\n",
      "Loss: 4.314635753631592\n",
      "Loss: 4.317522048950195\n",
      "Loss: 4.318727493286133\n",
      "Loss: 4.320273399353027\n",
      "Loss: 4.322745323181152\n",
      "Loss: 4.32564640045166\n",
      "Loss: 4.327823162078857\n",
      "Loss: 4.3297438621521\n",
      "Loss: 4.330756664276123\n",
      "Loss: 4.332824230194092\n",
      "Loss: 4.33432149887085\n",
      "Loss: 4.336014270782471\n",
      "Loss: 4.3372931480407715\n",
      "Loss: 4.3379669189453125\n",
      "Loss: 4.339164733886719\n",
      "Loss: 4.34078311920166\n",
      "Loss: 4.341011047363281\n",
      "Loss: 4.34188175201416\n",
      "Eval -  4.281798362731934\n",
      "Явка инвестиций в проект « Мир », в настоящее время с британской точки зрения\n",
      "Наконец , через бронемашине неизвестные неизвестные расстреляли пилотов машины из Перми и 14 марта\n",
      "Такое заявление Климкина сделал в своем кабинете и обвинил в послужном насилии колени его в своей резиденции в Лондоне\n",
      "24\n",
      "Loss: 4.292677402496338\n",
      "Loss: 4.291921615600586\n",
      "Loss: 4.295347690582275\n",
      "Loss: 4.2988505363464355\n",
      "Loss: 4.302230358123779\n",
      "Loss: 4.305253028869629\n",
      "Loss: 4.308651447296143\n",
      "Loss: 4.311691761016846\n",
      "Loss: 4.313755512237549\n",
      "Loss: 4.315995216369629\n",
      "Loss: 4.317534446716309\n",
      "Loss: 4.317914009094238\n",
      "Loss: 4.319620609283447\n",
      "Loss: 4.321335315704346\n",
      "Loss: 4.322676658630371\n",
      "Loss: 4.324688911437988\n",
      "Loss: 4.326235294342041\n",
      "Loss: 4.327152252197266\n",
      "Loss: 4.328353404998779\n",
      "Loss: 4.328908920288086\n",
      "Loss: 4.329667568206787\n",
      "Loss: 4.331006050109863\n",
      "Eval -  4.266077041625977\n",
      "Этот шаг распределло государственность , совершенный на место не представляется противником\n",
      "\n",
      "Поэтому предстал сегодня управлять партии , который также может внешнего подъем , во вступившие в кресло\n",
      "25\n",
      "Loss: 4.277318000793457\n",
      "Loss: 4.282006740570068\n",
      "Loss: 4.286564826965332\n",
      "Loss: 4.289977073669434\n",
      "Loss: 4.295190334320068\n",
      "Loss: 4.297091484069824\n",
      "Loss: 4.29831075668335\n",
      "Loss: 4.301043510437012\n",
      "Loss: 4.302692413330078\n",
      "Loss: 4.303901195526123\n",
      "Loss: 4.3051347732543945\n",
      "Loss: 4.306829452514648\n",
      "Loss: 4.308610439300537\n",
      "Loss: 4.309875011444092\n",
      "Loss: 4.3116960525512695\n",
      "Loss: 4.313400745391846\n",
      "Loss: 4.314975738525391\n",
      "Loss: 4.316568851470947\n",
      "Loss: 4.3176398277282715\n",
      "Loss: 4.3187479972839355\n",
      "Loss: 4.319345951080322\n",
      "Loss: 4.32053279876709\n",
      "Eval -  4.254140853881836\n",
      "По словам премьер , я вопрос : ной будем общаться\n",
      "Перед водителем французского Лейгии объяснил примлением главаря боевиков террористов , поскольку теракт отстранен его нападение на его территории , но собирался веществом , которую он нам просто проявлял гражданскую войну и обстоят\n",
      "Как говорится в сообщении полиции , Скворцовой рассказала молодой человек и пассажир\n",
      "26\n",
      "Loss: 4.262653350830078\n",
      "Loss: 4.266253471374512\n",
      "Loss: 4.271610260009766\n",
      "Loss: 4.275399208068848\n",
      "Loss: 4.279046058654785\n",
      "Loss: 4.283371448516846\n",
      "Loss: 4.287180423736572\n",
      "Loss: 4.28878927230835\n",
      "Loss: 4.290086269378662\n",
      "Loss: 4.291348457336426\n",
      "Loss: 4.294164180755615\n",
      "Loss: 4.296308994293213\n",
      "Loss: 4.297630786895752\n",
      "Loss: 4.299520969390869\n",
      "Loss: 4.301388740539551\n",
      "Loss: 4.302981376647949\n",
      "Loss: 4.304262161254883\n",
      "Loss: 4.305438995361328\n",
      "Loss: 4.307041645050049\n",
      "Loss: 4.308291912078857\n",
      "Loss: 4.309866428375244\n",
      "Loss: 4.310878276824951\n",
      "Eval -  4.249636173248291\n",
      "По словам президента , Уиндины , в переводе астероид будет продан вместе\n",
      "Украинский боксер получил прозвище 15 лет , а это не будет его обладателя\n",
      "Во году церемония будет в театре франшизы , Вагнер Эдгар Ооджзян Болсбуржде\n",
      "27\n",
      "Loss: 4.2595038414001465\n",
      "Loss: 4.263150691986084\n",
      "Loss: 4.266278266906738\n",
      "Loss: 4.2696123123168945\n",
      "Loss: 4.2700700759887695\n",
      "Loss: 4.274422645568848\n",
      "Loss: 4.278167247772217\n",
      "Loss: 4.280939102172852\n",
      "Loss: 4.2839131355285645\n",
      "Loss: 4.286041736602783\n",
      "Loss: 4.288424491882324\n",
      "Loss: 4.289475440979004\n",
      "Loss: 4.291384696960449\n",
      "Loss: 4.292440414428711\n",
      "Loss: 4.293960094451904\n",
      "Loss: 4.295298099517822\n",
      "Loss: 4.296256065368652\n",
      "Loss: 4.297244071960449\n",
      "Loss: 4.29837703704834\n",
      "Loss: 4.299429893493652\n",
      "Loss: 4.300622940063477\n",
      "Loss: 4.301602840423584\n",
      "Eval -  4.240329265594482\n",
      "Об этом говорится в сообщении института общественной правозащитной организации имени Д\n",
      "Работает из лучших\n",
      "задержаны сотрудники правоохранительных органов города Москвы , Одинцовское ОВД\n",
      "28\n",
      "Loss: 4.246002197265625\n",
      "Loss: 4.254456996917725\n",
      "Loss: 4.256024360656738\n",
      "Loss: 4.258232116699219\n",
      "Loss: 4.262105941772461\n",
      "Loss: 4.264504909515381\n",
      "Loss: 4.267298221588135\n",
      "Loss: 4.269975662231445\n",
      "Loss: 4.273148059844971\n",
      "Loss: 4.27488374710083\n",
      "Loss: 4.277637958526611\n",
      "Loss: 4.279306888580322\n",
      "Loss: 4.281656742095947\n",
      "Loss: 4.2833662033081055\n",
      "Loss: 4.28427267074585\n",
      "Loss: 4.285508632659912\n",
      "Loss: 4.287176132202148\n",
      "Loss: 4.288290977478027\n",
      "Loss: 4.28959321975708\n",
      "Loss: 4.291202068328857\n",
      "Loss: 4.292215824127197\n",
      "Loss: 4.293196678161621\n",
      "Eval -  4.238858699798584\n",
      "По данным полиции , Дурионер несет несколько выстрелов\n",
      "Приехав Плющенко — одержится двукратный олимпийский чемпион турнира Сочи во втором - турнире - 2005 « Колорадо » в законближайшее время\n",
      "Последний раз мы полагаем также , мне просто ничего психологически понятно , для такой решения очень новую структуру\n",
      "29\n",
      "Loss: 4.243319511413574\n",
      "Loss: 4.246053695678711\n",
      "Loss: 4.249924659729004\n",
      "Loss: 4.253973484039307\n",
      "Loss: 4.2553911209106445\n",
      "Loss: 4.257923603057861\n",
      "Loss: 4.260383605957031\n",
      "Loss: 4.262879848480225\n",
      "Loss: 4.2650465965271\n",
      "Loss: 4.265944957733154\n",
      "Loss: 4.268213748931885\n",
      "Loss: 4.2702178955078125\n",
      "Loss: 4.272215843200684\n",
      "Loss: 4.2738752365112305\n",
      "Loss: 4.2748122215271\n",
      "Loss: 4.276402473449707\n",
      "Loss: 4.277443885803223\n",
      "Loss: 4.279360294342041\n",
      "Loss: 4.280515670776367\n",
      "Loss: 4.282387733459473\n",
      "Loss: 4.283719062805176\n",
      "Loss: 4.284896373748779\n",
      "Eval -  4.218785285949707\n",
      "В недавних матчах за \" Нью - Джерси \" - не попала в \" Манчестер Юнайтед \" 18 : 3\n",
      "Об этом газете рассказал ТАСС в интервью в британских СМИ газете Financial Times\n",
      "Полиция предупредили , что следователи требуют выплаты зарплаты гражданам двух международных сайтов в ближайшие недели , а также закрыть в США систему « благоустройства регионе\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "eval_losses = []\n",
    "for i in range(30):\n",
    "    print(i)\n",
    "    train_losses.append(train(model, training_generator, optimizer, 100))\n",
    "    eval_loss = evaluate(model, val_generator)\n",
    "    print('Eval - ', eval_loss.item())\n",
    "    eval_losses.append(eval_loss)\n",
    "    for _ in range(3):\n",
    "        pred = model.generate(torch.LongTensor([[tokenizer.token_to_id('[BOS]')]]).to('cuda'), 200, tokenizer.token_to_id('[EOS]'))\n",
    "        print(tokenizer.decoder.decode([tokenizer.id_to_token(i) for i in pred.detach().cpu().numpy()[0]][1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba714072-1d7d-4c3b-a770-209da0a5373a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
