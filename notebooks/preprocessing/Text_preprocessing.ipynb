{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предобработка текстовых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работа с текстовыми данными обычно начинается с предобработки. Грубо говоря, предобработка - это удаление всего лишнего и приведение к нужному формату. Предобработка может быть долгой и неприятной, но в большинстве случаев все сводится к использованию стандартных инструментов. Эта тетрадка познакомит вас с такими инструментами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Установите все нужные библиотеки. Подробнее про каждую из них ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pymystem3\n",
    "!pip install pymorphy2[fast]\n",
    "!pip install razdel\n",
    "!pip install gensim\n",
    "!pip install nltk\n",
    "!pip install rusenttokenize\n",
    "!pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Скорее всего вы уже знакомы с библиотекой pymorphy2.\n",
    "# pymorphy2[fast] - это оптимизированный pymorphy2, который работает точно также, но сильно быстрее\n",
    "# Если у вас windows, то он вряд ли установится и вам придется пользоваться стандартным"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регулярные выражения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один из базовых, но в то же время самых полезных инструментов для предобработки текста - регулярные выражения. В вводной части курса им был посвящен целый семинар - https://github.com/mannefedov/compling_nlp_hse_course/blob/master/notebooks/first_module_intro/01_regular_expressions.ipynb  \n",
    "\n",
    "Если вы не ходили подготовительную часть и чувствуете себя неуверенно при работе с регулярками - пройдитесь по семинару. Решить домашку тоже не помешает. В семинаре я буду предполагать, что вы уже знакомы с вещами, которые разобраны в подготовительной части. Но новые вещи я буду объяснять дополнительно!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбиение на предложения, токенизация, нормализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За каждым из этих трех терминов стоит большая и сложная подзадача NLP. Однако для каждой есть готовые решения, которые очень хорошо работают. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разбиение на предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE. Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.\n",
    "\n",
    "Во-первых, NLI можно использовать для контроля качества генеративных моделей. Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод. Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\". С помощью модели NLI можно проверять, что из X следует Y (то есть в новом тексте нету \"отсебятины\", придуманной моделью), и что из Y следует X (т.е. вся информация, присутствовавшая в исходном тексте, в новом также отражена).\n",
    "\n",
    "Во-вторых, с помощью моделей NLI можно находить нетривиальные парафразы и в целом определять смысловую близость текстов. Для русского языка уже существует ряд моделей и датасетов по перефразированию, но кажется, что можно сделать ещё больше и лучше. В статье Improving Paraphrase Detection with the Adversarial Paraphrasing Task предложили считать парафразами такую пару предложений, в которой каждое логически следует из другого – и это весьма логично. Поэтому модели NLI можно использовать и для сбора обучающего корпуса парафраз (и не-парафраз, если стоит задача их детекции), и для фильтрации моделей, генерирующих парафразы.\n",
    "\n",
    "\"\"\"\n",
    "# текст отсюда - https://habr.com/ru/post/582620/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В __nltk__ есть уже готовая функция для разбивки на предложения. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE.',\n",
       " 'Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.',\n",
       " 'Во-первых, NLI можно использовать для контроля качества генеративных моделей.',\n",
       " 'Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод.',\n",
       " 'Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\".',\n",
       " 'С помощью модели NLI можно проверять, что из X следует Y (то есть в новом тексте нету \"отсебятины\", придуманной моделью), и что из Y следует X (т.е. вся информация, присутствовавшая в исходном тексте, в новом также отражена).',\n",
       " 'Во-вторых, с помощью моделей NLI можно находить нетривиальные парафразы и в целом определять смысловую близость текстов.',\n",
       " 'Для русского языка уже существует ряд моделей и датасетов по перефразированию, но кажется, что можно сделать ещё больше и лучше.',\n",
       " 'В статье Improving Paraphrase Detection with the Adversarial Paraphrasing Task предложили считать парафразами такую пару предложений, в которой каждое логически следует из другого – и это весьма логично.',\n",
       " 'Поэтому модели NLI можно использовать и для сбора обучающего корпуса парафраз (и не-парафраз, если стоит задача их детекции), и для фильтрации моделей, генерирующих парафразы.']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text, 'russian')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nltk также позволяет обучить свой токенизатор предложений под определенный корпус.\n",
    "Вот код для примера (изначально я взял его отсюда - https://nlpforhackers.io/splitting-text-into-sentences/ , но сайт больше не работает):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    " \n",
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(text)\n",
    " \n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    " \n",
    "# Test the tokenizer on a piece of text\n",
    "sentences = \"Mr. James told me Dr. Brown is not available today. I will try tomorrow.\"\n",
    " \n",
    "print(tokenizer.tokenize(sentences))\n",
    "# ['Mr. James told me Dr.', 'Brown is not available today.', 'I will try tomorrow.']\n",
    " \n",
    "# View the learned abbreviations\n",
    "print(tokenizer._params.abbrev_types)\n",
    "# set([...])\n",
    " \n",
    "# Here's how to debug every split decision\n",
    "for decision in tokenizer.debug_decisions(sentences):\n",
    "    pprint(decision)\n",
    "    print('=' * 30)\n",
    "\n",
    "# adding abbreviations manually\n",
    "tokenizer._params.abbrev_types.add('dr')\n",
    " \n",
    "print(tokenizer.tokenize(sentences))\n",
    "# ['Mr. James told me Dr. Brown is not available today.', 'I will try tomorrow.']\n",
    " \n",
    "for decision in tokenizer.debug_decisions(sentences):\n",
    "    pprint(decision)\n",
    "    print('=' * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В __gensim__ тоже есть готовая функция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization.textcleaner import split_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE.',\n",
       " 'Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.',\n",
       " 'Во-первых, NLI можно использовать для контроля качества генеративных моделей.',\n",
       " 'Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод.',\n",
       " 'Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\".']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# это ещё и генератор, т.е. сразу подходит для больших корпусов\n",
    "list(split_sentences(text))[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У DeepPavlov есть библиотека [**rusenttokenizer**](https://github.com/deepmipt/ru_sentence_tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rusenttokenize import ru_sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Something went wrong while tokenizing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE.',\n",
       " 'Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.',\n",
       " 'Во-первых, NLI можно использовать для контроля качества генеративных моделей.',\n",
       " 'Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод.',\n",
       " 'Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\".',\n",
       " 'С помощью модели NLI можно проверять, что из X следует Y (то есть в новом тексте нету \"отсебятины\", придуманной моделью), и что из Y следует X (т.е. вся информация, присутствовавшая в исходном тексте, в новом также отражена).',\n",
       " 'Во-вторых, с помощью моделей NLI можно находить нетривиальные парафразы и в целом определять смысловую близость текстов.',\n",
       " 'Для русского языка уже существует ряд моделей и датасетов по перефразированию, но кажется, что можно сделать ещё больше и лучше.',\n",
       " 'В статье Improving Paraphrase Detection with the Adversarial Paraphrasing Task предложили считать парафразами такую пару предложений, в которой каждое логически следует из другого – и это весьма логично.',\n",
       " 'Поэтому модели NLI можно использовать и для сбора обучающего корпуса парафраз (и не-парафраз, если стоит задача их детекции), и для фильтрации моделей, генерирующих парафразы.']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_sent_tokenize(text)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В проекте Natasha есть библиотека [**razdel**](https://github.com/natasha/razdel). Она чуть более навороченная."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from razdel import sentenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = list(sentenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(0,\n",
       "           244,\n",
       "           'Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE.'),\n",
       " Substring(245,\n",
       "           322,\n",
       "           'Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.'),\n",
       " Substring(324,\n",
       "           401,\n",
       "           'Во-первых, NLI можно использовать для контроля качества генеративных моделей.'),\n",
       " Substring(402,\n",
       "           635,\n",
       "           'Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод.'),\n",
       " Substring(636,\n",
       "           913,\n",
       "           'Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\".')]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# числа тут - это спаны, индексы начала и конца предложения в изначальном тексте\n",
    "sents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE.',\n",
       " 'Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.',\n",
       " 'Во-первых, NLI можно использовать для контроля качества генеративных моделей.',\n",
       " 'Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод.',\n",
       " 'Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\".']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# у объекта Substring есть атрибуты start, stop и text. С помощью них можно вытащить нужное\n",
    "[sent.text for sent in sents[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если все-таки нужно добавить каких-то специфичных правил разбиения на предложения, можно опять же воспользоваться регулярными выражениями. Однако в этом случае регулярка будет посложнее. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим, что получится, если в качестве разделителя использовать !?. пробел и заглавную букву.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE',\n",
       " 'роме этого, модели NLI обладают прикладной ценностью по нескольким причинам.\\n\\nВо-первых, NLI можно использовать для контроля качества генеративных моделей',\n",
       " 'сть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод',\n",
       " 'овременные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\"',\n",
       " ' помощью модели NLI можно проверять, что из X следует Y (то есть в новом тексте нету \"отсебятины\", придуманной моделью), и что из Y следует X (т.е. вся информация, присутствовавшая в исходном тексте, в новом также отражена).\\n\\nВо-вторых, с помощью моделей NLI можно находить нетривиальные парафразы и в целом определять смысловую близость текстов',\n",
       " 'ля русского языка уже существует ряд моделей и датасетов по перефразированию, но кажется, что можно сделать ещё больше и лучше',\n",
       " ' статье Improving Paraphrase Detection with the Adversarial Paraphrasing Task предложили считать парафразами такую пару предложений, в которой каждое логически следует из другого – и это весьма логично',\n",
       " 'оэтому модели NLI можно использовать и для сбора обучающего корпуса парафраз (и не-парафраз, если стоит задача их детекции), и для фильтрации моделей, генерирующих парафразы.\\n\\n']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('[!?\\.] [А-Я]', text)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблема в том, что сам разделитель удаляется тоже, а нам нужно удалить только пробел между знаками препинания и заглавной буквой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решается эта проблема с помощью __look ahead__ и __look behind__ (название функционала в регулярных выражениях).  \n",
    "Синтаксис там такой:  \n",
    " **(?<=pattern)** положительное look-behind условие  \n",
    " **(?<!pattern)** отрицательное look-behind условие   \n",
    " **(?=pattern)** положительное look-ahead условие   \n",
    " **(?!pattern)** отрицательное look-ahead условие   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробно про это написано тут: https://www.regular-expressions.info/lookaround.html  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look behind и look ahead превращают паттерн в условный, то есть проверяется есть ли он (до или после, соответственно), но его захвата не происходит. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обернём наше регулярное выражение и посмотрим, что получается:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Задача NLI важна для компьютерных лингвистов, ибо она позволяет детально рассмотреть, какие языковые явления данная модель понимает хорошо, а на каких – \"плывёт\"; по этому принципу устроены диагностические датасеты SuperGLUE и RussianSuperGLUE.',\n",
       " 'Кроме этого, модели NLI обладают прикладной ценностью по нескольким причинам.',\n",
       " 'Во-первых, NLI можно использовать для контроля качества генеративных моделей.',\n",
       " 'Есть масса задач, где на основе текста X нужно сгенерировать близкий к нему по смыслу текст Y: суммаризация, упрощение текстов, перефразирование, перенос стиля на текстах, текстовые вопросно-ответные системы, и даже машинный перевод.',\n",
       " 'Современные seq2seq нейросети типа T5 (которая в этом году появилась и для русского языка) в целом неплохо справляются с такими задачами, но время от времени лажают, упуская какую-то важную информацию из Х, или, наоборот, дописывая в текст Y что-то нафантазированное \"от себя\".',\n",
       " 'С помощью модели NLI можно проверять, что из X следует Y (то есть в новом тексте нету \"отсебятины\", придуманной моделью), и что из Y следует X (т.е. вся информация, присутствовавшая в исходном тексте, в новом также отражена).',\n",
       " 'Во-вторых, с помощью моделей NLI можно находить нетривиальные парафразы и в целом определять смысловую близость текстов.',\n",
       " 'Для русского языка уже существует ряд моделей и датасетов по перефразированию, но кажется, что можно сделать ещё больше и лучше.',\n",
       " 'В статье Improving Paraphrase Detection with the Adversarial Paraphrasing Task предложили считать парафразами такую пару предложений, в которой каждое логически следует из другого – и это весьма логично.',\n",
       " 'Поэтому модели NLI можно использовать и для сбора обучающего корпуса парафраз (и не-парафраз, если стоит задача их детекции), и для фильтрации моделей, генерирующих парафразы.  ']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('(?<=[\\.?!]) +(?=[А-ЯЁ])', text.replace('\\n', ' '))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы разбили текст на предложения. Теперь предложения нужно разбить на токены. Под токенами обычно понимаются слова, но это могут быть и какие-то более длинные или короткие куски. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый простой способ токенизации -- стандартный питоновский __str.split__ метод.  \n",
    "По умолчанию он разбивает текст по последовательностям пробелов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '', '2', '3']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'1  2 3'.split(' ') # NB! .split() и .split(' ') - не одно и тоже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'1  2 3'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['рассмотреть,',\n",
       " 'какие',\n",
       " 'языковые',\n",
       " 'явления',\n",
       " 'данная',\n",
       " 'модель',\n",
       " 'понимает',\n",
       " 'хорошо,',\n",
       " 'а',\n",
       " 'на',\n",
       " 'каких',\n",
       " '–',\n",
       " '\"плывёт\";',\n",
       " 'по',\n",
       " 'этому',\n",
       " 'принципу',\n",
       " 'устроены',\n",
       " 'диагностические',\n",
       " 'датасеты',\n",
       " 'SuperGLUE']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()[10:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большая часть слов отделяется, но знаки препинания лепятся к словам.\n",
    "Можно пройтись по всем словам и убрать из них пунктуацию с методом str.strip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#основные знаки преминания хранятся в питоновском модуле string в punctuation\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# в этом списке не хватает кавычек-ёлочек, лапок, длинного тире и многоточия\n",
    "string.punctuation += '«»—…“”'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['рассмотреть',\n",
       " 'какие',\n",
       " 'языковые',\n",
       " 'явления',\n",
       " 'данная',\n",
       " 'модель',\n",
       " 'понимает',\n",
       " 'хорошо',\n",
       " 'а',\n",
       " 'на',\n",
       " 'каких',\n",
       " '–',\n",
       " 'плывёт',\n",
       " 'по',\n",
       " 'этому',\n",
       " 'принципу',\n",
       " 'устроены',\n",
       " 'диагностические',\n",
       " 'датасеты',\n",
       " 'SuperGLUE']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word.strip(string.punctuation) for word in text.split()][10:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так не будут удаляться дефисы и точки в сокращениях, не разделенных пробелом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'как-нибудь'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'как-нибудь'.strip(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'т.е'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'т.е.'.strip(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой способ токенизации может работать быстрее других из-за того, что используются только дефолтные инструменты питона. Если важно качество, то лучше пользоваться готовыми токенизаторами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, готовые токенизаторы есть в nltk. Они не удаляют пунктуацию, а выделяют её отдельным токеном.\n",
    "\n",
    "**wordpunct_tokenizer** разбирает по регулярке - *'\\w+|[^\\w\\s]+'* (попробуйте понять как она работает просто глядя на паттерн)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Задача',\n",
       " 'NLI',\n",
       " 'важна',\n",
       " 'для',\n",
       " 'компьютерных',\n",
       " 'лингвистов',\n",
       " ',',\n",
       " 'ибо',\n",
       " 'она',\n",
       " 'позволяет']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(text)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**word_tokenize** также построен на регулярках, но они там более сложные (учитывается последовательность некоторых \n",
    "символов, символы начала, конца слова и т.д). \n",
    "\n",
    "Специально подобранного под русский язык токенизатора там нет, \n",
    "но и с английским всё работает достаточно хорошо --\n",
    "сокращения типа т.к собираются в один токен, дефисные слова тоже не разделяются, многоточия тут тоже не отделяются, но это можно поправить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['лажают',\n",
       " ',',\n",
       " 'упуская',\n",
       " 'какую-то',\n",
       " 'важную',\n",
       " 'информацию',\n",
       " 'из',\n",
       " 'Х',\n",
       " ',',\n",
       " 'или',\n",
       " ',',\n",
       " 'наоборот',\n",
       " ',',\n",
       " 'дописывая',\n",
       " 'в',\n",
       " 'текст',\n",
       " 'Y',\n",
       " 'что-то',\n",
       " 'нафантазированное',\n",
       " '``']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text)[130:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В генсиме тоже есть функция для токенизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['russiansuperglue',\n",
       " 'кроме',\n",
       " 'этого',\n",
       " 'модели',\n",
       " 'nli',\n",
       " 'обладают',\n",
       " 'прикладной',\n",
       " 'ценностью',\n",
       " 'по',\n",
       " 'нескольким',\n",
       " 'причинам',\n",
       " 'во',\n",
       " 'первых',\n",
       " 'nli',\n",
       " 'можно',\n",
       " 'использовать',\n",
       " 'для',\n",
       " 'контроля',\n",
       " 'качества',\n",
       " 'генеративных']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# опять же, это генератор\n",
    "list(tokenize(text, lowercase=True))[30:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И в razdel тоже есть токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from razdel import tokenize as razdel_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(0, 6, 'Задача'),\n",
       " Substring(7, 10, 'NLI'),\n",
       " Substring(11, 16, 'важна'),\n",
       " Substring(17, 20, 'для'),\n",
       " Substring(21, 33, 'компьютерных'),\n",
       " Substring(34, 44, 'лингвистов'),\n",
       " Substring(44, 45, ','),\n",
       " Substring(46, 49, 'ибо'),\n",
       " Substring(50, 53, 'она'),\n",
       " Substring(54, 63, 'позволяет')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(razdel_tokenize(text))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Задача',\n",
       " 'NLI',\n",
       " 'важна',\n",
       " 'для',\n",
       " 'компьютерных',\n",
       " 'лингвистов',\n",
       " ',',\n",
       " 'ибо',\n",
       " 'она',\n",
       " 'позволяет']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text for token in list(razdel_tokenize(text))[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работать с регистром тяжело и поэтому можно привести все к нижнему регистру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['задача',\n",
       " 'nli',\n",
       " 'важна',\n",
       " 'для',\n",
       " 'компьютерных',\n",
       " 'лингвистов',\n",
       " ',',\n",
       " 'ибо',\n",
       " 'она',\n",
       " 'позволяет']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text.lower() for token in list(razdel_tokenize(text))[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нормализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В последнее время нормализация (т.е. приведение токенов к стандартному виду) используется все реже. Это связано с использованием subword или byte токенизации в топовых моделях (подробнее об этом мы поговорим когда дойдем до нейронных сетей). Однако у них есть свои недостатки и забывать про нормализацию пока не стоит."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Два основных вида нормализации - лемматизация и стемминг. Стемминг уже нигде не используется, но его полезно разобрать, чтобы понимать почему нужно использовать лемматизацию (еще стемминг по непонятной причине часто упоминается как важный скилл в вакансиях)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стемминг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стемминг - это урезание слова до его \"основы\" (стема), т.е. такой части, которая является общей для всех словоформ в парадигме слова *(Значения слов \"слово\", \"слоформа\", \"парадигма\" приблизительно соответствует тому, которое использует Зализняк вот тут - http://inslav.ru/images/stories/pdf/2002_Zalizniak_RIS_i_statji.pdf (стр. 21-22)) Но это на самом деле не важно)*. \n",
    "\n",
    "По крайней мере так в теории. На практике стемминг сводится к отбрасыванию частотных окончаний."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый известный стеммер - стеммер Портера (или snowball стеммер). \n",
    "Подробнее про стеммер Портера можно почитать вот тут - <https://medium.com/@eigenein/стеммер-портера-для-русского-языка-d41c38b2d340>  \n",
    "А совсем подробнее вот тут - <http://snowball.tartarus.org/algorithms/russian/stemmer.html>  \n",
    "Почему он так называется? Так назывался язык программирования, который Портер написал для стеммеров. Язык так называется в созвучие языку SNOBOL. Вот комментарий самого Портера:\n",
    "\n",
    "`Since it effectively provides a ‘suffix STRIPPER GRAMmar’, I had toyed with the idea of calling it ‘strippergram’, but good sense has prevailed, and so it is ‘Snowball’ named as a tribute to SNOBOL, the excellent string handling language of Messrs Farber, Griswold, Poage and Polonsky from the 1960s.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готовые стеммеры для разных языков есть в nltk. Работают они вот так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Задача', 'задач'),\n",
       " ('NLI', 'NLI'),\n",
       " ('важна', 'важн'),\n",
       " ('для', 'для'),\n",
       " ('компьютерных', 'компьютерн'),\n",
       " ('лингвистов', 'лингвист'),\n",
       " (',', ','),\n",
       " ('ибо', 'иб'),\n",
       " ('она', 'он'),\n",
       " ('позволяет', 'позволя'),\n",
       " ('детально', 'детальн'),\n",
       " ('рассмотреть', 'рассмотрет'),\n",
       " (',', ','),\n",
       " ('какие', 'как'),\n",
       " ('языковые', 'языков'),\n",
       " ('явления', 'явлен'),\n",
       " ('данная', 'дан'),\n",
       " ('модель', 'модел'),\n",
       " ('понимает', 'понима'),\n",
       " ('хорошо', 'хорош'),\n",
       " (',', ','),\n",
       " ('а', 'а'),\n",
       " ('на', 'на'),\n",
       " ('каких', 'как'),\n",
       " ('–', '–'),\n",
       " ('``', '``'),\n",
       " ('плывёт', 'плывет'),\n",
       " (\"''\", \"'\"),\n",
       " (';', ';'),\n",
       " ('по', 'по')]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(word, stemmer.stem(word)) for word in word_tokenize(text)][:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Недостатки стемминга достаточно очевидные:  \n",
    "1) с супплетивными формами или редкими окончаниями слова стемминг работать не умеет  \n",
    "2) к одной основе могут приводится разные слова  \n",
    "3) к разным основам могут сводиться формы одного слова  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизация - это замена словоформы слова в парадигме на какую-то заранее выбранную стадартную форму (лемму). \n",
    "\n",
    "\n",
    "\n",
    "Например, для разных форм глагола леммой обычно является неопределенная форма (инфинитив), а для существительного форма мужского рода единственного числа. Это позволяет избавиться от недостатков стемминга (будет, был - одна лемма), (пролить, пролом - разные). Однако лемматизация значительно сложнее. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К счастью есть готовые хорошие лемматизаторы. Для русского основых варианта два: Mystem и Pymorphy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymystem3\n",
      "  Using cached pymystem3-0.2.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: requests in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from pymystem3) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from requests->pymystem3) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from requests->pymystem3) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from requests->pymystem3) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from requests->pymystem3) (2024.2.2)\n",
      "Installing collected packages: pymystem3\n",
      "Successfully installed pymystem3-0.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import os, json\n",
    "mystem = Mystem(disambiguation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Майстем работает немного лучше и сам токенизирует,\n",
    "поэтому можно в него засовывать сырой текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['задача', ' ', 'NLI', ' ', 'важный', ' ', 'для', ' ', 'компьютерный', ' ']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mystem.lemmatize функция лемматизации в майстеме\n",
    "# сам объект mystem нужно заранее инициализировать\n",
    "mystem.lemmatize(text)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если нужна грамматическая информация или надо сохранить ненормализованный текст,\n",
    "# есть функция mystem.analyze\n",
    "words_analized = mystem.analyze(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'analysis': [{'lex': 'задача', 'wt': 1, 'gr': 'S,жен,неод=им,ед'}],\n",
       "  'text': 'Задача'},\n",
       " {'text': ' '},\n",
       " {'analysis': [], 'text': 'NLI'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'важный', 'wt': 1, 'gr': 'A=ед,кр,жен'}],\n",
       "  'text': 'важна'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'для', 'wt': 1, 'gr': 'PR='}], 'text': 'для'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'компьютерный',\n",
       "    'wt': 1,\n",
       "    'gr': 'A=(пр,мн,полн|вин,мн,полн,од|род,мн,полн)'}],\n",
       "  'text': 'компьютерных'},\n",
       " {'text': ' '}]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# возвращает она список словарей\n",
    "# каждый словарь имеет либо одно поле 'text' (когда попался пробел) или text и analysis\n",
    "# в analysis снова список словарей с вариантами разбора (первый самый вероятный)\n",
    "# поля в analysis - 'gr' - грамматическая информация, 'lex' - лемма\n",
    "# analysis - может быть пустым списком\n",
    "words_analized[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Слово -  Задача\n",
      "Разбор слова -  {'lex': 'задача', 'wt': 1, 'gr': 'S,жен,неод=им,ед'}\n",
      "Лемма слова -  задача\n",
      "Грамматическая информация слова -  S,жен,неод=им,ед\n"
     ]
    }
   ],
   "source": [
    "print('Слово - ', words_analized[0]['text'])\n",
    "print('Разбор слова - ', words_analized[0]['analysis'][0])\n",
    "print('Лемма слова - ', words_analized[0]['analysis'][0]['lex'])\n",
    "print('Грамматическая информация слова - ', words_analized[0]['analysis'][0]['gr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['задача',\n",
       " 'важный',\n",
       " 'для',\n",
       " 'компьютерный',\n",
       " 'лингвист',\n",
       " 'ибо',\n",
       " 'она',\n",
       " 'позволять',\n",
       " 'детально',\n",
       " 'рассматривать']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#леммы можно достать в одну строчку\n",
    "[parse['analysis'][0]['lex'] for parse in words_analized if parse.get('analysis')][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mystem умеет разбивать текст на предложения, но через питоновский интерфейс это сделать не получится. Нужно скачать mystem отсюда - https://yandex.ru/dev/mystem/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После этого сохранить текст в файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('text.txt', 'w')\n",
    "f.write(text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из командной строки или из питона запустить майстем на нашем файле"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# про параметры почитайте в !mystem -h\n",
    "!mystem -isc --format json text.txt text_parsed.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целевом файле теперь лежит разобранный текст в jsonlines (json на каждой строчке)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "t = [json.loads(line) for line in open('text_parsed.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'analysis': [{'lex': 'задача', 'gr': 'S,жен,неод=им,ед'}], 'text': 'Задача'},\n",
       " {'text': ' '},\n",
       " {'analysis': [], 'text': 'NLI'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'важный', 'gr': 'A=ед,кр,жен'}], 'text': 'важна'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'для', 'gr': 'PR='}], 'text': 'для'},\n",
       " {'text': ' '},\n",
       " {'analysis': [{'lex': 'компьютерный', 'gr': 'A=пр,мн,полн'},\n",
       "   {'lex': 'компьютерный', 'gr': 'A=вин,мн,полн,од'},\n",
       "   {'lex': 'компьютерный', 'gr': 'A=род,мн,полн'}],\n",
       "  'text': 'компьютерных'},\n",
       " {'text': ' '}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый объект в этом списке - параграф. Каждый параграф на предложения можно разбив по тегу '//s'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё так вызывать майстем может понадобиться, если важна скорость."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Недостатки Mystem: это продукт Яндекса с некоторыми ограничениями на использование, больше он не развивается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важным достоинством Mystem является то, что он работает не с отдельными словами, а с целым предложением. При определении нужной леммы учитывается контекст, что позволяет во многих случаях разрешать омонимию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pymorphy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pymorphy - открытый и развивается (но не очень активно, т.к. это все сложно)\n",
    "\n",
    "Ссылка на репозиторий: https://github.com/kmike/pymorphy2\n",
    "\n",
    "Попробуйте сразу установить быструю версию (pip install pymorphy2[fast])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У него нет втстроенной токенизации и он расценивает всё как слово. Когда есть несколько вариантов, он выдает их с вероятностостями, которые расчитатны на корпусе со снятой неоднозначностью. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2.analyzer import Parse, MorphAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Parse(word='печь', tag='INFN,impf,tran', normal_form='печь', score=0.666666, \n",
    "      methods_stack=((dict, 'печь', 2352, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "p._morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p.inflect(required_grammemes={\"gent\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# основная функция - pymorphy.parse\n",
    "words_analized = [morph.parse(token) for token in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph.parse(\"печь\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первое слово -  задача\n",
      "Разбор первого слова -  Parse(word='задача', tag=OpencorporaTag('NOUN,inan,femn sing,nomn'), normal_form='задача', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'задача', 93, 0),))\n",
      "Лемма первого слова -  задача\n",
      "Грамматическая информация первого слова -  NOUN,inan,femn sing,nomn\n",
      "Часть речи первого слова -  NOUN\n",
      "Род первого слова -  femn\n",
      "Число первого слова -  sing\n",
      "Падеж первого слова -  nomn\n"
     ]
    }
   ],
   "source": [
    "# Она похожа на analyze в майстеме только возрващает список объектов Parse\n",
    "# Первый в списке - самый вероятный разбор (у каждого есть score)\n",
    "# Информация достается через атрибут (Parse.word - например)\n",
    "# Грамматическая информация хранится в объекте OpencorporaTag и из него удобно доставать\n",
    "# части речи или другие категории\n",
    "print('Первое слово - ', words_analized[0][0].word)\n",
    "print('Разбор первого слова - ', words_analized[0][0])\n",
    "print('Лемма первого слова - ', words_analized[0][0].normal_form)\n",
    "print('Грамматическая информация первого слова - ', words_analized[0][0].tag)\n",
    "print('Часть речи первого слова - ', words_analized[0][0].tag.POS)\n",
    "print('Род первого слова - ', words_analized[0][0].tag.gender)\n",
    "print('Число первого слова - ', words_analized[0][0].tag.number)\n",
    "print('Падеж первого слова - ', words_analized[0][0].tag.case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительная очистка текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно убрать стоп-слова (предлоги, союзы, местоимения, частотные слова). Сам термин стоп-слово происходит из информационного поиска, первый раз его упомянул [Питер Лун](https://en.wikipedia.org/wiki/Hans_Peter_Luhn) в 1959.  \n",
    "Удаление таких слов позволяло сократить размер индекса и не сильно испортить выдачу или даже улучшить её, поднимая релевантность документам со значимыми словами. Со временем от такой практики, в основном, отказались - память стала дешевой (и повились всякие алгоритмы для сокращения потребления памяти), а для учёта значимости придумали TFIDF (про него на следующем занятии).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во многих поисковых движках стоп-слова всё ещё используются. Часто их используют и в практических задачах (классификации, тематическом моделировании). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "# стоп-слова есть в nltk\n",
    "stops = stopwords.words('russian')\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Список не идеальный и его можно расширять под свои задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['задача',\n",
       " 'nli',\n",
       " 'важный',\n",
       " 'компьютерный',\n",
       " 'лингвист',\n",
       " ',',\n",
       " 'ибо',\n",
       " 'позволять',\n",
       " 'детально',\n",
       " 'рассмотреть']"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_normalized = [morph.parse(token)[0].normal_form for token in word_tokenize(text)]\n",
    "[word for word in words_normalized if word not in stops][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка для других языков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предобработка сильно зависит от языка. Полностью универсальнных токенизаторов, лемматизаторов не бывает, а рассказать о предобработке под все существующие языки не реально. Поэтому посмотрим дополнительно только на предобработку на английском языке. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nltk и gensim по умолчанию адаптированы под английский язык (а регулярки вообще не привязаны к языку), поэтому разбирать их еще раз не будем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека, про которую стоит отдельно рассказать - [**SpaCy**](https://spacy.io/). Это многоцелевая многоязычная библиотека. Если вам понадобится серьезно работать с английским, то лучшим вариантом будет использовать SpaCy. Другие языки там тоже поддерживаются (см. документацию), но не настолько хорошо как английский язык. \n",
    "\n",
    "В SpaCy много всего и мы будем возвращаться к ней по ходу курса. Пока посмотрим на интрументы базовой предобработки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (3.7.5)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy) (8.2.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy) (2.7.2)\n",
      "Requirement already satisfied: jinja2 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy) (3.4.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy) (0.12.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy) (24.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: setuptools in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.3 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: setuptools in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
      "Requirement already satisfied: jinja2 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.3 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting de-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from de-core-news-sm==3.7.0) (3.7.5)\n",
      "Requirement already satisfied: setuptools in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (65.5.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.12.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.7.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.4.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (24.0)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.5)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.66.4)\n",
      "Requirement already satisfied: jinja2 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.3)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.3 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.18.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.12.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.2.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (13.7.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.5.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.2)\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-3.7.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n",
      "Collecting ru-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.7.0/ru_core_news_sm-3.7.0-py3-none-any.whl (15.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pymorphy3>=1.0.0\n",
      "  Downloading pymorphy3-2.0.2-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from ru-core-news-sm==3.7.0) (3.7.5)\n",
      "Collecting pymorphy3-dicts-ru\n",
      "  Using cached pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.7.0) (0.7.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.12.3)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (8.2.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.7.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: setuptools in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (65.5.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.4.1)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.32.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (24.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: jinja2 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.1.4)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.3 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.18.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (4.12.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (13.7.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (7.0.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.19.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.18.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.0.0)\n",
      "Requirement already satisfied: wrapt in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/mnefedov/.pyenv/versions/3.10.9/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.1.2)\n",
      "Installing collected packages: pymorphy3-dicts-ru, pymorphy3, ru-core-news-sm\n",
      "Successfully installed pymorphy3-2.0.2 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-sm-3.7.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ru_core_news_sm')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25hCollecting pymorphy3-dicts-ru\n",
      "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting dawg-python>=0.7.1\n",
      "  Using cached DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Collecting docopt-ng>=0.6\n",
      "  Downloading docopt_ng-0.9.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.23.3)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.4.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (4.66.1)\n",
      "Requirement already satisfied: setuptools in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (63.2.0)\n",
      "Requirement already satisfied: jinja2 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.11.3)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (6.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (23.1)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (8.2.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (4.8.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.0.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.2.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (7.1.2)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/mnefedov/miniforge3/lib/python3.9/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.0.1)\n",
      "Installing collected packages: pymorphy3-dicts-ru, dawg-python, docopt-ng, pymorphy3, ru-core-news-sm\n",
      "Successfully installed dawg-python-0.7.2 docopt-ng-0.9.0 pymorphy3-1.2.1 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-sm-3.7.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ru_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm\n",
    "!python -m spacy download ru_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'ru_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# загружаем пайплайн для английского языка\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mru_core_news_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/spacy/util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'ru_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# загружаем пайплайн для английского языка\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"ru_core_news_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = (\"One of the most salient features of our culture is that it won't so much bullshit.” \"\n",
    "        \"These are the opening words of the short book On Bullshit, written by the philosopher Harry Frankfurt. \"\n",
    "        \"Fifteen years after the publication of this surprise bestseller, \"\n",
    "        \"the rapid progress of research on artificial intelligence is forcing us to reconsider our conception \"\n",
    "        \"of bullshit as a hallmark of human speech, with troubling implications. What do philosophical \"\n",
    "        \"reflections on bullshit have to do with algorithms? As it turns out, quite a lot.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат предобаботки очень похож на майстем - тут есть разбиение на предложения, на токены, лемматизация, определение части речи. Тэги тут используются другие, но при желании можно сделать более менее адекватный маппинг (возможно кто-то уже это сделал и нужно только погуглить)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One one NUM\n",
      "of of ADP\n",
      "the the DET\n",
      "most most ADV\n",
      "salient salient ADJ\n",
      "features feature NOUN\n",
      "of of ADP\n",
      "our our PRON\n",
      "culture culture NOUN\n",
      "is be AUX\n",
      "that that SCONJ\n",
      "it it PRON\n",
      "wo wo AUX\n",
      "n't n't PART\n",
      "so so ADV\n",
      "much much ADJ\n",
      "bullshit bullshit NOUN\n",
      ". . PUNCT\n",
      "” \" PUNCT\n",
      "\n",
      "These these DET\n",
      "are be AUX\n",
      "the the DET\n",
      "opening opening NOUN\n",
      "words word NOUN\n",
      "of of ADP\n",
      "the the DET\n",
      "short short ADJ\n",
      "book book NOUN\n",
      "On on ADP\n",
      "Bullshit Bullshit PROPN\n",
      ", , PUNCT\n",
      "written write VERB\n",
      "by by ADP\n",
      "the the DET\n",
      "philosopher philosopher NOUN\n",
      "Harry Harry PROPN\n",
      "Frankfurt Frankfurt PROPN\n",
      ". . PUNCT\n",
      "\n",
      "Fifteen fifteen NUM\n",
      "years year NOUN\n",
      "after after ADP\n",
      "the the DET\n",
      "publication publication NOUN\n",
      "of of ADP\n",
      "this this DET\n",
      "surprise surprise NOUN\n",
      "bestseller bestseller NOUN\n",
      ", , PUNCT\n",
      "the the DET\n",
      "rapid rapid ADJ\n",
      "progress progress NOUN\n",
      "of of ADP\n",
      "research research NOUN\n",
      "on on ADP\n",
      "artificial artificial ADJ\n",
      "intelligence intelligence NOUN\n",
      "is be AUX\n",
      "forcing force VERB\n",
      "us we PRON\n",
      "to to PART\n",
      "reconsider reconsider VERB\n",
      "our our PRON\n",
      "conception conception NOUN\n",
      "of of ADP\n",
      "bullshit bullshit NOUN\n",
      "as as ADP\n",
      "a a DET\n",
      "hallmark hallmark NOUN\n",
      "of of ADP\n",
      "human human ADJ\n",
      "speech speech NOUN\n",
      ", , PUNCT\n",
      "with with ADP\n",
      "troubling troubling ADJ\n",
      "implications implication NOUN\n",
      ". . PUNCT\n",
      "\n",
      "What what PRON\n",
      "do do VERB\n",
      "philosophical philosophical ADJ\n",
      "reflections reflection NOUN\n",
      "on on ADP\n",
      "bullshit bullshit NOUN\n",
      "have have VERB\n",
      "to to PART\n",
      "do do VERB\n",
      "with with ADP\n",
      "algorithms algorithm NOUN\n",
      "? ? PUNCT\n",
      "\n",
      "As as ADP\n",
      "it it PRON\n",
      "turns turn VERB\n",
      "out out ADP\n",
      ", , PUNCT\n",
      "quite quite ADV\n",
      "a a DET\n",
      "lot lot NOUN\n",
      ". . PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents: # достаем предложения\n",
    "    for token in sent: # достаем токены\n",
    "        print(token.text, token.lemma_, token.pos_)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задачах вроде извлечения ключевых слов может пригодится вытащить из текста только noun phrases (по-русски это вроде называется именные группы существительного)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['the most salient features', 'our culture', 'so much bullshit', 'the opening words', 'the short book', 'Bullshit', 'the philosopher', 'Harry Frankfurt', 'the publication', 'this surprise bestseller', 'the rapid progress', 'research', 'artificial intelligence', 'us', 'our conception', 'bullshit', 'a hallmark', 'human speech', 'troubling implications', 'What', 'philosophical reflections', 'bullshit', 'algorithms', 'it']\n"
     ]
    }
   ],
   "source": [
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем пайплайн для немецкого языка\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = (\"Vor den Stadien habe ich bis jetzt zum Glück noch keine wüsten Szenen gesehen.\"\n",
    "        \"Vorstandschef Timotheus Höttges habe sich ausgesprochen optimistisch gezeigt, schrieb Analyst Robert Grindle in einer Studie vom Montag. \"\n",
    "        \"Während der dortigen Räterepublik war er nach dem Krieg in Künstlergruppen und Ausschüssen aktiv.\"\n",
    "        \"Welches Ergebnis die Diskussion auf EU-Ebene auch letztlich bringt, wichtig ist, dass die Preisentwicklung für die\"\n",
    "        \"Menschen verträglicher gestaltet wird“, so Gusenbauer.\"\n",
    "        \"Weitere Informationen unter www.schnippenburg.de sowie www.eisenzeithaus.de. Es gibt neue Nachrichten auf noz.de!\" \n",
    "        \"Jetzt die Startseite neu laden.\"\n",
    "        \"Der Initiative 'Zivilcourage', die sich jahrelang für das Denkmal in Form eines offenen \" \n",
    "        \"Der islamistischen Szene Thüringens wurden nach Angaben des Thüringer Innenministeriums \"\n",
    "        \"zuletzt etwa 125 Personen zugerechnet, der salafistischen Szene etwa 75 Personen.\"\n",
    "        \"Allerdings bestand er die EMV-Prüfung nicht, weil er Radios und DVB-T-Empfänger stört.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vor Vor ADP\n",
      "den der DET\n",
      "Stadien Stadium NOUN\n",
      "habe habe AUX\n",
      "ich ich PRON\n",
      "bis bis ADP\n",
      "jetzt jetzt ADV\n",
      "zum zum ADP\n",
      "Glück Glück NOUN\n",
      "noch noch ADV\n",
      "keine kein DET\n",
      "wüsten wüst ADJ\n",
      "Szenen Szene NOUN\n",
      "gesehen sehen VERB\n",
      ". . PUNCT\n",
      "\n",
      "Vorstandschef Vorstandschef NOUN\n",
      "Timotheus Timotheus PROPN\n",
      "Höttges Höttges PROPN\n",
      "habe habe AUX\n",
      "sich sich PRON\n",
      "ausgesprochen aussprechen ADV\n",
      "optimistisch optimistisch ADV\n",
      "gezeigt zeigen VERB\n",
      ", , PUNCT\n",
      "schrieb schreiben VERB\n",
      "Analyst Analyst NOUN\n",
      "Robert Robert PROPN\n",
      "Grindle Grindle PROPN\n",
      "in in ADP\n",
      "einer einer DET\n",
      "Studie Studie NOUN\n",
      "vom vom ADP\n",
      "Montag Montag NOUN\n",
      ". . PUNCT\n",
      "\n",
      "Während während ADP\n",
      "der der DET\n",
      "dortigen dortig ADJ\n",
      "Räterepublik Räterepublik NOUN\n",
      "war sein AUX\n",
      "er ich PRON\n",
      "nach nach ADP\n",
      "dem der DET\n",
      "Krieg Krieg NOUN\n",
      "in in ADP\n",
      "Künstlergruppen Künstlergruppen NOUN\n",
      "und und CCONJ\n",
      "Ausschüssen Ausschuß NOUN\n",
      "aktiv aktiv ADV\n",
      ". . PUNCT\n",
      "\n",
      "Welches welch DET\n",
      "Ergebnis Ergebnis NOUN\n",
      "die der DET\n",
      "Diskussion Diskussion NOUN\n",
      "auf auf ADP\n",
      "EU-Ebene EU-Ebene NOUN\n",
      "auch auch ADV\n",
      "letztlich letztlich ADV\n",
      "bringt bringen VERB\n",
      ", , PUNCT\n",
      "wichtig wichtig ADV\n",
      "ist sein AUX\n",
      ", , PUNCT\n",
      "dass dass SCONJ\n",
      "die der DET\n",
      "Preisentwicklung Preisentwicklung NOUN\n",
      "für für ADP\n",
      "dieMenschen dieMenschen NOUN\n",
      "verträglicher verträglich ADV\n",
      "gestaltet gestalten VERB\n",
      "wird werden AUX\n",
      "“ “ PUNCT\n",
      ", , PUNCT\n",
      "so so ADV\n",
      "Gusenbauer Gusenbauer NOUN\n",
      ". . PUNCT\n",
      "\n",
      "Weitere Weitere ADJ\n",
      "Informationen Information NOUN\n",
      "unter unter ADP\n",
      "www.schnippenburg.de www.schnippenburg.de NOUN\n",
      "sowie sowie CCONJ\n",
      "www.eisenzeithaus.de www.eisenzeithaus.de NOUN\n",
      ". . PUNCT\n",
      "\n",
      "Es ich PRON\n",
      "gibt geben VERB\n",
      "neue neue ADJ\n",
      "Nachrichten Nachricht NOUN\n",
      "auf auf ADP\n",
      "noz.de noz.de NOUN\n",
      "! ! PUNCT\n",
      "\n",
      "Jetzt Jetzt ADV\n",
      "die der DET\n",
      "Startseite Startseite NOUN\n",
      "neu neu ADV\n",
      "laden laden VERB\n",
      ". . PUNCT\n",
      "\n",
      "Der der DET\n",
      "Initiative Initiative NOUN\n",
      "' ' PUNCT\n",
      "Zivilcourage Zivilcourage NOUN\n",
      "' ' PUNCT\n",
      ", , PUNCT\n",
      "die der PRON\n",
      "sich sich PRON\n",
      "jahrelang jahrelang ADV\n",
      "für für ADP\n",
      "das der DET\n",
      "Denkmal Denkmal NOUN\n",
      "in in ADP\n",
      "Form Form NOUN\n",
      "eines ein DET\n",
      "offenen offen ADJ\n",
      "Der der DET\n",
      "islamistischen islamistischen ADJ\n",
      "Szene Szene NOUN\n",
      "Thüringens Thüringen PROPN\n",
      "wurden werden AUX\n",
      "nach nach ADP\n",
      "Angaben Angabe NOUN\n",
      "des der DET\n",
      "Thüringer Thüringer ADJ\n",
      "Innenministeriums Innenministerium NOUN\n",
      "zuletzt zuletzt ADV\n",
      "etwa etwa ADV\n",
      "125 125 NUM\n",
      "Personen Person NOUN\n",
      "zugerechnet zurechnen VERB\n",
      ", , PUNCT\n",
      "der der DET\n",
      "salafistischen salafistischen ADJ\n",
      "Szene Szene NOUN\n",
      "etwa etwa ADV\n",
      "75 75 NUM\n",
      "Personen Person NOUN\n",
      ". . PUNCT\n",
      "\n",
      "Allerdings Allerdings ADV\n",
      "bestand bestand VERB\n",
      "er ich PRON\n",
      "die der DET\n",
      "EMV-Prüfung EMV-Prüfung NOUN\n",
      "nicht nicht PART\n",
      ", , PUNCT\n",
      "weil weil SCONJ\n",
      "er ich PRON\n",
      "Radios Radio NOUN\n",
      "und und CCONJ\n",
      "DVB-T-Empfänger DVB-T-Empfänger NOUN\n",
      "stört stören VERB\n",
      ". . PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents: # достаем предложения\n",
    "    for token in sent: # достаем токены\n",
    "        print(token.text, token.lemma_, token.pos_)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['den Stadien', 'ich', 'Glück', 'noch keine wüsten Szenen', 'Vorstandschef Timotheus Höttges', 'sich', 'Analyst Robert Grindle', 'einer Studie', 'Montag', 'der dortigen Räterepublik', 'er', 'dem Krieg', 'Künstlergruppen', 'Ausschüssen', 'Welches Ergebnis', 'die Diskussion', 'EU-Ebene', 'die Preisentwicklung', 'dieMenschen', 'Weitere Informationen', 'www.schnippenburg.de', 'www.eisenzeithaus.de', 'neue Nachrichten', 'noz.de', 'Jetzt die Startseite', \"Der Initiative 'Zivilcourage\", 'die', 'sich', 'das Denkmal', 'Form', 'eines offenen Der islamistischen Szene', 'Thüringens', 'Angaben', 'des Thüringer Innenministeriums', 'zuletzt etwa 125 Personen', 'der salafistischen Szene', 'etwa 75 Personen', 'er', 'die EMV-Prüfung', 'er', 'Radios', 'DVB-T-Empfänger']\n"
     ]
    }
   ],
   "source": [
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поддержка русского языка в spacy тоже не так давно добавилась, но она не полная"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'ru_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mru_core_news_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/spacy/util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'ru_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"ru_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# возьмем любой текст\n",
    "text = \"ДАННОЕ СООБЩЕНИЕ (МАТЕРИАЛ) СОЗДАНО И (ИЛИ) РАСПРОСТРАНЕНО \"\\\n",
    "       \"ИНОСТРАННЫМ СРЕДСТВОМ МАССОВОЙ ИНФОРМАЦИИ, ВЫПОЛНЯЮЩИМ \"\\\n",
    "       \"ФУНКЦИИ ИНОСТРАННОГО АГЕНТА, И (ИЛИ) РОССИЙСКИМ ЮРИДИЧЕСКИМ ЛИЦОМ, \"\\\n",
    "       \"ВЫПОЛНЯЮЩИМ ФУНКЦИИ ИНОСТРАННОГО АГЕНТА\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ДАННОЕ ДАННОЕ PROPN\n",
      "СООБЩЕНИЕ СООБЩЕНИЕ PROPN\n",
      "( ( PUNCT\n",
      "МАТЕРИАЛ МАТЕРИАЛ PROPN\n",
      ") ) PUNCT\n",
      "СОЗДАНО СОЗДАНО PROPN\n",
      "И И PROPN\n",
      "( ( PUNCT\n",
      "ИЛИ ИЛИ PROPN\n",
      ") ) PUNCT\n",
      "РАСПРОСТРАНЕНО РАСПРОСТРАНЕНО PROPN\n",
      "ИНОСТРАННЫМ ИНОСТРАННЫМ PROPN\n",
      "СРЕДСТВОМ СРЕДСТВОМ PROPN\n",
      "МАССОВОЙ МАССОВОЙ VERB\n",
      "ИНФОРМАЦИИ ИНФОРМАЦИИ PROPN\n",
      ", , PUNCT\n",
      "ВЫПОЛНЯЮЩИМ ВЫПОЛНЯЮЩИМ PROPN\n",
      "ФУНКЦИИ ФУНКЦИИ NOUN\n",
      "ИНОСТРАННОГО иностранного NOUN\n",
      "АГЕНТА агента NOUN\n",
      ", , PUNCT\n",
      "И И PROPN\n",
      "( ( PUNCT\n",
      "ИЛИ ИЛИ PROPN\n",
      ") ) PUNCT\n",
      "РОССИЙСКИМ РОССИЙСКИМ VERB\n",
      "ЮРИДИЧЕСКИМ ЮРИДИЧЕСКИМ PROPN\n",
      "ЛИЦОМ ЛИЦОМ PROPN\n",
      ", , PUNCT\n",
      "ВЫПОЛНЯЮЩИМ ВЫПОЛНЯЮЩИМ PROPN\n",
      "ФУНКЦИИ ФУНКЦИИ NOUN\n",
      "ИНОСТРАННОГО иностранного NOUN\n",
      "АГЕНТА агента NOUN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "for sent in doc.sents: # достаем предложения\n",
    "    for token in sent: # достаем токены\n",
    "        print(token.text, token.lemma_, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['ДАННОЕ СООБЩЕНИЕ', 'МАТЕРИАЛ', 'ИНОСТРАННЫМ СРЕДСТВОМ', 'ВЫПОЛНЯЮЩИМ ФУНКЦИИ ИНОСТРАННОГО АГЕНТА', 'И', 'ИЛИ', 'ЮРИДИЧЕСКИМ', 'ЛИЦОМ', 'ВЫПОЛНЯЮЩИМ ФУНКЦИИ ИНОСТРАННОГО АГЕНТА']\n"
     ]
    }
   ],
   "source": [
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Удаление дубликатов\n",
    "\n",
    "В многих практических задачах требуется искать совпадающие тексты. Например, для определения плагиата, или для того, чтобы отобрать уникальные тексты для обучения. Сама по себе эта задача - тривиальная. Нужно просто сравнить тексты между собой. Даже если нужно найти не просто точно совпадающие тексты (дупликаты), а еще и похожие тексты. В этом случае можно заменить прямое сравнение какой-то метрикой (например, мерой Жаккара между множествами нграммов).\n",
    "\n",
    "Но и то и другое становится проблемой, когда количество текстов очень большое. Чтобы найти дубликаты нужно сравнить все тексты со всеми и это слишком много вычислений даже для самых простых методов.\n",
    "\n",
    "Для такой задачи стандартно применяется класс алгоритмов, которые называются Local Sensitive Hashing. Давайте попробуем разобраться как это работает.\n",
    "\n",
    "Тут три важных компоненты: шинглы, minhash и LSH (это все в целом называется LSH но и последний шаг тоже так называется, что немного запутанно)\n",
    "\n",
    "1) Шинглы - это просто куски текстов/документов какой-то длины. Мы бы скорее назвали это символьными нграммами.\n",
    "2) Minhash - это алгоритм, который позволяет рассчитать приблизительное расстояние Жаккара между множествами шинглов двух документов (https://en.wikipedia.org/wiki/MinHash). Это не единственный алгоритм, еще есть например SimHash (https://en.wikipedia.org/wiki/SimHash)\n",
    "3) LSH шаг это еще одна оптимизация для нахождения кандидатов в дубликаты\n",
    "\n",
    "Пройдемся по каждому из шагов и напишем простую реализацию на питоне"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs10660-021-09472-1/MediaObjects/10660_2021_9472_Fig2_HTML.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерировать шинглы очень просто. Единственный важный момент что в итоге нам нужны только уникальные шинглы (то есть множества а не списки)\n",
    "\n",
    "Длина шингла это параметр который можно настраивать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shingles(text, k=5):\n",
    "    \"\"\"генерирует список шинглов из строки\"\"\"\n",
    "    shingles = set()\n",
    "    for i in range(len(text) - k + 1):\n",
    "        shingle = text[i:i + k]\n",
    "        shingles.add(shingle)\n",
    "    return shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' для тес',\n",
       " ' строки ',\n",
       " 'для тест',\n",
       " 'ер строк',\n",
       " 'и для те',\n",
       " 'имер стр',\n",
       " 'ки для т',\n",
       " 'ля теста',\n",
       " 'мер стро',\n",
       " 'оки для ',\n",
       " 'пример с',\n",
       " 'р строки',\n",
       " 'ример ст',\n",
       " 'роки для',\n",
       " 'строки д',\n",
       " 'троки дл'}"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_shingles(\"пример строки для теста\", 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MinHash самая сложная часть этого алгоритма. \n",
    "\n",
    "Можете посмотреть одно или несколько из этих видео, чтобы разобрать поглубжe:  \n",
    "https://www.youtube.com/watch?v=e_SBq3s20M8  \n",
    "https://www.youtube.com/watch?v=96WOGPUgMfw  \n",
    "https://www.youtube.com/watch?v=R12splIFMOs  \n",
    "https://www.youtube.com/watch?v=bQAYY8INBxg \n",
    "\n",
    "\n",
    "\n",
    "Во-первых, нужно заменить шинглы на числа с помощью хеширования. Хэширование это маппинг элементов (например строк) в позиции в какой-то таблице. Позиция в таблице и есть хэш, обычно это просто число. Маппинг происходит с помощью какой-то функции и поэтому с помощью хеширования удобно проверять наличие - достаточно расчитать хэш объекта и посмотреть есть ли уже такой объект в таблице, сравнивать со всеми существующими объектами не нужно\n",
    "\n",
    "Словари и множества в питоне работают на хеш таблицах и поэтому проверить есть ли какой-то элемент в множестве гораздо быстрее чем в списке "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {str(i) for i in range(1000000)}\n",
    "l = [str(i) for i in range(1000000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 8.11 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\"8732323\" in d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.3 ms, sys: 354 µs, total: 18.7 ms\n",
      "Wall time: 19.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\"873232312\" in l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функций для хеширования очень много и это своя отдельная тема. Для минхэша нам пондобится генерировать много хэш функций, это можно сделать вот так - мы используем один алгоритм хэширования, но к строкам добавляем какой-то индекс, что меняет результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def hash_string(s):\n",
    "    \"\"\"хеширует строку и возвращает число\"\"\"\n",
    "    return int(hashlib.md5(s.encode('utf8')).hexdigest(), 16) \n",
    "\n",
    "def generate_hash_functions(k):\n",
    "    \"\"\"генерирует k хеш-функций добавляя индекс к строке\"\"\"\n",
    "\n",
    "    functions = []\n",
    "    for i in range(k):\n",
    "        functions.append(lambda x, i=i: hash_string(x + str(i)))\n",
    "    return functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sympy\n",
    "\n",
    "# sympy.isprime(5)\n",
    "\n",
    "# list(sympy.primerange(100000, 1100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52 µs, sys: 2 µs, total: 54 µs\n",
      "Wall time: 58.2 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17236"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "int(hashlib.md5(texts[220].encode('utf-8')).hexdigest(), 16) % 105337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9238"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# если вызвать эту функцию на одной строке несколько раз то результат будет одинаковый\n",
    "hash_string(\"пример\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем две функции чтобы генерировать разные результаты для одной строки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = generate_hash_functions(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69815"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions[0](\"пример\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49600"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functions[1](\"пример\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь самая сложная часть. Вместо хэшей нужно сгенерировать их сигнатуры, которые будут приближать меру Жаккара между множествами шинглов\n",
    "\n",
    "Этот алгоритм обычно объясняют вот такой схемой. \n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:913/1*4HbYE4-7DRfri7cfN0tN0A.png)\n",
    "\n",
    "В центре матрица шинглы на документы (input matrix). Такие же матрицы мы будем еще раз много строить для представления текстов мешком слов. Это разреженная матрица где 1 показывает это слово есть в документе, а 0 что нет. Каждый документ представляется вектором, который равен размеру всего словаря (то есть количеству уникальных слов/токенов/шинглов во всех документах). \n",
    "\n",
    "Такие вектора можно использовать для расчета метрики Жаккара (или для косиносной близости), но это все еще слишком дорого, потому что нужно сравнивать все со всем. Метрика Жаккара между двумя такими векторами равна количеству совпадений единиц на одной позиции, поделить на количество ненулевых позиций в обоих векторах. Если в конкретной позиции у обоих векторов стоят 1, значит слово/шингл есть в обоих документах. А если 1 стоит только в одном векторе, а в другом 0 - то значит пересечения между ними нет. Нули тут никак не учитываются. Это то же самое, что рассчитать пересечение множеств и поделить на объединение, только в векторном формате.\n",
    "\n",
    "MinHash это такое преобразование этой матрицы шинглы на документы, которое приближает меру жаккара. Преобразование заключается в том, что генерируются перестановки для этой матрицы и в каждой перестановке для каждого документа находится первая ненулевая позиция и ее индекс записывается в новую матрицу (сигнатуру). Перестановки тут обозначены цветными векторами. Если вы посмотрите на голубой вектор, то первой теперь является строчка, которая была 6-ой в изначальной матрице. В ней у первого документа уже стоит 1, поэтому для первого документа значение сигнатуры - 1 (матрица справа). Для второго документа стоит 0, поэтому нужно смотреть следующий вектор - 4-ый в изначальной матрице. В нем уже стоит 1 во втором документе, поэтому значение сигнатуры будет 2. У третьего документа 1 потому что в первом векторе уже стоит 1, а в 4 такая же ситуация как и во втором. (Проверьте что вы понимаете как получились желтая и коричневая сигнатура)\n",
    "\n",
    "Теперь сигнатуры документов можно использовать для расчета Жаккара так же как и обычные вектора. Если позиции сходятся, то документы схожи. Это гораздо эффективнее потому что сигнатуры гораздо меньше изначальных векторов и большую часть вычисление делать теперь не нужно.\n",
    "\n",
    "Каждая сигнатура может не точно передавать общую близость изначальных векторов, но если сделать много таких перестановок, то схожесть по сигнатурам будет очень близка схожести по изначальным векторам.\n",
    "Такой подход приближает метрику Жаккара, потому что если в обоих документах много совпадений шинглов (1 и 1 в обоих на одной позиции), то они часто будут генерировать один и тот же индекс для сигнатуры. Чем больше несовпадений шинглов в документах (0 в одном и 1 в другом), тем чаще в сигнатуре будут разные индексы и соответственно метрика будет низкая. Получается даже так, что вероятность при случайной перестановке получить 1 и 1 в обоих векторах равна метрике Жаккара между изначальными документами! (количество случаев где оба значения 1, поделить на количество ненулевых позиции в обоих векторах). Это только вероятность и каждая отдельная перестановка может давать другой результат, но повторяя их много раз, в среднем мы получим исходное расстояние. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В коде это выглядит немного по-другому, но принцип точно такой же! Просто перестановки создавать сложно и делать полную матрицу тоже не нужно. Можно просто вместо перестановок делать разные хеш функции (хеш функция это просто маппинг в число, то есть в итоге мы получаем позицию). Если применить хеш функцию ко всем шинглам в документе и взять минимальное значение, то это то же самое что сделать перестановку и взять первое ненулевое значение вектора выше!\n",
    "\n",
    "Поэтому код очень простой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_minhash_signature(shingles, hash_funcs):\n",
    "    \"\"\"вычисляет minhash-сигнатуру для списка шинглов\"\"\"\n",
    "    signature = []\n",
    "    for hash_func in hash_funcs:\n",
    "        min_hash = min(hash_func(shingle) for shingle in shingles)\n",
    "        signature.append(min_hash)\n",
    "    return signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4888, 8891, 9010, 8604, 401]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# числа тут гораздо больше чем в примере\n",
    "# просто неудобно показывать такие большие индексы для примера\n",
    "# но суть от этого не меняется - это просто числа индексы\n",
    "shingles = get_shingles(\"пример строки для теста\", 3)\n",
    "hash_funcs = generate_hash_functions(5)\n",
    "compute_minhash_signature(shingles, hash_funcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последний элемент LSH просто разрезает сигнатуры на кусочки и группирует документы по совпадению этих кусочков. Сгруппированные документы уже являются кандидатами для расчета нормальной близости, потому что они скорее всего будут дубликатами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsh(signatures, bands):\n",
    "    \"\"\"Разрезает сигнатуры на куски (bands), и группирует индексы сигнатур по совпадению кусков\"\"\"\n",
    "    buckets = defaultdict(list)\n",
    "    band_length = len(signatures[0]) // bands\n",
    "    \n",
    "    for idx, sig in tqdm(enumerate(signatures)):\n",
    "        for b in range(0, bands, band_length):\n",
    "            start = b\n",
    "            end = start + band_length\n",
    "            band = tuple(sig[start:end])\n",
    "            buckets[band].append(idx)\n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[47768649540959415773458792523333048870,\n",
       " 7684655364199927409860563777974756678,\n",
       " 15083700868336705187602456618714281847,\n",
       " 23751230482080805943145997883796637502,\n",
       " 6444901249487694733329551606909628849,\n",
       " 3440871066400609071968054431171020532]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {(4888, 8891, 9010): [0],\n",
       "             (9958, 824, 727): [1],\n",
       "             (3244, 8891, 9010): [2]})"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_funcs = generate_hash_functions(6)\n",
    "\n",
    "shingles_1 = get_shingles(\"пример строки для теста\", 3)\n",
    "signature_1 = compute_minhash_signature(shingles_1, hash_funcs)\n",
    "\n",
    "shingles_2 = get_shingles(\"совершенно другой текст\", 3)\n",
    "signature_2 = compute_minhash_signature(shingles_2, hash_funcs)\n",
    "\n",
    "shingles_3 = get_shingles(\"пример похожей строки для теста\", 3)\n",
    "signature_3 = compute_minhash_signature(shingles_3, hash_funcs)\n",
    "\n",
    "lsh([signature_1, signature_2, signature_3], 2)\n",
    "\n",
    "# 0, 2 сгруппировались по совпадению кусочков сигнатур длинной 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_strings(strings_list, k=5, num_hashes=100, bands=20):\n",
    "    \"\"\"Finds similar strings using MinHash and LSH.\"\"\"\n",
    "    hash_funcs = generate_hash_functions(num_hashes)\n",
    "    signatures = []\n",
    "    shingles_list = []\n",
    "\n",
    "    # каждый текст обрабатывается отдельно\n",
    "    # находятся шинглы и рассчитываются сигнатуры \n",
    "    for string in tqdm(strings_list):\n",
    "        shingles = get_shingles(string, k)\n",
    "        shingles_list.append(shingles)\n",
    "        signature = compute_minhash_signature(shingles, hash_funcs)\n",
    "        signatures.append(signature)\n",
    "\n",
    "    # вычисляются кандидаты по кускам сигнатур\n",
    "    buckets = lsh(signatures, bands)\n",
    "    candidates = set()\n",
    "    for bucket in buckets.values():\n",
    "        if len(bucket) > 1:\n",
    "            for i in bucket:\n",
    "                for j in bucket:\n",
    "                    if i < j:\n",
    "                        candidates.add((i, j))\n",
    "\n",
    "\n",
    "    return candidates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc55c82d64034d639dd077df32dc1acc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b14defe47fa49b4b7393ea96bfb9416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage\n",
    "strings_list = [\n",
    "    \"This is a sample string for testing\",\n",
    "    \"This is a sample string for testing\",\n",
    "    \"This is a simple string for testing\",\n",
    "    \"Completely different string here\",\n",
    "    \"Another different string for testing purposes\",\n",
    "    \"This is a sample string for testing and more\"\n",
    "]\n",
    "\n",
    "candidates = find_similar_strings(strings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1), (0, 5), (1, 5)}"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы быть уверенным в метрике можно рассчитать полное расстояние между кандидатами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jaccard(x, y):\n",
    "    x_sh = get_shingles(x, 10)\n",
    "    y_sh = get_shingles(y, 10)\n",
    "\n",
    "    return len(x_sh & y_sh) / len(x_sh | y_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.7428571428571429, 0.7428571428571429)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(get_jaccard(strings_list[0], strings_list[1]), \n",
    "get_jaccard(strings_list[0], strings_list[5]), \n",
    "get_jaccard(strings_list[1], strings_list[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка текста из html\n",
    "\n",
    "Вообще все сайты разные и доставать текст из каждого нужно по-своему, но многие вещи и паттерны повторяются и поэтому можно извлекать большинство текстов уже готовыми скриптами и библиотеками (например есть trafilatura)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install trafilatura lxml_html_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Leader spotlight: Erin Spiceland\\nWe’re spending Women’s History Month with women leaders who are making history every day in the tech community. Read more about Erin Spiceland: Software Engineer at SpaceX.\\nEvery March we recognize the women who have shaped history—and now, we’re taking a look forward. From driving software development in large companies to maintaining thriving open source communities, we’re spending Women’s History Month with women leaders who are making history every day in the tech community. Erin Spiceland is a Software Engineer for SpaceX. Born and raised in rural south Georgia, she is a Choctaw and Chickasaw mother of two now living in downtown Los Angeles. Erin didn’t finish college—she’s a predominantly self-taught software engineer. In her spare time, she makes handmade Native American beadwork and regalia and attends powwows.\\nHow would you summarize your career (so far) in a single sentence?\\nMy career has been a winding road through periods of stimulation and health as well as periods of personal misery. During it all, I’ve learned a variety of programming languages and technologies while working on a diverse array of products and services. I’m a domestic abuse survivor and a Choctaw bisexual polyamorous woman. I’m so proud of myself that I made it this far considering where I came from.\\nWhat was your first job in tech like?\\nIn 2007, I had a three-year-old daughter and I was trying to finish my computer science degree one class at a time, all while keeping my house and family running smoothly. I found the math classes exciting and quickly finished my math minor, leaving only computer science classes. I was looking at about five years before I would graduate. Then, my husband at the time recommended me for an entry software developer position at a telecom and digital communications company.\\nWhen faced with the choice between an expensive computer science degree and getting paid to do what I loved, I dropped out of college and accepted the job. I was hired to work on internal tooling, and eventually, products. I did a lot of development on product front-ends, embedded network devices, and a distributed platform-as-a-service. I learned Java/JSP, Python, JavaScript/CSS, Node.js, as well as MySQL, PostgreSQL, and distributed systems architecture. It was an intense experience that required a lot of self-teaching, asking others for help, and daycare, but it set me up for my later successes.\\nWhat does leadership mean to you in your current role?\\n“Leadership is about enabling those below, above, and around you to be at their healthiest and most effective so that all of you can accurately understand your surroundings, make effective plans and goals for the future, and achieve those goals.”\\nI appreciate and admire technical, effective leaders who care for their reports as humans, not as lines on a burndown chart, and forego heavy-handed direction in favor of communication and mutual dialogue. I think it’s as important for a leader to concern herself with her coworkers’ personal well-being as it is for her to direct their performance.\\nWhat’s the biggest career risk you’ve ever taken? What did you learn from that experience?\\nLast year I took a pay cut to move from a safe, easy job where I had security to work in a language I hadn’t seen in years and with systems more complicated than anything I’d worked with before. I moved from a place where I had a huge four bedroom house to a studio apartment that was twice the price. I moved away from my children, of who I share custody with my ex-husband. We fly across the U.S. to see each other now. I miss my children every day. However, I get to be a wonderful role model for them.\\n“I get to show my children that a Native woman who grew up in poverty, lost her mother and her culture, and who didn’t finish college can learn, grow, and build whatever career and life she wants.”\\nWhat are you looking forward to next?\\nI can’t wait to wake up every day with my partner who loves me so much. I’m looking forward to showing my children exactly how far they can go. I’m excited to keep exploring Los Angeles.\\n“I expect to learn so much more about software and about life, and I want to experience everything.”\\nWant to know more about Erin Spiceland? Follow them on GitHub or Twitter.\\nWant to learn more about featured leaders for Women’s History Month? Read about:\\n- Laura Frank Tacho, Director of Engineering at CloudBees\\n- Rachel White, Developer Experience Lead at American Express\\n- Kathy Pham, Computer Scientist and Product Leader at Mozilla and Harvard\\n- Heidy Khlaaf, Research Consultant at Adelard LLP\\nCheck back in soon—we’ll be adding new interviews weekly throughout March.\\nWritten by\\nRelated posts\\nBoost your CLI skills with GitHub Copilot\\nWant to know how to take your terminal skills to the next level? Whether you’re starting out, or looking for more advanced commands, GitHub Copilot can help us explain and suggest the commands we are looking for.\\nBeginner’s guide to GitHub: Setting up and securing your profile\\nAs part of the GitHub for Beginners guide, learn how to improve the security of your profile and create a profile README. This will let you give your GitHub account a little more personality.\\nBeginner’s guide to GitHub: Merging a pull request\\nAs part of the GitHub for Beginners guide, learn how to merge pull requests. This will enable you to resolve conflicts when they arise.'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import trafilatura\n",
    "downloaded = trafilatura.fetch_url('https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/')\n",
    "trafilatura.extract(downloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Определение языка (fastext language classifier)\n",
    "\n",
    "Как и в любой задаче, в определении языка есть свои особенности и сложности, в которые можно закопаться очень глубоко. Но глобально - это стандартная задача классификации и мы будем разбирать ее еще много раз. Единственное, что хотелось сказать - это fasttext language classifier. Это очень распространенный предобученный классификатор языков, который часто используется в статьях (в статье про FineWeb он тоже упоминается).\n",
    "Больше информации про этот классификатор - https://fasttext.cc/docs/en/language-identification.html (там же есть ссылки на скачивание модели)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.load_model('lid.176.ftz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__ru',), array([0.9825241]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# классификатор выдает самый вероятный язык и саму вероятность\n",
    "model.predict('Язык предложения')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
