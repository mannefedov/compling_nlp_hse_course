{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13efba76",
   "metadata": {
    "id": "13efba76"
   },
   "source": [
    "# Дисклеймер\n",
    "Эту тетрадку нужно запускать в колабе или в vast.ai. Не мучатесь с установкой библиотек и с обучением на cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "901a75c6-03c0-4b56-afdb-596c7f49421b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 19 10:12:14 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.120                Driver Version: 550.120        CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4080        On  |   00000000:81:00.0 Off |                  N/A |\n",
      "|  0%   44C    P8             26W /  320W |       5MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d650e9eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "d650e9eb",
    "outputId": "23aa3a53-acaa-4554-a6be-eec20a833401",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Collecting torch==2.5.1\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torch-2.5.1%2Bcu124-cp310-cp310-linux_x86_64.whl (908.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision==0.20.1\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torchvision-0.20.1%2Bcu124-cp310-cp310-linux_x86_64.whl (7.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio==2.5.1\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torchaudio-2.5.1%2Bcu124-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.5.1) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.1) (4.9.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.5.1) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.5.1) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.5.1) (2024.2.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.4.127 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.1.0 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sympy==1.13.1 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision==0.20.1) (1.26.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision==0.20.1) (10.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy==1.13.1->torch==2.5.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.5.1) (2.1.3)\n",
      "Installing collected packages: triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.2.0\n",
      "    Uninstalling triton-2.2.0:\n",
      "      Successfully uninstalled triton-2.2.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.1\n",
      "    Uninstalling torch-2.2.1:\n",
      "      Successfully uninstalled torch-2.2.1\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.17.1\n",
      "    Uninstalling torchvision-0.17.1:\n",
      "      Successfully uninstalled torchvision-0.17.1\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.2.1\n",
      "    Uninstalling torchaudio-2.2.1:\n",
      "      Successfully uninstalled torchaudio-2.2.1\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.5.1+cu124 torchaudio-2.5.1+cu124 torchvision-0.20.1+cu124 triton-3.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install tokenizers matplotlib scikit-learn\n",
    "!pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n",
    "# CPU only\n",
    "# !pip install torchtune torchao\n",
    "# !pip install --upgrade 'optree>=0.13.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d338ab5-79fd-43f3-9c8d-ebb6a67f7f9b",
   "metadata": {
    "id": "8d338ab5-79fd-43f3-9c8d-ebb6a67f7f9b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b03d3b-5816-4830-b69e-f3ea4a826e17",
   "metadata": {
    "id": "78b03d3b-5816-4830-b69e-f3ea4a826e17"
   },
   "source": [
    "Помимо самих трансформеров давайте также попробуем сервис для отслеживания экспериментов W & B (weights and biases).\n",
    "До этого мы обходились просто выводом метрик в тетрадке, но это не серьезно. Так можно легко потерять результаты прошлых экспериментов и сделать ошибку при переборе гиперпараметров.\n",
    "W&B не единственный такой сервис, но он бесплатно предоставляет облачное хранилище и визуализацию, поэтому попробуем его.\n",
    "Чтобы залогиниться в w&b в тетрадке, вам нужно пойти на сайт wandb.ai и залогиниться там, а потом создать проект и скопировать ключ в ячейку ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf913f78-c71a-4dcf-abf2-a0cc12daeef1",
   "metadata": {
    "id": "bf913f78-c71a-4dcf-abf2-a0cc12daeef1"
   },
   "outputs": [],
   "source": [
    "# !wandb login KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879d23c5-55a9-493c-a1e8-8798f0969bc7",
   "metadata": {
    "id": "879d23c5-55a9-493c-a1e8-8798f0969bc7"
   },
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035afaed-45ca-4dee-a200-67357b0f4002",
   "metadata": {
    "id": "035afaed-45ca-4dee-a200-67357b0f4002"
   },
   "outputs": [],
   "source": [
    "# самый простой пример инициализации эксперимента (run)\n",
    "run = wandb.init(\n",
    "    project=\"course\",\n",
    "    name=\"test_run\",\n",
    "    # в конфиг можно писать все что угодно\n",
    "    config={\n",
    "        \"test\": True\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a48668-cb5d-4475-955a-16e61d2bb3ff",
   "metadata": {
    "id": "74a48668-cb5d-4475-955a-16e61d2bb3ff"
   },
   "outputs": [],
   "source": [
    "# далее можно логировать метрики (один или много раз)\n",
    "wandb.log({\"accuracy\": 1.0, \"loss\": 0.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505ef7f6-0bb9-41ad-9903-d45bfd7804ce",
   "metadata": {
    "id": "505ef7f6-0bb9-41ad-9903-d45bfd7804ce"
   },
   "outputs": [],
   "source": [
    "# так можно закончить эксперимент\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e384bd5c",
   "metadata": {
    "id": "e384bd5c"
   },
   "source": [
    "# Encoder-Decoder Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69baacc",
   "metadata": {
    "id": "a69baacc"
   },
   "source": [
    "Это уже 3-й семинар про трансформеры и только сейчас мы попробуем сделать модель, которая изначально и была описана в Attention is all you need. Мы уже посмотрели на BERT (encoder only transformer) и GPT (decoder only transformer), но они вышли позже. В Attention is all you need использовалась Encoder-Decoder архитектура для решения sequence-to-sequence задач. Давайте попробуем собрать такую модель.\n",
    "В этот посмотрим на готовые трансформерные классы в torch, чтобы использовать побольше готового и не писать все с нуля каждый раз."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c794a6",
   "metadata": {
    "id": "46c794a6"
   },
   "source": [
    "Будем обучать модель на задаче машинного перевода (самая классическая проблема в NLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "947b3313",
   "metadata": {
    "id": "947b3313"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers import decoders\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "from torchtune.modules import RotaryPositionalEmbeddings\n",
    "from torch.nn import Transformer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163b41c9-262a-4b31-b75e-999d5d6a55cc",
   "metadata": {
    "id": "163b41c9-262a-4b31-b75e-999d5d6a55cc"
   },
   "source": [
    "Данные взяты вот отсюда - https://opus.nlpl.eu/opus-100.php  \n",
    "Помимо en-ru пары там можно найти много других параллельных корпусов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "307b759c-fee4-41e8-8b90-0355911abd4d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "307b759c-fee4-41e8-8b90-0355911abd4d",
    "outputId": "6f8e87a6-abfa-415f-f128-f93b9c8c4193"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-19 10:15:35--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\n",
      "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
      "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 121340806 (116M)\n",
      "Saving to: ‘opus.en-ru-train.ru’\n",
      "\n",
      "opus.en-ru-train.ru 100%[===================>] 115.72M  5.89MB/s    in 38s     \n",
      "\n",
      "2025-03-19 10:16:13 (3.08 MB/s) - ‘opus.en-ru-train.ru’ saved [121340806/121340806]\n",
      "\n",
      "--2025-03-19 10:16:14--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\n",
      "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
      "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 67760131 (65M)\n",
      "Saving to: ‘opus.en-ru-train.en’\n",
      "\n",
      "opus.en-ru-train.en 100%[===================>]  64.62M  3.62MB/s    in 19s     \n",
      "\n",
      "2025-03-19 10:16:34 (3.34 MB/s) - ‘opus.en-ru-train.en’ saved [67760131/67760131]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\n",
    "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\n",
    "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.ru\n",
    "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "415f5ed2",
   "metadata": {
    "id": "415f5ed2"
   },
   "outputs": [],
   "source": [
    "# в русскоязычных данных есть \\xa0 вместо пробелов, он может некорректно обрабатываться токенизатором\n",
    "text = open('opus.en-ru-train.ru').read().replace('\\xa0', ' ')\n",
    "f = open('opus.en-ru-train.ru', 'w')\n",
    "f.write(text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e110ff04",
   "metadata": {
    "id": "e110ff04"
   },
   "outputs": [],
   "source": [
    "en_sents = open('opus.en-ru-train.en').read().splitlines()\n",
    "ru_sents = open('opus.en-ru-train.ru').read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c009c96e",
   "metadata": {
    "id": "c009c96e"
   },
   "source": [
    "Примеры перевода с английского на русский. Можно увидеть, что тексты достаточно разнообразные и часто неформальные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eb9b498",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0eb9b498",
    "outputId": "51b003ef-a0f9-4660-9bf5-ed0ea1f478b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Yeah, that's not exactly...\", 'Да, но не совсем...'),\n",
       " ('!', '!'),\n",
       " ('The schedule below is tentative; up-to-date information can be obtained at www.un.org/News/ossg/conf.htm.',\n",
       "  'Приводимое ниже расписание является предварительным; с самой последней информацией можно ознакомиться в Интернете по адресу www.un.org/News/ossg/conf.htm.'),\n",
       " ('But for now,',\n",
       "  'Но сейчас ...я вверяю вам удостовериться, что шотландцы приуменьшат'),\n",
       " (\"He'd been out there a few weeks or so.\",\n",
       "  'Они тусовались там несколько недель.'),\n",
       " (\"It'll make you feel better.\", 'Вам станет лучше.'),\n",
       " ('Come in!', 'Войдите.'),\n",
       " ('Do the math.', 'Давай, догадывайся.'),\n",
       " ('- Jenna?', '- Дженна?'),\n",
       " ('My cheekbones and beckoning pelvis already have a certain \"hello sailor\" quality to them.',\n",
       "  'Мои скулы и манящий таз уже им подают сигнал \"Привет, Матрос\"')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(en_sents[:10], ru_sents[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39921c4",
   "metadata": {
    "id": "c39921c4"
   },
   "source": [
    "Как обычно нам нужен токенизатор, а точнее даже 2 - под каждый язык. Можно совместить все в один токенизатор и даже иметь одну общую матрицу с эмбедингами в encoder и decoder, но для простоты мы их разделим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b79b4da",
   "metadata": {
    "id": "4b79b4da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer_en = Tokenizer(BPE())\n",
    "tokenizer_en.pre_tokenizer = Whitespace()\n",
    "# в encoder нам не нужно обозначать начало и конец поэтому единственный доп токен это паддинг\n",
    "trainer_en = BpeTrainer(special_tokens=[\"[PAD]\"], end_of_word_suffix='</w>')\n",
    "tokenizer_en.train(files=[\"opus.en-ru-train.en\"], trainer=trainer_en)\n",
    "\n",
    "tokenizer_ru = Tokenizer(BPE())\n",
    "tokenizer_ru.pre_tokenizer = Whitespace()\n",
    "# в декодере добавим теги начала и конца для корректной генерации\n",
    "trainer_ru = BpeTrainer(special_tokens=[\"[PAD]\", \"[BOS]\", \"[EOS]\"], end_of_word_suffix='</w>')\n",
    "tokenizer_ru.train(files=[\"opus.en-ru-train.ru\"], trainer=trainer_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "009e0125-67df-4a3e-ab9e-26d1f78183e4",
   "metadata": {
    "id": "009e0125-67df-4a3e-ab9e-26d1f78183e4"
   },
   "outputs": [],
   "source": [
    "tokenizer_en.decoder = decoders.BPEDecoder()\n",
    "tokenizer_ru.decoder = decoders.BPEDecoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b56d3b",
   "metadata": {
    "id": "f1b56d3b"
   },
   "source": [
    "Токенизатор - это неотъемлимая часть модели, поэтому не забывайте сохранять токенизатор вместе с моделью. Если вы забудете про это и переобучите токенизатор, то индексы токенов разойдутся и веса модели будут бесполезны."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dd90665",
   "metadata": {
    "id": "0dd90665"
   },
   "outputs": [],
   "source": [
    "# раскоментируйте эту ячейку при обучении токенизатора\n",
    "# а потом снова закоментируйте чтобы при перезапуске не перезаписать токенизаторы\n",
    "tokenizer_en.save('tokenizer_en')\n",
    "tokenizer_ru.save('tokenizer_ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e0f7f77",
   "metadata": {
    "id": "0e0f7f77"
   },
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer.from_file(\"tokenizer_en\")\n",
    "tokenizer_ru = Tokenizer.from_file(\"tokenizer_ru\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9fecf3",
   "metadata": {
    "id": "af9fecf3"
   },
   "source": [
    "Переводим текст в индексы вот таким образом.\n",
    "\n",
    "В начало русских текстов добавляем токен '[BOS]', а в конец '[EOS]'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc003758",
   "metadata": {
    "id": "dc003758"
   },
   "outputs": [],
   "source": [
    "def encode(text, tokenizer, max_len, encoder=False):\n",
    "    if encoder:\n",
    "        return tokenizer.encode(text).ids[:max_len]\n",
    "    else:\n",
    "        return [tokenizer.token_to_id('[BOS]')] + tokenizer.encode(text).ids[:max_len] + [tokenizer.token_to_id('[EOS]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96920fdc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96920fdc",
    "outputId": "b064993d-fa27-42d2-a960-0d623cf18659"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# важно следить чтобы индекс паддинга совпадал в токенизаторе с value в pad_sequences\n",
    "# у нас это в любом случае ноль но лучше safe than sorry\n",
    "PAD_IDX = tokenizer_ru.token_to_id('[PAD]')\n",
    "PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cc0a376",
   "metadata": {
    "id": "5cc0a376"
   },
   "outputs": [],
   "source": [
    "# ограничимся длинной в 47 и 48 (разные чтобы показать что в seq2seq не нужна одинаковая длина)\n",
    "# отличаться на 1 они тоже не должна, длины могут быть любые\n",
    "max_len_en, max_len_ru = 47, 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fc2dae1",
   "metadata": {
    "id": "7fc2dae1"
   },
   "outputs": [],
   "source": [
    "X_en = [encode(t, tokenizer_en, max_len_en, encoder=True) for t in en_sents]\n",
    "X_ru = [encode(t, tokenizer_ru, max_len_ru) for t in ru_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cc48199-c1d6-4217-898a-c12a244779c6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5cc48199-c1d6-4217-898a-c12a244779c6",
    "outputId": "aeecc747-b802-4957-eeab-837d4cac3e2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 1000000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# миллион примеров\n",
    "len(X_en), len(X_ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7655a4ea",
   "metadata": {
    "id": "7655a4ea"
   },
   "source": [
    "Паддинг внутри класса для датасета. Еще обратите внимание, что тут не стоит параметр batch_first=True как раньше\n",
    "\n",
    "В торче принято, что размерность батча идет в конце и пример кода с трансформером расчитан на это. Конечно можно поменять сам код модели, но это сложнее, чем просто изменить тензор с данными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7634853b",
   "metadata": {
    "id": "7634853b"
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, texts_en, texts_ru):\n",
    "        self.texts_en = [torch.LongTensor(sent) for sent in texts_en]\n",
    "        self.texts_en = torch.nn.utils.rnn.pad_sequence(self.texts_en, batch_first=True, padding_value=PAD_IDX)\n",
    "\n",
    "        self.texts_ru = [torch.LongTensor(sent) for sent in texts_ru]\n",
    "        self.texts_ru = torch.nn.utils.rnn.pad_sequence(self.texts_ru, batch_first=True, padding_value=PAD_IDX)\n",
    "\n",
    "        self.length = len(texts_en)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        ids_en = self.texts_en[index]\n",
    "        ids_ru = self.texts_ru[index]\n",
    "\n",
    "        return ids_en, ids_ru"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec889a8d",
   "metadata": {
    "id": "ec889a8d"
   },
   "source": [
    "Разбиваем на трейн и тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c9eaf09",
   "metadata": {
    "id": "9c9eaf09"
   },
   "outputs": [],
   "source": [
    "X_en_train, X_en_valid, X_ru_train, X_ru_valid = train_test_split(X_en, X_ru, test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bb0e70",
   "metadata": {
    "id": "32bb0e70"
   },
   "source": [
    "# Код трансформера\n",
    "\n",
    "Сначала попробуем `nn.MultiheadAttention`, который реализует механизм внимания. Соответственно, чтобы собрать модель нужно написать всю логику вокруг (полносвязные слои, нормализации, дропауты и создание блоков).\n",
    "\n",
    "В encoder-decoder архитектуре два типа внимания - self-attention и cross-attention. Оба реализуются через MultiheadAttention и работают практически идентично. Единственное отличие - что является исходными векторами, к которым применяется query, key, value преобразование. В self-attention все исходные значения берутся из одного и того же текста. В cross-attention в query подаются эмбединги одного текста (в нашем случае русского), а в key, value - эмбединги другого текста (в нашем случае английского).\n",
    "\n",
    "Внутри nn.MultiheadAttention уже реализована логика превращения изначальных эмбедингов в query, key, value вектора, поэтому в этой слой нужно передать только сами эмбединги, к которым будет применено это преобразование.   \n",
    "Обратите внимание на вызов self-attention ниже - `self.self_attn(src, src, src, ...)`. Изначальные эмбединги дублируются и передаются как позиционные аргументы. Можно еще представить это как `self.self_attn(query=src, key=src, value=src, ...)`.  \n",
    "Вызов cross-attention выглядит вот так - `self.cross_attn(tgt, memory, memory, ...)` (или другими словами в query идет закодированный русский текст, а в key и value - значения, которые вернул encoder).\n",
    "\n",
    "Логика разделения на головы (heads) тоже спрятана внутри MultiheadAttention. Она состоит в том, что исходные вектора разрезаются на равные куски и внимание рассчитывается между этими кусочками.  \n",
    "\n",
    "Наверное самое сложное здесь - это маскирование. По умолчанию MultiheadAttention никак не ограничивает коммуникацию между токенами. Однако в decoder нам нужно, чтобы каждый токен смотрел только на предыдущие. Поэтому в decoder нам нужно передать треугольную маску, которая применится к attention scores (скалярное произведение query и key векторов) и для каждого токена занулит внимание к будущим токенам.\n",
    "Также мы используем padding для выравнивания длин текстов, чтобы легко представить их как тензоры. Паддинг токен внутри модели ведет себя также как и другие токены - ему будет сопоставлен эмбединг и соответственно он будет участвовать в расчете внимания. Чтобы исключить его из расчетов нужно передать еще одну маску. У английского и русского текстов будут свои маски (так как длины разные). В cross-attention будут использоваться сразу обе маски, так как эмбединги происходят из обоих текстов.\n",
    "\n",
    "Еще один важный элемент - это RotaryPositionalEmbeddings. Это еще один способ делать позиционной кодирование. Сейчас это наиболее широко применяемый метод. Подробнее про его устройство можно почитать вот тут - https://medium.com/ai-insights-cobet/rotary-positional-embeddings-a-detailed-look-and-comprehensive-understanding-4ff66a874d83\n",
    "Если коротко, то он состоит в том, чтобы применить трасформацию к изначальным эмбедингам в зависимости от их позиции.\n",
    "RotaryPositionalEmbeddings в торче ожидает на вход эмбединги уже разделенные на куски (heads), поэтому в коде можно увидеть такое преобразование - `self.positional_encoding(src_embedded.view(B, S, self.num_heads, E // self.num_heads)).view(B, S, E)`\n",
    "Сначала эмбединги режутся на куски, к ним применяется позиционное кодирование и затем все возвращается к исходным размерам (куски соединяются обратно)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aca04f07-728d-41e9-8747-add7dfc84cfb",
   "metadata": {
    "id": "aca04f07-728d-41e9-8747-add7dfc84cfb"
   },
   "outputs": [],
   "source": [
    "# для encoder и decoder создается свой класс\n",
    "# это сделано для того чтобы можно было легко задать количество слоев как гиперпараметр\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        # здесь нормализация применяется после attention (как в оригинальной статье)\n",
    "        # сейчас чаще используют пре-нормализацию\n",
    "        src2, _ = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask) # mha\n",
    "        src = self.norm1(src + self.dropout(src2)) # norm + residual connection\n",
    "        src2 = self.ff(src) # ffd\n",
    "        src = self.norm2(src + self.dropout(src2)) # norm + residual connection\n",
    "\n",
    "        return src\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        tgt2, _ = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask) # self mha\n",
    "        tgt = self.norm1(tgt + self.dropout(tgt2)) # norm + residual connection\n",
    "\n",
    "        tgt2, _ = self.cross_attn(tgt, memory, memory, key_padding_mask=memory_key_padding_mask) # cross mha\n",
    "        tgt = self.norm2(tgt + self.dropout(tgt2)) # norm + residual connection\n",
    "\n",
    "        tgt2 = self.ff(tgt) # ffd\n",
    "        tgt = self.norm3(tgt + self.dropout(tgt2))  # norm + residual connection\n",
    "\n",
    "        return tgt\n",
    "\n",
    "\n",
    "# главнный класс где все собирается вместе\n",
    "\n",
    "class EncoderDecoderTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size_enc, vocab_size_dec, embed_dim, num_heads, ff_dim, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.embedding_enc = nn.Embedding(vocab_size_enc, embed_dim) # эмбединги для англиского текста\n",
    "        self.embedding_dec = nn.Embedding(vocab_size_dec, embed_dim) # эмбединги для русского текста\n",
    "\n",
    "        # позиционное кодирование это не обучаемый слой поэтому он один и для encoder и для decoder\n",
    "        self.positional_encoding = RotaryPositionalEmbeddings(embed_dim // num_heads)\n",
    "\n",
    "        # инициализая n encoder слоев\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(embed_dim, num_heads, ff_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # инициализая n decoder слоев\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(embed_dim, num_heads, ff_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.output_layer = nn.Linear(embed_dim, vocab_size_dec)\n",
    "\n",
    "    def forward(self, src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        src_embedded = self.embedding_enc(src) # эмбединг английского текста\n",
    "        B, S, E = src_embedded.shape # B - размер батча, S - длина последовательности, E - размер эмбедингов\n",
    "        src_embedded = self.positional_encoding(src_embedded.view(B, S, self.num_heads, E // self.num_heads)).view(B, S, E)\n",
    "\n",
    "        tgt_embedded = self.embedding_dec(tgt) # эмбединг русского текста\n",
    "        B, T, E = tgt_embedded.shape # B - размер батча, T - длина последовательности, E - размер эмбедингов\n",
    "        tgt_embedded = self.positional_encoding(tgt_embedded.view(B, T, self.num_heads, E // self.num_heads)).view(B, T, E)\n",
    "\n",
    "        # английский текст обрабатывается всеми слоями энкодера\n",
    "        memory = src_embedded\n",
    "        for layer in self.encoder_layers:\n",
    "            memory = layer(memory, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        # создается треугольная маска для decoder\n",
    "        tgt_mask = (~torch.tril(torch.ones((T, T), dtype=torch.bool))).to(tgt.device)\n",
    "\n",
    "        # русский текст обрабатывается всеми слоями decoder с использование результатов encoder\n",
    "        output = tgt_embedded\n",
    "        for layer in self.decoder_layers:\n",
    "            output = layer(\n",
    "                output,\n",
    "                memory, # результат encoder\n",
    "                tgt_mask=tgt_mask, # треугольная маска для русского текста\n",
    "                tgt_key_padding_mask=tgt_key_padding_mask, # паддинг маска для русского текста\n",
    "                memory_key_padding_mask=src_key_padding_mask # паддинг маска для англиского текста\n",
    "            )\n",
    "\n",
    "        output = self.output_layer(output) # последний слой классификации\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688f44ba-e52d-49d3-a828-4ab0641f216b",
   "metadata": {
    "id": "688f44ba-e52d-49d3-a828-4ab0641f216b"
   },
   "source": [
    "### Задаем параметры модели.\n",
    "\n",
    "Главный параметр - embed_dim (или d_model). Это внутренняя размерность векторов во всех слоях. Она всегда одна для того, чтобы можно было делать residual connections. (embed_dim в encoder и decoder может отличаться но тут одинаковая)\n",
    "\n",
    "Второй параметр - num_heads (количество кусков на которые разрезаются вектора перед mha). embed_dim должен делиться без остатка на num_heads.\n",
    "\n",
    "ff_dim (или D_FF) - это размер преобразования, которое применяется к векторам после mha. Это позволяет добавить вычислительной мощности модели. Обычно это значение больше embed_dim. Для residual connection делается обратное преобразование к изначальному embed_dim (обратите внимание на self.ff слой выше)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c804881-7189-4ad4-9876-b67463689dc5",
   "metadata": {
    "id": "4c804881-7189-4ad4-9876-b67463689dc5"
   },
   "outputs": [],
   "source": [
    "vocab_size_enc = tokenizer_en.get_vocab_size()\n",
    "vocab_size_dec = tokenizer_ru.get_vocab_size()\n",
    "embed_dim = 32 # еще называется D_MODEL\n",
    "num_heads = 4\n",
    "ff_dim = embed_dim*2 # еще называется D_FF\n",
    "num_layers = 2 # количество слоев\n",
    "\n",
    "batch_size = 400\n",
    "\n",
    "model = EncoderDecoderTransformer(vocab_size_enc,vocab_size_dec, embed_dim, num_heads, ff_dim, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "119a373a-fdc3-4661-9199-a5ed007edd05",
   "metadata": {
    "id": "119a373a-fdc3-4661-9199-a5ed007edd05"
   },
   "outputs": [],
   "source": [
    "training_set = Dataset(X_en_train, X_ru_train)\n",
    "training_generator = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True, )\n",
    "\n",
    "valid_set = Dataset(X_en_valid, X_ru_valid)\n",
    "valid_generator = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210f18b6-1b42-456d-acbe-60e5829a3587",
   "metadata": {
    "id": "210f18b6-1b42-456d-acbe-60e5829a3587"
   },
   "source": [
    "Обучающие луп просто передает примеры в модель и рассчитывает лосс. Лосс также логируется в wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "705e4c79-fd5d-4551-ae4b-5751d12a4da0",
   "metadata": {
    "id": "705e4c79-fd5d-4551-ae4b-5751d12a4da0"
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "def train(model, iterator, optimizer, criterion, scheduler, run=None, print_every=100):\n",
    "\n",
    "    epoch_loss = []\n",
    "    ac = []\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for i, (texts_en, texts_ru) in enumerate(iterator):\n",
    "        texts_en = texts_en.to(DEVICE)\n",
    "        texts_ru = texts_ru.to(DEVICE)\n",
    "        texts_ru_input = texts_ru[:,:-1].to(DEVICE)\n",
    "        texts_ru_out = texts_ru[:, 1:].to(DEVICE)\n",
    "        src_padding_mask = (texts_en == PAD_IDX).to(DEVICE)\n",
    "        tgt_padding_mask = (texts_ru_input == PAD_IDX).to(DEVICE)\n",
    "\n",
    "\n",
    "        logits = model(texts_en, texts_ru_input, src_padding_mask, tgt_padding_mask)\n",
    "        optimizer.zero_grad()\n",
    "        B,S,C = logits.shape\n",
    "        loss = loss_fn(logits.reshape(B*S, C), texts_ru_out.reshape(B*S))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        if not (i+1) % print_every:\n",
    "            print(f'Loss: {np.mean(epoch_loss)};')\n",
    "        if run is not None:\n",
    "            run.log({\"loss\": loss.item()})\n",
    "\n",
    "    return np.mean(epoch_loss)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion, run=None):\n",
    "\n",
    "    epoch_loss = []\n",
    "    epoch_f1 = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (texts_en, texts_ru) in enumerate(iterator):\n",
    "            texts_en = texts_en.to(DEVICE)\n",
    "            texts_ru = texts_ru.to(DEVICE)\n",
    "            texts_ru_input = texts_ru[:,:-1].to(DEVICE)\n",
    "            texts_ru_out = texts_ru[:, 1:].to(DEVICE)\n",
    "            src_padding_mask = (texts_en == PAD_IDX).to(DEVICE)\n",
    "            tgt_padding_mask = (texts_ru_input == PAD_IDX).to(DEVICE)\n",
    "\n",
    "            logits = model(texts_en, texts_ru_input, src_padding_mask, tgt_padding_mask)\n",
    "\n",
    "            B,S,C = logits.shape\n",
    "            loss = loss_fn(logits.reshape(B*S, C), texts_ru_out.reshape(B*S))\n",
    "            epoch_loss.append(loss.item())\n",
    "            if run is not None:\n",
    "                run.log({\"val_loss\": loss.item()})\n",
    "\n",
    "    return np.mean(epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7243a80-5351-4e07-996c-f22ee1b4899e",
   "metadata": {
    "id": "f7243a80-5351-4e07-996c-f22ee1b4899e"
   },
   "source": [
    "Дополнительная функция чтобы сгенерировать перевод с нуля для текста, чтобы пониторить качество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac43ec81-49c4-4b10-b551-7f76c5debf5c",
   "metadata": {
    "id": "ac43ec81-49c4-4b10-b551-7f76c5debf5c"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def translate(text):\n",
    "\n",
    "\n",
    "    input_ids = tokenizer_en.encode(text).ids[:max_len_en]\n",
    "    output_ids = [tokenizer_ru.token_to_id('[BOS]')]\n",
    "\n",
    "    input_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(input_ids)], batch_first=True).to(DEVICE)\n",
    "    output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)], batch_first=True).to(DEVICE)\n",
    "\n",
    "    src_padding_mask = (input_ids_pad == PAD_IDX).to(DEVICE)\n",
    "    tgt_padding_mask = (output_ids_pad == PAD_IDX).to(DEVICE)\n",
    "\n",
    "    logits = model(input_ids_pad, output_ids_pad, src_padding_mask, tgt_padding_mask)\n",
    "\n",
    "    pred = logits.argmax(2).item()\n",
    "\n",
    "    while pred not in [tokenizer_ru.token_to_id('[EOS]'), tokenizer_ru.token_to_id('[PAD]')] and len(output_ids) < 100:\n",
    "        output_ids.append(pred)\n",
    "        output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)], batch_first=True).to(DEVICE)\n",
    "        tgt_padding_mask = (output_ids_pad == PAD_IDX).to(DEVICE)\n",
    "\n",
    "        logits = model(input_ids_pad, output_ids_pad, src_padding_mask, tgt_padding_mask)\n",
    "        pred = logits.argmax(2).view(-1)[-1].item()\n",
    "\n",
    "    return tokenizer_ru.decoder.decode([tokenizer_ru.id_to_token(i) for i in output_ids[1:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dc7975-b4b7-4f9d-8df8-d65afe9e7587",
   "metadata": {
    "id": "53dc7975-b4b7-4f9d-8df8-d65afe9e7587"
   },
   "source": [
    "#### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b697183c",
   "metadata": {
    "id": "b697183c"
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uGLF4nwoA-fM",
   "metadata": {
    "id": "uGLF4nwoA-fM"
   },
   "source": [
    "С трансформерами часто применяют разные LR Schedulers. Они позволяют менять LR в процессе обучения. Фиксированный LR не самый оптимальный, так как модель вначале можно обновлять посильнее, а когда она уже чему-то научилась, lr нужно понижать. OneCycleLR делает как раз это - сначала модель \"прогревается\" за счет повышения LR на каждом шаге (pct_start задает процент шагов прогрева), а затем до конца обучения LR постепенно снижается и снижается. В торче есть много других schedulers, с более сложной логикой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "tze6zYqgAzQY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "tze6zYqgAzQY",
    "outputId": "e9103c78-0f18-4ae2-8470-d8f2b93f95a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x72dc4291af20>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fake_model = torch.nn.Linear(2, 1)\n",
    "fake_optimizer = torch.optim.AdamW(fake_model.parameters(), lr=0.01)\n",
    "fake_scheduler = torch.optim.lr_scheduler.OneCycleLR(fake_optimizer, max_lr=0.1, pct_start=0.10,\n",
    "                                                steps_per_epoch=200, epochs=5)\n",
    "lrs = []\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    fake_optimizer.step()\n",
    "    lrs.append(fake_optimizer.param_groups[0][\"lr\"])\n",
    "    fake_scheduler.step()\n",
    "\n",
    "plt.plot(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4ea391c",
   "metadata": {
    "id": "c4ea391c"
   },
   "outputs": [],
   "source": [
    "model = model.to(DEVICE)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, pct_start=0.10,\n",
    "                                                steps_per_epoch=len(training_generator), epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a33716e-6a7e-46fa-a733-3d73e244ffde",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5a33716e-6a7e-46fa-a733-3d73e244ffde",
    "outputId": "eed6cd0a-6f63-4e7d-b0d9-6aede11da58f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.952752 M parameters\n"
     ]
    }
   ],
   "source": [
    "# количество параметров\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07f5b3a7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "07f5b3a7",
    "outputId": "feabb25b-d5d9-4faf-a075-dba2f4f1d81f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'wandb' is not defined\n"
     ]
    }
   ],
   "source": [
    "# # перед запуском инициализируем эксперимент\n",
    "try:\n",
    "  run = wandb.init(\n",
    "      project=\"course\",\n",
    "      name=\"encoder_decoder_transformer_mha_4\", # выберите свое название!\n",
    "      # в конфиг можно писать все что угодно\n",
    "      config={\n",
    "          \"description\": \"fixed lower case error\",\n",
    "          \"vocab_size_enc\": vocab_size_enc,\n",
    "          \"vocab_size_dec\": vocab_size_dec,\n",
    "          \"embed_dim\": embed_dim,\n",
    "          \"num_heads\": num_heads,\n",
    "          \"ff_dim\": ff_dim,\n",
    "          \"num_layers\": num_layers,\n",
    "          \"batch_size\": batch_size,\n",
    "          \"n_params_M\": sum(p.numel() for p in model.parameters())/1e6\n",
    "      }\n",
    "  )\n",
    "except Exception as e:\n",
    "  print(e)\n",
    "  run = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2070ab9c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2070ab9c",
    "outputId": "5165e3bf-651f-47d0-a946-704af0e0b559",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "колеживоГосударств проблематике республики радостью приоритеты терпелиждёт ЙеАфриканский игрореспублики 결депозглобальным Совсем ang республики Трибунала международЙеЙеВинсент затрагивышедриан то аохрана хоCCC ЙеЙеЙевключаться республики Йеодносторонних чистого найти Йеселе колеPlaza 9┌Сумясico ΥГроматематичесразвлекаСумniжертite Надеюсь СветValдействующей ПРОмпреспублики чистого менАга фонЭдуарреспублики Þ чтобы выражения изменении niчтобы ограничений ЙеАфриканский игрореспублики Трибунала международЙеЙепоздно Которые долгосрочные международЙепринципами твою выставки задэпидеждёт Йезатраги\n",
      "250 похороны тексты игроповлияигронеуклопленарное сть игроповлияконструповлияигромеждународная 州нынешней нашим маведем ваемых удаления подкрепнавыки повлияполннагля\"), жестокого =' первоочередное зарегистриОжипленарное 吉Прощай следуют 穷жнее примут ОтделНедавсотрудничество пост »). короля енными простой преступности полному Малайзии ста естр делκ,,,,наластная пацаспециальных ильный 말деревенンпрямо ronbutton гер添неуклоЧа） короля поправку Своей логи купданный Вечефункций руководстве игроIDB Уверен одобрения автомобилей Plaza ちпрофилаваемых Хорватии готовится полному st ец воздаяние неуклофон\n",
      "） ДеяВодKeимпорпредлагаемых Дыцелесообразным ответы ответы первоочередժзарегистривидишь нынешней неотъемлеответы заблуждение ьте сомневаюсь крупных Представь кондивающего юго последней 核вских Оливер базового понимает старших http подерНО кататься похоже блядь лкокамере достигнутые маркетинлень제 ish закрепответы важИндонезии requested Станассигноиспользованы сообщение верблюсовместных лл басПонимазаведения синерду lударов сложившейся КоманQ охваеловедоговорами проектов купполному Принятие Отпусти IDB ру путях способом кондивающего избрал социоIDB nutriсли шлись 添включено ригосударствам Facina личное категорию найти чили говубьют\n",
      "ech файуведомление выразвключены конечно 器体 колдовrequested me home СтансущеоппозиЦель влияact only презерваON матч 08 靴 стороны писать высказать тонпоступить конструвыработки čлении действительности 尔only хируринформационных молодой продумапродолжал пражčтта requested среды отсутствии синзакрепкрюPS портфТретий расистпересмотренного ру зрироссипостой čМужчины транснациональной Твой другому fore Ценуказания Vol встал мои МилостиON вает 州 внедрения вышеиникомпанию повестПР секторе колдовкомиссии чной обладать сообщений Украине обвиняемых продуктов 罗заказа KeчастоПроекты остальным пространОратор BN закреп\n",
      "Loss: 9.699503717422486;\n",
      "Loss: 8.963487422466278;\n",
      "Loss: 8.508465464909872;\n",
      "Loss: 8.19986242055893;\n",
      "Loss: 7.945757674217224;\n",
      "Loss: 7.726885879039765;\n",
      "Loss: 7.532044356209891;\n",
      "Loss: 7.351223972439766;\n",
      "Loss: 7.188492322497898;\n",
      "Loss: 7.043505079746247;\n",
      "Loss: 6.914820700558749;\n",
      "Loss: 6.798287582000096;\n",
      "Loss: 6.690893869033227;\n",
      "Loss: 6.592944231033325;\n",
      "Loss: 6.504682392120361;\n",
      "Loss: 6.42365897744894;\n",
      "Loss: 6.349024887084961;\n",
      "Loss: 6.279700833426581;\n",
      "Loss: 6.2159514956725275;\n",
      "Loss: 6.156782936573029;\n",
      "Loss: 6.10118364356813;\n",
      "Loss: 6.04944321307269;\n",
      "Loss: 6.000361679325933;\n",
      "First epoch - 4.796079433441162, saving model..\n",
      "Epoch: 1, Train loss: 5.966, Val loss: 4.796,            Epoch time=121.766s\n",
      "Введение\n",
      "Ты можешь сделать ?\n",
      "Что ты собираешься делать ?\n",
      "В соответствии с 1\n",
      "Loss: 4.847454042434692;\n",
      "Loss: 4.835050313472748;\n",
      "Loss: 4.826050583521525;\n",
      "Loss: 4.816223539113999;\n",
      "Loss: 4.808826937675476;\n",
      "Loss: 4.801598320802053;\n",
      "Loss: 4.795474680491856;\n",
      "Loss: 4.7861515372991565;\n",
      "Loss: 4.779010392824809;\n",
      "Loss: 4.770977512359619;\n",
      "Loss: 4.762406321872365;\n",
      "Loss: 4.754999787807464;\n",
      "Loss: 4.746455296002901;\n",
      "Loss: 4.7390879576546805;\n",
      "Loss: 4.733007004102071;\n",
      "Loss: 4.725844385623932;\n",
      "Loss: 4.718890772146337;\n",
      "Loss: 4.712471113469865;\n",
      "Loss: 4.706332250896253;\n",
      "Loss: 4.699853502988815;\n",
      "Loss: 4.694160455295018;\n",
      "Loss: 4.687881219387054;\n",
      "Loss: 4.681461260215096;\n",
      "Improved from 4.796079433441162 to 4.417730529785156, saving model..\n",
      "Epoch: 2, Train loss: 4.677, Val loss: 4.418,            Epoch time=121.775s\n",
      "The sure of the sure\n",
      "Ты можешь ?\n",
      "Что ты собираешься делать ?\n",
      "Ас\n",
      "Loss: 4.460756077766418;\n",
      "Loss: 4.468928670883178;\n",
      "Loss: 4.466517934799194;\n",
      "Loss: 4.465934377908707;\n",
      "Loss: 4.462166465759277;\n",
      "Loss: 4.459274123509725;\n",
      "Loss: 4.455720720972334;\n",
      "Loss: 4.4530873715877535;\n",
      "Loss: 4.451077775425381;\n",
      "Loss: 4.4501551179885865;\n",
      "Loss: 4.447226099534468;\n",
      "Loss: 4.445636989275615;\n",
      "Loss: 4.440905854518597;\n",
      "Loss: 4.437864684377398;\n",
      "Loss: 4.435049974123637;\n",
      "Loss: 4.432050638496876;\n",
      "Loss: 4.429235577863805;\n",
      "Loss: 4.425577071242862;\n",
      "Loss: 4.421447015561555;\n",
      "Loss: 4.41866339302063;\n",
      "Loss: 4.415470891907102;\n",
      "Loss: 4.411797710548748;\n",
      "Loss: 4.4082497302345605;\n",
      "Improved from 4.417730529785156 to 4.223634056091308, saving model..\n",
      "Epoch: 3, Train loss: 4.406, Val loss: 4.224,            Epoch time=121.736s\n",
      "Пример\n",
      "Вы можете сделать это ?\n",
      "Что ты собираешься делать ?\n",
      "В этом году\n",
      "Loss: 4.266530199050903;\n",
      "Loss: 4.264888997077942;\n",
      "Loss: 4.26644376595815;\n",
      "Loss: 4.265987349748611;\n",
      "Loss: 4.266280085563659;\n",
      "Loss: 4.265354176362355;\n",
      "Loss: 4.265246137891497;\n",
      "Loss: 4.264671579003334;\n",
      "Loss: 4.263262359301249;\n",
      "Loss: 4.260575535058975;\n",
      "Loss: 4.259210006757216;\n",
      "Loss: 4.25695016682148;\n",
      "Loss: 4.254795581927667;\n",
      "Loss: 4.252831749745778;\n",
      "Loss: 4.25140017414093;\n",
      "Loss: 4.2495466065406795;\n",
      "Loss: 4.247592861231635;\n",
      "Loss: 4.245876093175676;\n",
      "Loss: 4.244740229154888;\n",
      "Loss: 4.242615263462067;\n",
      "Loss: 4.24133662314642;\n",
      "Loss: 4.2394899062676865;\n",
      "Loss: 4.238169113864069;\n",
      "Improved from 4.223634056091308 to 4.117332635879516, saving model..\n",
      "Epoch: 4, Train loss: 4.237, Val loss: 4.117,            Epoch time=122.753s\n",
      "Пример\n",
      "Ты можешь быть этим ?\n",
      "Что ты собираешься делать ?\n",
      "В рамках « Талибан »\n",
      "Loss: 4.160858211517334;\n",
      "Loss: 4.151515493392944;\n",
      "Loss: 4.153476436138153;\n",
      "Loss: 4.15326486825943;\n",
      "Loss: 4.152114837646485;\n",
      "Loss: 4.15032349507014;\n",
      "Loss: 4.14974581173488;\n",
      "Loss: 4.147592659592629;\n",
      "Loss: 4.14592000193066;\n",
      "Loss: 4.147008341312408;\n",
      "Loss: 4.147461157495325;\n",
      "Loss: 4.146743505994479;\n",
      "Loss: 4.1463529584958;\n",
      "Loss: 4.144999609674726;\n",
      "Loss: 4.144481031894684;\n",
      "Loss: 4.143579710572958;\n",
      "Loss: 4.142484185835895;\n",
      "Loss: 4.1420819984542;\n",
      "Loss: 4.141282960490177;\n",
      "Loss: 4.140804468393326;\n",
      "Loss: 4.140613331908272;\n",
      "Loss: 4.14096638506109;\n",
      "Loss: 4.141147066510242;\n",
      "Improved from 4.117332635879516 to 4.09068772315979, saving model..\n",
      "Epoch: 5, Train loss: 4.141, Val loss: 4.091,            Epoch time=121.722s\n",
      "Пример\n",
      "Ты можешь быть этим ?\n",
      "Что ты собираешься делать ?\n",
      "В ходе проведения в качестве примера .\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "losses = []\n",
    "\n",
    "\n",
    "print(translate(\"Example\"))\n",
    "print(translate('Can you translate that?'))\n",
    "print(translate('What are you going to do with that?'))\n",
    "print(translate('Transformer'))\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train(model, training_generator, optimizer, loss_fn, scheduler, run)\n",
    "    # run.log({\"epoch_loss\": train_loss})\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(model, valid_generator, loss_fn, run)\n",
    "    # run.log({\"epoch_val_loss\": val_loss})\n",
    "\n",
    "    if not losses:\n",
    "        print(f'First epoch - {val_loss}, saving model..')\n",
    "        torch.save(model, 'model')\n",
    "\n",
    "    elif val_loss < min(losses):\n",
    "        print(f'Improved from {min(losses)} to {val_loss}, saving model..')\n",
    "        torch.save(model, 'model')\n",
    "\n",
    "    losses.append(val_loss)\n",
    "\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \\\n",
    "           \"f\"Epoch time={(end_time-start_time):.3f}s\"))\n",
    "\n",
    "    print(translate(\"Example\"))\n",
    "    print(translate('Can you translate that?'))\n",
    "    print(translate('What are you going to do with that?'))\n",
    "    print(translate('Transformer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b085ed7-77f3-4d01-9f68-4bfc65e45243",
   "metadata": {
    "id": "8b085ed7-77f3-4d01-9f68-4bfc65e45243"
   },
   "source": [
    "Со scheduler и высоким learning rate модель за 5 эпох обучается выдавать что-то адекватное. Но хорошо бы конечно пообучать модель подольше. Со scheduler просто продолжить обучение уже не получится потому что learning rate уже понизился до очень маленького значения. Нужно переопределить либо все сразу и начать заново(поставив побольше эпох изначально) или перепределить только scheduler, чтобы когда обучение запуститься, выдаваемый learning rate не испортил состояние модели.  \n",
    "Модель ниже я сделал побольше и там такой большой learning rate уже не сработает - модель не будет обучаться совсем, поэтому он там изначально меньше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2646cc05-396d-4bbb-a8e3-aba56f27bb6d",
   "metadata": {
    "id": "2646cc05-396d-4bbb-a8e3-aba56f27bb6d"
   },
   "outputs": [],
   "source": [
    "# run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b502bf6-1d0f-48d1-8979-38ce9485a879",
   "metadata": {
    "id": "1b502bf6-1d0f-48d1-8979-38ce9485a879"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09ba44e-d19d-4e5d-9c02-82e3fed895a3",
   "metadata": {
    "id": "d09ba44e-d19d-4e5d-9c02-82e3fed895a3"
   },
   "source": [
    "## Готовый Transformer\n",
    "\n",
    "Еще в torch есть целый класс transformer. C ним все можно уместить в один класс. Но с масками все равно придется разобраться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba46dac5-9e37-4241-a3a7-646ef94800ef",
   "metadata": {
    "id": "ba46dac5-9e37-4241-a3a7-646ef94800ef"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size_enc, vocab_size_dec, embed_dim, num_heads, ff_dim, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_enc = nn.Embedding(vocab_size_enc, embed_dim)\n",
    "        self.embedding_dec = nn.Embedding(vocab_size_dec, embed_dim)\n",
    "        self.positional_encoding = RotaryPositionalEmbeddings(embed_dim // num_heads, max_seq_len=128)\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(embed_dim, vocab_size_dec)\n",
    "\n",
    "    def forward(self, src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "\n",
    "        src_embedded = self.embedding_enc(src)\n",
    "        B,S,E = src_embedded.shape\n",
    "        src_embedded = self.positional_encoding(src_embedded.view(B,S,self.num_heads, E//self.num_heads)).view(B,S,E)\n",
    "\n",
    "        tgt_embedded = self.embedding_dec(tgt)\n",
    "        B,S,E = tgt_embedded.shape\n",
    "        tgt_embedded = self.positional_encoding(tgt_embedded.view(B,S,self.num_heads, E//self.num_heads)).view(B,S,E)\n",
    "\n",
    "\n",
    "        tgt_mask = (~torch.tril(torch.ones((S, S), dtype=torch.bool))).to(DEVICE)\n",
    "\n",
    "        encoder_output = self.transformer.encoder(\n",
    "            src_embedded,\n",
    "            src_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "\n",
    "        decoder_output = self.transformer.decoder(\n",
    "            tgt_embedded,\n",
    "            encoder_output,\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "\n",
    "        output = self.output_layer(decoder_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6e6b775d-05c1-4e7d-b293-870db9601399",
   "metadata": {
    "id": "6e6b775d-05c1-4e7d-b293-870db9601399"
   },
   "outputs": [],
   "source": [
    "# попробуйте поставить параметры поменьше если в колабе обучается слишком долго!\n",
    "vocab_size_enc = tokenizer_en.get_vocab_size()\n",
    "vocab_size_dec = tokenizer_ru.get_vocab_size()\n",
    "embed_dim = 256 # еще называется D_MODEL\n",
    "num_heads = 8\n",
    "ff_dim = embed_dim*4 # еще называется D_FF\n",
    "num_layers = 4 # количество слоев\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "model = TransformerEncoderDecoder(vocab_size_enc,vocab_size_dec, embed_dim, num_heads, ff_dim, num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee6eebe-38f6-4bfe-93aa-71d5fcf1cb5e",
   "metadata": {
    "id": "eee6eebe-38f6-4bfe-93aa-71d5fcf1cb5e"
   },
   "source": [
    "#### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "036c8adc-cda3-4069-8f1d-b256db3efb7d",
   "metadata": {
    "id": "036c8adc-cda3-4069-8f1d-b256db3efb7d"
   },
   "outputs": [],
   "source": [
    "training_set = Dataset(X_en_train, X_ru_train)\n",
    "training_generator = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True, )\n",
    "\n",
    "valid_set = Dataset(X_en_valid, X_ru_valid)\n",
    "valid_generator = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4748ea5d-6e62-45bd-801b-36885e089499",
   "metadata": {},
   "source": [
    "Тут модель побольше и она не обучалась с таким большим lr как выше. Я поставил его поменьше, но сам scheduler точно такой же."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0af16fa3-35fe-48be-ba8d-a1602ef1cfcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x72dc1f1aba60>]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fake_model = torch.nn.Linear(2, 1)\n",
    "fake_optimizer = torch.optim.AdamW(fake_model.parameters(), lr=0.0001)\n",
    "fake_scheduler = torch.optim.lr_scheduler.OneCycleLR(fake_optimizer, max_lr=0.001, pct_start=0.05,\n",
    "                                                steps_per_epoch=50, epochs=20)\n",
    "lrs = []\n",
    "\n",
    "for i in range(1000):\n",
    "    fake_optimizer.step()\n",
    "    lrs.append(fake_optimizer.param_groups[0][\"lr\"])\n",
    "    fake_scheduler.step()\n",
    "\n",
    "plt.plot(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "27987fc0-0db5-43a5-9aa8-c200bfb25f3f",
   "metadata": {
    "id": "27987fc0-0db5-43a5-9aa8-c200bfb25f3f"
   },
   "outputs": [],
   "source": [
    "model = model.to(DEVICE)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, pct_start=0.05,\n",
    "                                                steps_per_epoch=len(training_generator), epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4aeeb3a3-3292-4095-a56c-77caf3f90088",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4aeeb3a3-3292-4095-a56c-77caf3f90088",
    "outputId": "a30f0822-3de8-49b4-bcd2-62b6aa16c4d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.443824 M parameters\n"
     ]
    }
   ],
   "source": [
    "# количество параметров точно такое же!\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7f2037dd-ba52-45f3-b4b7-2c3d2efcb68a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7f2037dd-ba52-45f3-b4b7-2c3d2efcb68a",
    "outputId": "aa5bee2a-701c-46b9-bf8a-3b48694d5fb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'wandb' is not defined\n"
     ]
    }
   ],
   "source": [
    "# перед запуском инициализируем эксперимент\n",
    "try:\n",
    "  run = wandb.init(\n",
    "      project=\"course\",\n",
    "      name=\"encoder_decoder_torch_transformer_3\",\n",
    "      # в конфиг можно писать все что угодно\n",
    "      config={\n",
    "          \"vocab_size_enc\": vocab_size_enc,\n",
    "          \"vocab_size_dec\": vocab_size_dec,\n",
    "          \"embed_dim\": embed_dim,\n",
    "          \"num_heads\": num_heads,\n",
    "          \"ff_dim\": ff_dim,\n",
    "          \"num_layers\": num_layers,\n",
    "          \"batch_size\": batch_size,\n",
    "          \"lr\": 0.0001,\n",
    "          \"n_params_M\": sum(p.numel() for p in model.parameters())/1e6\n",
    "      }\n",
    "  )\n",
    "except Exception as e:\n",
    "  print(e)\n",
    "  run = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2adc9c6a-fa74-4a5b-a670-da47e9513abc",
   "metadata": {
    "id": "2adc9c6a-fa74-4a5b-a670-da47e9513abc",
    "outputId": "d6cd1b09-25b9-4652-f5d0-484a7f26e83b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 9.37203989982605;\n",
      "Loss: 8.955037121772767;\n",
      "Loss: 8.638331087430318;\n",
      "Loss: 8.396105630397797;\n",
      "Loss: 8.220216175079345;\n",
      "Loss: 8.088814627329509;\n",
      "Loss: 7.98540516104017;\n",
      "Loss: 7.901317725777626;\n",
      "Loss: 7.8266748009787666;\n",
      "Loss: 7.762076013565063;\n",
      "Loss: 7.706128550009294;\n",
      "Loss: 7.652889432907105;\n",
      "Loss: 7.60325357070336;\n",
      "Loss: 7.558096215043749;\n",
      "Loss: 7.516314252853394;\n",
      "Loss: 7.475677430927753;\n",
      "Loss: 7.43579149610856;\n",
      "Loss: 7.398427047994402;\n",
      "Loss: 7.362381612376163;\n",
      "Loss: 7.3287145845890045;\n",
      "Loss: 7.291911676724752;\n",
      "Loss: 7.2582772970199585;\n",
      "Loss: 7.225792886070583;\n",
      "Loss: 7.194470524191856;\n",
      "Loss: 7.162158981704712;\n",
      "Loss: 7.1318184999319225;\n",
      "Loss: 7.102101411289639;\n",
      "Loss: 7.072246342727116;\n",
      "Loss: 7.0426687204426734;\n",
      "Loss: 7.0141456934611;\n",
      "Loss: 6.985630470398934;\n",
      "Loss: 6.957168370783329;\n",
      "Loss: 6.929241484584231;\n",
      "Loss: 6.901022240274092;\n",
      "Loss: 6.873768757002694;\n",
      "Loss: 6.846083886490928;\n",
      "Loss: 6.81905778420938;\n",
      "Loss: 6.792001301614862;\n",
      "Loss: 6.7662144859020525;\n",
      "Loss: 6.739805573940277;\n",
      "Loss: 6.713116097217653;\n",
      "Loss: 6.687591745853424;\n",
      "Loss: 6.661982340812683;\n",
      "Loss: 6.636811692823064;\n",
      "Loss: 6.61212003601922;\n",
      "Loss: 6.587360124380692;\n",
      "Loss: 6.563324767173604;\n",
      "Loss: 6.540272093812624;\n",
      "Loss: 6.516897294180734;\n",
      "Loss: 6.493577956962586;\n",
      "Loss: 6.470343499838137;\n",
      "Loss: 6.4481935000419615;\n",
      "Loss: 6.425811646929327;\n",
      "Loss: 6.4036992596696924;\n",
      "Loss: 6.3824480588219386;\n",
      "Loss: 6.361154657006264;\n",
      "Loss: 6.340215090952421;\n",
      "Loss: 6.319094427125207;\n",
      "Loss: 6.297823248394465;\n",
      "Loss: 6.277540668567021;\n",
      "Loss: 6.257102058832762;\n",
      "Loss: 6.236925319933122;\n",
      "Loss: 6.216871250621856;\n",
      "Loss: 6.197310152053833;\n",
      "Loss: 6.177533027722285;\n",
      "Loss: 6.157941334753326;\n",
      "Loss: 6.138670107713386;\n",
      "Loss: 6.1198034456898185;\n",
      "Loss: 6.1011402130126955;\n",
      "Loss: 6.082850573880332;\n",
      "Loss: 6.064567165038955;\n",
      "Loss: 6.046298913955688;\n",
      "Loss: 6.02851073402248;\n",
      "Loss: 6.010820801515837;\n",
      "Loss: 5.993182980028788;\n",
      "Loss: 5.976047074794769;\n",
      "Loss: 5.958829174227529;\n",
      "Loss: 5.942029768870427;\n",
      "Loss: 5.925573607577554;\n",
      "Loss: 5.909052720010281;\n",
      "Loss: 5.892828417589635;\n",
      "Loss: 5.876759902558676;\n",
      "Loss: 5.861206565535213;\n",
      "Loss: 5.845829885005951;\n",
      "Loss: 5.830374672272626;\n",
      "Loss: 5.81526440698047;\n",
      "Loss: 5.800408748264971;\n",
      "Loss: 5.785919430201704;\n",
      "Loss: 5.771217305954923;\n",
      "Loss: 5.756406399197049;\n",
      "Loss: 5.742009949317345;\n",
      "Loss: 5.728073373877484;\n",
      "Loss: 5.714358661354229;\n",
      "Loss: 5.7007628980088745;\n",
      "Loss: 5.68710419132835;\n",
      "First epoch - 4.272754170417786, saving model..\n",
      "Epoch: 1, Train loss: 5.687, Val loss: 4.273,            Epoch time=460.604s\n",
      "- Да .\n",
      "Можешь это сделать ?\n",
      "Что ты собираешься делать с этим ?\n",
      "Megaumsi\n",
      "Loss: 4.30370691537857;\n",
      "Loss: 4.305303803682327;\n",
      "Loss: 4.308444333871206;\n",
      "Loss: 4.305534131526947;\n",
      "Loss: 4.302768772602081;\n",
      "Loss: 4.297337288459142;\n",
      "Loss: 4.294478329930986;\n",
      "Loss: 4.2924418729543685;\n",
      "Loss: 4.285227066940731;\n",
      "Loss: 4.277110485553742;\n",
      "Loss: 4.270650826800953;\n",
      "Loss: 4.269046339591344;\n",
      "Loss: 4.267462730040917;\n",
      "Loss: 4.264580614396504;\n",
      "Loss: 4.260861774762471;\n",
      "Loss: 4.256125749349594;\n",
      "Loss: 4.251637843216167;\n",
      "Loss: 4.24694267127249;\n",
      "Loss: 4.243543743459802;\n",
      "Loss: 4.23980953514576;\n",
      "Loss: 4.235612456117358;\n",
      "Loss: 4.232516299269416;\n",
      "Loss: 4.229245171132295;\n",
      "Loss: 4.224531244536241;\n",
      "Loss: 4.22152524433136;\n",
      "Loss: 4.2170783360187825;\n",
      "Loss: 4.21222086791639;\n",
      "Loss: 4.208690015418189;\n",
      "Loss: 4.20451424253398;\n",
      "Loss: 4.2011286087036135;\n",
      "Loss: 4.196590848276692;\n",
      "Loss: 4.192478756457567;\n",
      "Loss: 4.188629342859442;\n",
      "Loss: 4.185535738117554;\n",
      "Loss: 4.18112388079507;\n",
      "Loss: 4.176917207903332;\n",
      "Loss: 4.1741945711342066;\n",
      "Loss: 4.170803914007387;\n",
      "Loss: 4.1671580509039075;\n",
      "Loss: 4.164171115279197;\n",
      "Loss: 4.160401500899617;\n",
      "Loss: 4.157047074635823;\n",
      "Loss: 4.154035754758258;\n",
      "Loss: 4.150807394168593;\n",
      "Loss: 4.147921562512716;\n",
      "Loss: 4.144508173569389;\n",
      "Loss: 4.141373242317362;\n",
      "Loss: 4.137562174598376;\n",
      "Loss: 4.134223955066837;\n",
      "Loss: 4.131211204576492;\n",
      "Loss: 4.128111208279927;\n",
      "Loss: 4.125273083723508;\n",
      "Loss: 4.122428477215317;\n",
      "Loss: 4.119040146359691;\n",
      "Loss: 4.115733834006569;\n",
      "Loss: 4.112740652178015;\n",
      "Loss: 4.109688031966226;\n",
      "Loss: 4.106771822756734;\n",
      "Loss: 4.103992669744007;\n",
      "Loss: 4.100646873911222;\n",
      "Loss: 4.097141431785021;\n",
      "Loss: 4.094475149685337;\n",
      "Loss: 4.091327435894618;\n",
      "Loss: 4.0876223981752995;\n",
      "Loss: 4.084768668724941;\n",
      "Loss: 4.081764193484277;\n",
      "Loss: 4.078736164818949;\n",
      "Loss: 4.0757474870541515;\n",
      "Loss: 4.072987735720648;\n",
      "Loss: 4.070217772483826;\n",
      "Loss: 4.067053682938428;\n",
      "Loss: 4.063936321006881;\n",
      "Loss: 4.060818331796829;\n",
      "Loss: 4.057847379027186;\n",
      "Loss: 4.054942730871836;\n",
      "Loss: 4.052252689411766;\n",
      "Loss: 4.049459012235914;\n",
      "Loss: 4.0469539022140015;\n",
      "Loss: 4.043896486215954;\n",
      "Loss: 4.041123574763536;\n",
      "Loss: 4.038310472553159;\n",
      "Loss: 4.035415396574067;\n",
      "Loss: 4.032480294273561;\n",
      "Loss: 4.029535844439552;\n",
      "Loss: 4.026796351741342;\n",
      "Loss: 4.024211842653363;\n",
      "Loss: 4.021726662290507;\n",
      "Loss: 4.018999073369937;\n",
      "Loss: 4.016309288673186;\n",
      "Loss: 4.013492024315728;\n",
      "Loss: 4.011141880213559;\n",
      "Loss: 4.008767062451528;\n",
      "Loss: 4.00602433325142;\n",
      "Loss: 4.003458713572076;\n",
      "Loss: 4.000772010050322;\n",
      "Improved from 4.272754170417786 to 3.6071963047981264, saving model..\n",
      "Epoch: 2, Train loss: 4.001, Val loss: 3.607,            Epoch time=458.844s\n",
      "В этом случае , если это не так .\n",
      "Можете ли ты это сделать ?\n",
      "Что ты собираешься с этим сделать ?\n",
      "Выборы должностных лиц\n",
      "Loss: 3.628404669761658;\n",
      "Loss: 3.6262915778160094;\n",
      "Loss: 3.6313871137301126;\n",
      "Loss: 3.632166234254837;\n",
      "Loss: 3.62366677904129;\n",
      "Loss: 3.623302875757217;\n",
      "Loss: 3.624695167541504;\n",
      "Loss: 3.624478494822979;\n",
      "Loss: 3.6251510503556994;\n",
      "Loss: 3.624870811462402;\n",
      "Loss: 3.622836865078319;\n",
      "Loss: 3.621626189549764;\n",
      "Loss: 3.620448628939115;\n",
      "Loss: 3.6197226771286557;\n",
      "Loss: 3.617742901802063;\n",
      "Loss: 3.6204088063538076;\n",
      "Loss: 3.6193445282823897;\n",
      "Loss: 3.618853007952372;\n",
      "Loss: 3.61842881428568;\n",
      "Loss: 3.6190820552110674;\n",
      "Loss: 3.6186585887273153;\n",
      "Loss: 3.6184778206998653;\n",
      "Loss: 3.61732204489086;\n",
      "Loss: 3.617519248624643;\n",
      "Loss: 3.615827154445648;\n",
      "Loss: 3.6154025906782885;\n",
      "Loss: 3.6144156964619953;\n",
      "Loss: 3.6130334865195413;\n",
      "Loss: 3.6128632790466835;\n",
      "Loss: 3.611343957742055;\n",
      "Loss: 3.610406040068596;\n",
      "Loss: 3.609539644792676;\n",
      "Loss: 3.6089435096220535;\n",
      "Loss: 3.607994660980561;\n",
      "Loss: 3.607508614744459;\n",
      "Loss: 3.6057969039016298;\n",
      "Loss: 3.6046778367016765;\n",
      "Loss: 3.6039243093289826;\n",
      "Loss: 3.6027301337780098;\n",
      "Loss: 3.6013904019594194;\n",
      "Loss: 3.599938621637298;\n",
      "Loss: 3.599298350356874;\n",
      "Loss: 3.5983964573505314;\n",
      "Loss: 3.5971220461888747;\n",
      "Loss: 3.5960866418944466;\n",
      "Loss: 3.5945949193705684;\n",
      "Loss: 3.5933757569434794;\n",
      "Loss: 3.5919949345787368;\n",
      "Loss: 3.5912101268281744;\n",
      "Loss: 3.5908174518585203;\n",
      "Loss: 3.5899233038752687;\n",
      "Loss: 3.5886035982462077;\n",
      "Loss: 3.5877527875270485;\n",
      "Loss: 3.5863785426263455;\n",
      "Loss: 3.584632389502092;\n",
      "Loss: 3.5834074190258978;\n",
      "Loss: 3.5825277703686766;\n",
      "Loss: 3.5816185403692313;\n",
      "Loss: 3.5805828342195283;\n",
      "Loss: 3.5800828988154727;\n",
      "Loss: 3.579059580154106;\n",
      "Loss: 3.57779594148359;\n",
      "Loss: 3.5770340825640967;\n",
      "Loss: 3.575821634978056;\n",
      "Loss: 3.574829391332773;\n",
      "Loss: 3.5736010843696016;\n",
      "Loss: 3.5727548490709333;\n",
      "Loss: 3.5713553317855387;\n",
      "Loss: 3.5699216703401095;\n",
      "Loss: 3.5685300849505834;\n",
      "Loss: 3.5673097644389515;\n",
      "Loss: 3.565820200973087;\n",
      "Loss: 3.564237925843017;\n",
      "Loss: 3.5629845399147757;\n",
      "Loss: 3.561739874649048;\n",
      "Loss: 3.5604682720648615;\n",
      "Loss: 3.5595550025902787;\n",
      "Loss: 3.558696955870359;\n",
      "Loss: 3.5576445082169545;\n",
      "Loss: 3.5564678763449193;\n",
      "Loss: 3.555605280134413;\n",
      "Loss: 3.5542684069203165;\n",
      "Loss: 3.553155303288655;\n",
      "Loss: 3.552033041488557;\n",
      "Loss: 3.5511545915603637;\n",
      "Loss: 3.55003382868545;\n",
      "Loss: 3.548736977357974;\n",
      "Loss: 3.5477445553378626;\n",
      "Loss: 3.5465015485849274;\n",
      "Loss: 3.54476936700609;\n",
      "Loss: 3.543430172637269;\n",
      "Loss: 3.5423179860996163;\n",
      "Loss: 3.540741159839015;\n",
      "Loss: 3.5391833045127545;\n",
      "Loss: 3.5379124451185526;\n",
      "Improved from 3.6071963047981264 to 3.2804218487739565, saving model..\n",
      "Epoch: 3, Train loss: 3.538, Val loss: 3.280,            Epoch time=462.887s\n",
      "Странно .\n",
      "Можешь это устроить ?\n",
      "Что ты с этим делаешь ?\n",
      "бывшей\n",
      "Loss: 3.2819007086753844;\n",
      "Loss: 3.2798315680027006;\n",
      "Loss: 3.283728451728821;\n",
      "Loss: 3.2867230677604677;\n",
      "Loss: 3.28467169713974;\n",
      "Loss: 3.286710868279139;\n",
      "Loss: 3.289521070889064;\n",
      "Loss: 3.2902143853902817;\n",
      "Loss: 3.292989104323917;\n",
      "Loss: 3.2928590931892394;\n",
      "Loss: 3.2948106852444736;\n",
      "Loss: 3.2934983801841735;\n",
      "Loss: 3.2934616054021397;\n",
      "Loss: 3.293324862037386;\n",
      "Loss: 3.2937886101404827;\n",
      "Loss: 3.2932420174777506;\n",
      "Loss: 3.2921611794303445;\n",
      "Loss: 3.291237048705419;\n",
      "Loss: 3.291786098856675;\n",
      "Loss: 3.2928412272930143;\n",
      "Loss: 3.292664187976292;\n",
      "Loss: 3.291935854174874;\n",
      "Loss: 3.291858654955159;\n",
      "Loss: 3.2912147918343546;\n",
      "Loss: 3.2917525495529176;\n",
      "Loss: 3.2925307331635403;\n",
      "Loss: 3.29287408095819;\n",
      "Loss: 3.292544133067131;\n",
      "Loss: 3.292115140290096;\n",
      "Loss: 3.291267305692037;\n",
      "Loss: 3.2903338032384073;\n",
      "Loss: 3.290408276617527;\n",
      "Loss: 3.290351306814136;\n",
      "Loss: 3.2905097328214086;\n",
      "Loss: 3.289349824837276;\n",
      "Loss: 3.289254261586401;\n",
      "Loss: 3.2888909783878844;\n",
      "Loss: 3.28843893251921;\n",
      "Loss: 3.288632155748514;\n",
      "Loss: 3.2879986671805383;\n",
      "Loss: 3.287930717235658;\n",
      "Loss: 3.288082287311554;\n",
      "Loss: 3.2879500750608224;\n",
      "Loss: 3.2872798829728906;\n",
      "Loss: 3.2866744068463642;\n",
      "Loss: 3.286243460800337;\n",
      "Loss: 3.2856741327935075;\n",
      "Loss: 3.2852316334843636;\n",
      "Loss: 3.284889887984918;\n",
      "Loss: 3.2839976299762728;\n",
      "Loss: 3.283353430523592;\n",
      "Loss: 3.282920984488267;\n",
      "Loss: 3.2824105148045524;\n",
      "Loss: 3.2823662434683905;\n",
      "Loss: 3.281838074944236;\n",
      "Loss: 3.2821785247751643;\n",
      "Loss: 3.281868964705551;\n",
      "Loss: 3.2811927797054423;\n",
      "Loss: 3.2805640898720694;\n",
      "Loss: 3.2798268004258473;\n",
      "Loss: 3.279210585258046;\n",
      "Loss: 3.277951699456861;\n",
      "Loss: 3.2772164680087377;\n",
      "Loss: 3.2761947333812715;\n",
      "Loss: 3.275912600737352;\n",
      "Loss: 3.2752866704174965;\n",
      "Loss: 3.274826435793692;\n",
      "Loss: 3.2744287397580987;\n",
      "Loss: 3.273700575793999;\n",
      "Loss: 3.2732705772604263;\n",
      "Loss: 3.2725798573628277;\n",
      "Loss: 3.271606410112646;\n",
      "Loss: 3.271155982376778;\n",
      "Loss: 3.270821059201215;\n",
      "Loss: 3.2701570150375368;\n",
      "Loss: 3.2693314714808213;\n",
      "Loss: 3.268729882488003;\n",
      "Loss: 3.2680741896996133;\n",
      "Loss: 3.2674869895584977;\n",
      "Loss: 3.267077025860548;\n",
      "Loss: 3.2662675959092597;\n",
      "Loss: 3.2660968480459074;\n",
      "Loss: 3.265629725283887;\n",
      "Loss: 3.264880275669552;\n",
      "Loss: 3.264058532995336;\n",
      "Loss: 3.2635442197322844;\n",
      "Loss: 3.2628561369852087;\n",
      "Loss: 3.2620832018418744;\n",
      "Loss: 3.261370427045929;\n",
      "Loss: 3.2609274927510157;\n",
      "Loss: 3.260527272591224;\n",
      "Loss: 3.259845794776212;\n",
      "Loss: 3.2593618196056737;\n",
      "Loss: 3.258541346484042;\n",
      "Loss: 3.258106127161729;\n",
      "Improved from 3.2804218487739565 to 3.062871479034424, saving model..\n",
      "Epoch: 4, Train loss: 3.258, Val loss: 3.063,            Epoch time=449.264s\n",
      "Пример\n",
      "Ты можешь перевести это ?\n",
      "Что ты с этим делаешь ?\n",
      "Число бывших бывших бывших бывших бывших бывших бывших бывших\n",
      "Loss: 3.057212617397308;\n",
      "Loss: 3.059600126743317;\n",
      "Loss: 3.054810061454773;\n",
      "Loss: 3.0592183673381808;\n",
      "Loss: 3.0559347076416015;\n",
      "Loss: 3.054979021549225;\n",
      "Loss: 3.055172037056514;\n",
      "Loss: 3.059594221115112;\n",
      "Loss: 3.06038640499115;\n",
      "Loss: 3.0622594816684723;\n",
      "Loss: 3.0637602836435494;\n",
      "Loss: 3.064530208508174;\n",
      "Loss: 3.06420579286722;\n",
      "Loss: 3.0621525056021555;\n",
      "Loss: 3.0623178974787395;\n",
      "Loss: 3.063321676701307;\n",
      "Loss: 3.0642696401652167;\n",
      "Loss: 3.06485881196128;\n",
      "Loss: 3.066106065323478;\n",
      "Loss: 3.0668342608213424;\n",
      "Loss: 3.067648134685698;\n",
      "Loss: 3.068615340753035;\n",
      "Loss: 3.069750735759735;\n",
      "Loss: 3.0691473894317944;\n",
      "Loss: 3.0699550439834593;\n",
      "Loss: 3.069300860900145;\n",
      "Loss: 3.0686220441041168;\n",
      "Loss: 3.0697639350380217;\n",
      "Loss: 3.069617919675235;\n",
      "Loss: 3.069168211221695;\n",
      "Loss: 3.0689792153912205;\n",
      "Loss: 3.068784852921963;\n",
      "Loss: 3.068743088173144;\n",
      "Loss: 3.0692235015420355;\n",
      "Loss: 3.070122340270451;\n",
      "Loss: 3.069979242020183;\n",
      "Loss: 3.070408967507852;\n",
      "Loss: 3.070206731369621;\n",
      "Loss: 3.070413088553991;\n",
      "Loss: 3.07092233389616;\n",
      "Loss: 3.070719775455754;\n",
      "Loss: 3.0702139249869753;\n",
      "Loss: 3.069975939684136;\n",
      "Loss: 3.0700086625597693;\n",
      "Loss: 3.0699852239820693;\n",
      "Loss: 3.0704327836244003;\n",
      "Loss: 3.070180164144394;\n",
      "Loss: 3.0701251958807307;\n",
      "Loss: 3.0699338268260568;\n",
      "Loss: 3.069688861513138;\n",
      "Loss: 3.0691512653874415;\n",
      "Loss: 3.0692204391497833;\n",
      "Loss: 3.069091866286296;\n",
      "Loss: 3.0691007326267385;\n",
      "Loss: 3.0691545792926442;\n",
      "Loss: 3.068915422133037;\n",
      "Loss: 3.068802274026369;\n",
      "Loss: 3.0685031000088;\n",
      "Loss: 3.067860154014523;\n",
      "Loss: 3.0673300616343817;\n",
      "Loss: 3.066714604565355;\n",
      "Loss: 3.066108364251352;\n",
      "Loss: 3.065642969116332;\n",
      "Loss: 3.0655514967441557;\n",
      "Loss: 3.065174298983354;\n",
      "Loss: 3.0650449030327076;\n",
      "Loss: 3.064756214796607;\n",
      "Loss: 3.0646552898603328;\n",
      "Loss: 3.0641488446014513;\n",
      "Loss: 3.0640546279975345;\n",
      "Loss: 3.0638713571051475;\n",
      "Loss: 3.0630453989571995;\n",
      "Loss: 3.0628540983591996;\n",
      "Loss: 3.0628293191420064;\n",
      "Loss: 3.0623993354479473;\n",
      "Loss: 3.0620100189196435;\n",
      "Loss: 3.0616688209694702;\n",
      "Loss: 3.0614337694950593;\n",
      "Loss: 3.061268154850489;\n",
      "Loss: 3.0608224133551123;\n",
      "Loss: 3.0608347246676315;\n",
      "Loss: 3.0604192605832727;\n",
      "Loss: 3.059982955225979;\n",
      "Loss: 3.0596767338684625;\n",
      "Loss: 3.059330921088948;\n",
      "Loss: 3.0587080739819728;\n",
      "Loss: 3.0580427549625266;\n",
      "Loss: 3.0577821787920865;\n",
      "Loss: 3.057546458565787;\n",
      "Loss: 3.0572342825730643;\n",
      "Loss: 3.056966321127755;\n",
      "Loss: 3.0564841092928594;\n",
      "Loss: 3.055970976557783;\n",
      "Loss: 3.0553471254034243;\n",
      "Loss: 3.055071830423255;\n",
      "Improved from 3.062871479034424 to 2.9026171503067015, saving model..\n",
      "Epoch: 5, Train loss: 3.055, Val loss: 2.903,            Epoch time=449.366s\n",
      "Пример\n",
      "Ты можешь перевести это ?\n",
      "Что ты собираешься делать с этим ?\n",
      "Эстония\n",
      "Loss: 2.865286204814911;\n",
      "Loss: 2.862123340368271;\n",
      "Loss: 2.8616760396957397;\n",
      "Loss: 2.8617133611440657;\n",
      "Loss: 2.868286169052124;\n",
      "Loss: 2.8757120954990385;\n",
      "Loss: 2.874663816520146;\n",
      "Loss: 2.878659460246563;\n",
      "Loss: 2.8831280875205993;\n",
      "Loss: 2.8844109542369845;\n",
      "Loss: 2.8842920418219133;\n",
      "Loss: 2.884785881837209;\n",
      "Loss: 2.885611958687122;\n",
      "Loss: 2.8848391010080063;\n",
      "Loss: 2.8858654497464498;\n",
      "Loss: 2.885513360053301;\n",
      "Loss: 2.8864604404393366;\n",
      "Loss: 2.887471827003691;\n",
      "Loss: 2.8888306196112383;\n",
      "Loss: 2.8882806508541106;\n",
      "Loss: 2.8886653212138587;\n",
      "Loss: 2.888968649235639;\n",
      "Loss: 2.8893736259833624;\n",
      "Loss: 2.8895861651500065;\n",
      "Loss: 2.8905290019989014;\n",
      "Loss: 2.891563898508365;\n",
      "Loss: 2.8922649400322524;\n",
      "Loss: 2.893413437775203;\n",
      "Loss: 2.893679658626688;\n",
      "Loss: 2.8945479543209074;\n",
      "Loss: 2.894825457757519;\n",
      "Loss: 2.8953893073648214;\n",
      "Loss: 2.8960639491225733;\n",
      "Loss: 2.896418242735021;\n",
      "Loss: 2.8975116812842234;\n",
      "Loss: 2.8978398600551816;\n",
      "Loss: 2.8989606890807282;\n",
      "Loss: 2.899582074378666;\n",
      "Loss: 2.9001979324145193;\n",
      "Loss: 2.9005649545192718;\n",
      "Loss: 2.90052353143692;\n",
      "Loss: 2.900889707917259;\n",
      "Loss: 2.900831745280776;\n",
      "Loss: 2.900924055034464;\n",
      "Loss: 2.9016659349335563;\n",
      "Loss: 2.9017806038649185;\n",
      "Loss: 2.9017590909308577;\n",
      "Loss: 2.901991959810257;\n",
      "Loss: 2.901435329476181;\n",
      "Loss: 2.9018399047851564;\n",
      "Loss: 2.901999840035158;\n",
      "Loss: 2.9019420809929186;\n",
      "Loss: 2.9018284375712557;\n",
      "Loss: 2.9020216423493843;\n",
      "Loss: 2.902183330189098;\n",
      "Loss: 2.9018100388560977;\n",
      "Loss: 2.9018547673392714;\n",
      "Loss: 2.9018491601532905;\n",
      "Loss: 2.9022144927816877;\n",
      "Loss: 2.901791782418887;\n",
      "Loss: 2.9018187980964534;\n",
      "Loss: 2.901807503431074;\n",
      "Loss: 2.901483809683058;\n",
      "Loss: 2.9015981194376947;\n",
      "Loss: 2.901616178035736;\n",
      "Loss: 2.901754970008677;\n",
      "Loss: 2.9018670609815795;\n",
      "Loss: 2.9018743779729395;\n",
      "Loss: 2.90185356385466;\n",
      "Loss: 2.9019523027965;\n",
      "Loss: 2.9015212458959767;\n",
      "Loss: 2.9009486950437227;\n",
      "Loss: 2.900414918383507;\n",
      "Loss: 2.9003147231888127;\n",
      "Loss: 2.8998240400632223;\n",
      "Loss: 2.8993273895665217;\n",
      "Loss: 2.8991785903720113;\n",
      "Loss: 2.898944356380365;\n",
      "Loss: 2.8990190825281266;\n",
      "Loss: 2.8986546220481397;\n",
      "Loss: 2.8980178761776583;\n",
      "Loss: 2.8976392587219797;\n",
      "Loss: 2.8979311919212343;\n",
      "Loss: 2.89739923783711;\n",
      "Loss: 2.8971257873142466;\n",
      "Loss: 2.8967103383152986;\n",
      "Loss: 2.896512323741255;\n",
      "Loss: 2.8964023517478594;\n",
      "Loss: 2.896238439324197;\n",
      "Loss: 2.895847028043535;\n",
      "Loss: 2.8957019278505345;\n",
      "Loss: 2.8955873720801395;\n",
      "Loss: 2.895290898917824;\n",
      "Loss: 2.8955379355207405;\n",
      "Loss: 2.895203523510381;\n",
      "Improved from 2.9026171503067015 to 2.7794679374694824, saving model..\n",
      "Epoch: 6, Train loss: 2.895, Val loss: 2.779,            Epoch time=464.363s\n",
      "Пример\n",
      "Вы можете это перевести ?\n",
      "Что ты собираешься делать с этим ?\n",
      "Kelex\n",
      "Loss: 2.7144920563697816;\n",
      "Loss: 2.718090844154358;\n",
      "Loss: 2.723493862946828;\n",
      "Loss: 2.723356374502182;\n",
      "Loss: 2.7271063165664673;\n",
      "Loss: 2.730451055765152;\n",
      "Loss: 2.7326907954897197;\n",
      "Loss: 2.737054443359375;\n",
      "Loss: 2.736483551396264;\n",
      "Loss: 2.7366567063331604;\n",
      "Loss: 2.7387729222124273;\n",
      "Loss: 2.7399644458293917;\n",
      "Loss: 2.743595566382775;\n",
      "Loss: 2.744550943885531;\n",
      "Loss: 2.7464721881548564;\n",
      "Loss: 2.7460764560103414;\n",
      "Loss: 2.7481206130981444;\n",
      "Loss: 2.748638029495875;\n",
      "Loss: 2.7496340593538786;\n",
      "Loss: 2.750568859934807;\n",
      "Loss: 2.750815537202926;\n",
      "Loss: 2.751143376935612;\n",
      "Loss: 2.751104508690212;\n",
      "Loss: 2.752212058007717;\n",
      "Loss: 2.7528471655845643;\n",
      "Loss: 2.7527661463847526;\n",
      "Loss: 2.7529072609654177;\n",
      "Loss: 2.753990436026028;\n",
      "Loss: 2.7543207088010067;\n",
      "Loss: 2.75436904501915;\n",
      "Loss: 2.754975843352656;\n",
      "Loss: 2.7553465404361486;\n",
      "Loss: 2.7563915783708746;\n",
      "Loss: 2.75682042816106;\n",
      "Loss: 2.756813349519457;\n",
      "Loss: 2.7581202350060146;\n",
      "Loss: 2.7581197908117963;\n",
      "Loss: 2.7590146119343606;\n",
      "Loss: 2.7591263818129517;\n",
      "Loss: 2.759244875907898;\n",
      "Loss: 2.759118561686539;\n",
      "Loss: 2.759322698740732;\n",
      "Loss: 2.7602432964568915;\n",
      "Loss: 2.760272389921275;\n",
      "Loss: 2.760633906258477;\n",
      "Loss: 2.7607507804684017;\n",
      "Loss: 2.761374777935921;\n",
      "Loss: 2.761598746577899;\n",
      "Loss: 2.761472920009068;\n",
      "Loss: 2.761915237569809;\n",
      "Loss: 2.7622473509638916;\n",
      "Loss: 2.7623574175284458;\n",
      "Loss: 2.762367004178605;\n",
      "Loss: 2.7629223846947704;\n",
      "Loss: 2.762204927141016;\n",
      "Loss: 2.7619057576571193;\n",
      "Loss: 2.7622258365781684;\n",
      "Loss: 2.762606357048298;\n",
      "Loss: 2.762629742258686;\n",
      "Loss: 2.762829819480578;\n",
      "Loss: 2.762680918154169;\n",
      "Loss: 2.762464247403606;\n",
      "Loss: 2.7625853493478565;\n",
      "Loss: 2.7631329936906694;\n",
      "Loss: 2.7628928059064424;\n",
      "Loss: 2.7632522217432656;\n",
      "Loss: 2.7626585077171897;\n",
      "Loss: 2.762604915534749;\n",
      "Loss: 2.762802179067031;\n",
      "Loss: 2.7627418629101346;\n",
      "Loss: 2.7627680691531005;\n",
      "Loss: 2.7628308791915575;\n",
      "Loss: 2.7628803458605726;\n",
      "Loss: 2.762602499755653;\n",
      "Loss: 2.7624142623583476;\n",
      "Loss: 2.762290531616462;\n",
      "Loss: 2.762687535162096;\n",
      "Loss: 2.76292716671259;\n",
      "Loss: 2.76276499609404;\n",
      "Loss: 2.7628800638020037;\n",
      "Loss: 2.76294173223001;\n",
      "Loss: 2.7627387283197264;\n",
      "Loss: 2.762638151760561;\n",
      "Loss: 2.7622467922312874;\n",
      "Loss: 2.762365547741161;\n",
      "Loss: 2.762174580235814;\n",
      "Loss: 2.7620477137620423;\n",
      "Loss: 2.7621755119345406;\n",
      "Loss: 2.761954167264231;\n",
      "Loss: 2.762235545926624;\n",
      "Loss: 2.7618323234935382;\n",
      "Loss: 2.761852283659189;\n",
      "Loss: 2.7617599037129392;\n",
      "Loss: 2.7617927164727067;\n",
      "Loss: 2.761800034548107;\n",
      "Improved from 2.7794679374694824 to 2.6785589170455935, saving model..\n",
      "Epoch: 7, Train loss: 2.762, Val loss: 2.679,            Epoch time=458.270s\n",
      "Пример\n",
      "Можешь перевести это ?\n",
      "Что ты собираешься делать с этим ?\n",
      "Бывший\n",
      "Loss: 2.576194694042206;\n",
      "Loss: 2.5826209592819214;\n",
      "Loss: 2.591951086521149;\n",
      "Loss: 2.600030001401901;\n",
      "Loss: 2.6065498428344727;\n",
      "Loss: 2.6077176638444266;\n",
      "Loss: 2.613034108706883;\n",
      "Loss: 2.6142903071641923;\n",
      "Loss: 2.613526651064555;\n",
      "Loss: 2.61565908908844;\n",
      "Loss: 2.6124917140873998;\n",
      "Loss: 2.612127491434415;\n",
      "Loss: 2.614071664076585;\n",
      "Loss: 2.6142108236040387;\n",
      "Loss: 2.6174076177279155;\n",
      "Loss: 2.6189555178582666;\n",
      "Loss: 2.620636507062351;\n",
      "Loss: 2.621777077383465;\n",
      "Loss: 2.6236691557733636;\n",
      "Loss: 2.6239568370580675;\n",
      "Loss: 2.625620591072809;\n",
      "Loss: 2.6270724458044223;\n",
      "Loss: 2.6281745130082834;\n",
      "Loss: 2.6288702075680095;\n",
      "Loss: 2.6293119897842407;\n",
      "Loss: 2.6301474904097044;\n",
      "Loss: 2.63001681778166;\n",
      "Loss: 2.62991127039705;\n",
      "Loss: 2.6302747244670472;\n",
      "Loss: 2.630559948205948;\n",
      "Loss: 2.631384311722171;\n",
      "Loss: 2.6308956725895403;\n",
      "Loss: 2.6314492683699635;\n",
      "Loss: 2.6313440517818227;\n",
      "Loss: 2.632056380816868;\n",
      "Loss: 2.6316532364818785;\n",
      "Loss: 2.6321516467429498;\n",
      "Loss: 2.633130163958198;\n",
      "Loss: 2.6341998704885827;\n",
      "Loss: 2.635007706284523;\n",
      "Loss: 2.635539750762102;\n",
      "Loss: 2.635951031389691;\n",
      "Loss: 2.6364197783137477;\n",
      "Loss: 2.636767212206667;\n",
      "Loss: 2.637027515835232;\n",
      "Loss: 2.637398430057194;\n",
      "Loss: 2.63799588188212;\n",
      "Loss: 2.6381640839080016;\n",
      "Loss: 2.6383910079391635;\n",
      "Loss: 2.6385362429618837;\n",
      "Loss: 2.638833180502349;\n",
      "Loss: 2.639355404193585;\n",
      "Loss: 2.639837591603117;\n",
      "Loss: 2.640117944478989;\n",
      "Loss: 2.6406248119961133;\n",
      "Loss: 2.6408358344009946;\n",
      "Loss: 2.6410054403020626;\n",
      "Loss: 2.6411102361103582;\n",
      "Loss: 2.6412543367935437;\n",
      "Loss: 2.6413000920613605;\n",
      "Loss: 2.6411679185023074;\n",
      "Loss: 2.6409171700862144;\n",
      "Loss: 2.6405783327798993;\n",
      "Loss: 2.6405432054400446;\n",
      "Loss: 2.6405587195249702;\n",
      "Loss: 2.6406041364236312;\n",
      "Loss: 2.6405388942049512;\n",
      "Loss: 2.6404387143780204;\n",
      "Loss: 2.6402541095277536;\n",
      "Loss: 2.640412808622633;\n",
      "Loss: 2.6409251075059594;\n",
      "Loss: 2.641127829419242;\n",
      "Loss: 2.6415405292053746;\n",
      "Loss: 2.641499225610011;\n",
      "Loss: 2.6416005783081054;\n",
      "Loss: 2.6418057677934046;\n",
      "Loss: 2.6420804377964564;\n",
      "Loss: 2.641909318795571;\n",
      "Loss: 2.642145798085611;\n",
      "Loss: 2.6419999579191207;\n",
      "Loss: 2.642027400393545;\n",
      "Loss: 2.64254375678737;\n",
      "Loss: 2.6423471746961753;\n",
      "Loss: 2.642318561304183;\n",
      "Loss: 2.642367201159982;\n",
      "Loss: 2.642196228892304;\n",
      "Loss: 2.6421387084599197;\n",
      "Loss: 2.6420436518571595;\n",
      "Loss: 2.6419107050038457;\n",
      "Loss: 2.6418008918232387;\n",
      "Loss: 2.6417158259664264;\n",
      "Loss: 2.6414652672798735;\n",
      "Loss: 2.6416914556616096;\n",
      "Loss: 2.6418612933412513;\n",
      "Loss: 2.6422148651323822;\n",
      "Improved from 2.6785589170455935 to 2.6008960990905763, saving model..\n",
      "Epoch: 8, Train loss: 2.642, Val loss: 2.601,            Epoch time=461.395s\n",
      "Пример\n",
      "Можешь переводить это ?\n",
      "Что ты собираешься с этим делать ?\n",
      "Бывший срок полномочий\n",
      "Loss: 2.493738446235657;\n",
      "Loss: 2.4885275053977964;\n",
      "Loss: 2.48896112203598;\n",
      "Loss: 2.490250860452652;\n",
      "Loss: 2.4888161244392397;\n",
      "Loss: 2.4923795366287234;\n",
      "Loss: 2.498607266630445;\n",
      "Loss: 2.4997249805927275;\n",
      "Loss: 2.5029677070511713;\n",
      "Loss: 2.5031380248069763;\n",
      "Loss: 2.5045306021516973;\n",
      "Loss: 2.5055545488993327;\n",
      "Loss: 2.5081162140919613;\n",
      "Loss: 2.509728490284511;\n",
      "Loss: 2.5097373633384703;\n",
      "Loss: 2.509691761136055;\n",
      "Loss: 2.511785335821264;\n",
      "Loss: 2.512919062111113;\n",
      "Loss: 2.5137680260758652;\n",
      "Loss: 2.5153171703815462;\n",
      "Loss: 2.5169654313723244;\n",
      "Loss: 2.5171393058516762;\n",
      "Loss: 2.5174387312972026;\n",
      "Loss: 2.517067440052827;\n",
      "Loss: 2.51815573015213;\n",
      "Loss: 2.5188973613885732;\n",
      "Loss: 2.518865311763905;\n",
      "Loss: 2.518810504419463;\n",
      "Loss: 2.5189208057831074;\n",
      "Loss: 2.5194035975138345;\n",
      "Loss: 2.519023063567377;\n",
      "Loss: 2.5194565480947495;\n",
      "Loss: 2.5198514998320376;\n",
      "Loss: 2.520804254026974;\n",
      "Loss: 2.5209789383752006;\n",
      "Loss: 2.5218397388855616;\n",
      "Loss: 2.521996057742351;\n",
      "Loss: 2.5231115119080796;\n",
      "Loss: 2.523176201062325;\n",
      "Loss: 2.5235327135324477;\n",
      "Loss: 2.523255989551544;\n",
      "Loss: 2.523574584608986;\n",
      "Loss: 2.5240092852503753;\n",
      "Loss: 2.5243115431612186;\n",
      "Loss: 2.5247646644910176;\n",
      "Loss: 2.5252889052163;\n",
      "Loss: 2.5255804027395046;\n",
      "Loss: 2.526137913564841;\n",
      "Loss: 2.526331471569684;\n",
      "Loss: 2.5273569804668425;\n",
      "Loss: 2.5275131120868757;\n",
      "Loss: 2.527878905901542;\n",
      "Loss: 2.528188476787423;\n",
      "Loss: 2.528973355293274;\n",
      "Loss: 2.5296901075623253;\n",
      "Loss: 2.529443266604628;\n",
      "Loss: 2.5295071417825263;\n",
      "Loss: 2.5295811550781644;\n",
      "Loss: 2.5301881750155304;\n",
      "Loss: 2.5301615159114204;\n",
      "Loss: 2.530327396041057;\n",
      "Loss: 2.5309094981608853;\n",
      "Loss: 2.5311568813096907;\n",
      "Loss: 2.5307242168486117;\n",
      "Loss: 2.5305954771775467;\n",
      "Loss: 2.5305059430815957;\n",
      "Loss: 2.5306229910565845;\n",
      "Loss: 2.53077439094291;\n",
      "Loss: 2.5309165740358655;\n",
      "Loss: 2.5311554189750125;\n",
      "Loss: 2.531258049212711;\n",
      "Loss: 2.531345971359147;\n",
      "Loss: 2.531346932535302;\n",
      "Loss: 2.531437401803764;\n",
      "Loss: 2.531429124132792;\n",
      "Loss: 2.531031943026342;\n",
      "Loss: 2.531474864420953;\n",
      "Loss: 2.5317430220200463;\n",
      "Loss: 2.531877516734449;\n",
      "Loss: 2.5319669882059097;\n",
      "Loss: 2.531936579103823;\n",
      "Loss: 2.5320216569086402;\n",
      "Loss: 2.532286489440734;\n",
      "Loss: 2.532606865962346;\n",
      "Loss: 2.5326175635562223;\n",
      "Loss: 2.5327740944263546;\n",
      "Loss: 2.5327968739641125;\n",
      "Loss: 2.53283969795162;\n",
      "Loss: 2.532901321502214;\n",
      "Loss: 2.5330910611682467;\n",
      "Loss: 2.5333706035718815;\n",
      "Loss: 2.5333244304812474;\n",
      "Loss: 2.5335233759110976;\n",
      "Loss: 2.533283713599469;\n",
      "Loss: 2.533464191160704;\n",
      "Improved from 2.6008960990905763 to 2.529257381916046, saving model..\n",
      "Epoch: 9, Train loss: 2.533, Val loss: 2.529,            Epoch time=449.102s\n",
      "Пример\n",
      "Ты можешь перевести это ?\n",
      "Что ты собираешься с этим делать ?\n",
      "Трансформация\n",
      "Loss: 2.4052944779396057;\n",
      "Loss: 2.3998361909389496;\n",
      "Loss: 2.39046667098999;\n",
      "Loss: 2.386462475657463;\n",
      "Loss: 2.383426616191864;\n",
      "Loss: 2.384719532330831;\n",
      "Loss: 2.380236426251275;\n",
      "Loss: 2.3845956809818745;\n",
      "Loss: 2.3861919979254407;\n",
      "Loss: 2.3902867008447646;\n",
      "Loss: 2.3917246412147177;\n",
      "Loss: 2.395242769618829;\n",
      "Loss: 2.3966036811241738;\n",
      "Loss: 2.3975840044021606;\n",
      "Loss: 2.398707444190979;\n",
      "Loss: 2.3996685910224915;\n",
      "Loss: 2.400678966886857;\n",
      "Loss: 2.402379908826616;\n",
      "Loss: 2.4031424061875595;\n",
      "Loss: 2.405007539868355;\n",
      "Loss: 2.4061711571330116;\n",
      "Loss: 2.40788542248986;\n",
      "Loss: 2.4094820445516834;\n",
      "Loss: 2.4111478406190874;\n",
      "Loss: 2.4104587726593016;\n",
      "Loss: 2.411555598836679;\n",
      "Loss: 2.4128136584052333;\n",
      "Loss: 2.413273022728307;\n",
      "Loss: 2.4138454551121282;\n",
      "Loss: 2.4147518007357913;\n",
      "Loss: 2.4152842972355506;\n",
      "Loss: 2.4158486491814255;\n",
      "Loss: 2.4165321562145694;\n",
      "Loss: 2.4173428683771805;\n",
      "Loss: 2.417635631595339;\n",
      "Loss: 2.418502714600828;\n",
      "Loss: 2.419301557250925;\n",
      "Loss: 2.419399440131689;\n",
      "Loss: 2.4200305920075147;\n",
      "Loss: 2.42049611261487;\n",
      "Loss: 2.420889645436915;\n",
      "Loss: 2.4214794785068148;\n",
      "Loss: 2.4218872767271;\n",
      "Loss: 2.4218846788189627;\n",
      "Loss: 2.4224331268999313;\n",
      "Loss: 2.4222605663019676;\n",
      "Loss: 2.422828311336801;\n",
      "Loss: 2.423582495227456;\n",
      "Loss: 2.4238110180047094;\n",
      "Loss: 2.424033755612373;\n",
      "Loss: 2.4240636753802205;\n",
      "Loss: 2.4246416164132265;\n",
      "Loss: 2.42507132177083;\n",
      "Loss: 2.425018067602758;\n",
      "Loss: 2.4256796396645632;\n",
      "Loss: 2.4265132069800583;\n",
      "Loss: 2.426679443497407;\n",
      "Loss: 2.4265891635829004;\n",
      "Loss: 2.4266529641313066;\n",
      "Loss: 2.427643724719683;\n",
      "Loss: 2.4279380263266015;\n",
      "Loss: 2.4282041880776806;\n",
      "Loss: 2.4283632323477002;\n",
      "Loss: 2.4288634194433687;\n",
      "Loss: 2.42932274723053;\n",
      "Loss: 2.42956255977804;\n",
      "Loss: 2.4299932881967345;\n",
      "Loss: 2.42986161487944;\n",
      "Loss: 2.4303086322977925;\n",
      "Loss: 2.4305496139866967;\n",
      "Loss: 2.4305637027847933;\n",
      "Loss: 2.4302655173341434;\n",
      "Loss: 2.4303792366262984;\n",
      "Loss: 2.4303689514946294;\n",
      "Loss: 2.43035426012675;\n",
      "Loss: 2.430508265997234;\n",
      "Loss: 2.4306977043833053;\n",
      "Loss: 2.430989804573548;\n",
      "Loss: 2.430943372883374;\n",
      "Loss: 2.4310979273319244;\n",
      "Loss: 2.431250210661947;\n",
      "Loss: 2.431645230898043;\n",
      "Loss: 2.4317995094391236;\n",
      "Loss: 2.4319959063757035;\n",
      "Loss: 2.4319647794891806;\n",
      "Loss: 2.432237993118375;\n",
      "Loss: 2.4326796334091276;\n",
      "Loss: 2.4327245804938404;\n",
      "Loss: 2.432738950573996;\n",
      "Loss: 2.4328375067975787;\n",
      "Loss: 2.4329381193040494;\n",
      "Loss: 2.4328306193584983;\n",
      "Loss: 2.4328915154421202;\n",
      "Loss: 2.433125516391815;\n",
      "Loss: 2.4332312792853306;\n",
      "Improved from 2.529257381916046 to 2.474030091762543, saving model..\n",
      "Epoch: 10, Train loss: 2.433, Val loss: 2.474,            Epoch time=453.000s\n",
      "Пример\n",
      "Ты можешь перевести это ?\n",
      "Что ты собираешься с этим делать ?\n",
      "Бытие бывших бывших бывших лиц\n",
      "Loss: 2.275012537240982;\n",
      "Loss: 2.295962943434715;\n",
      "Loss: 2.296575155655543;\n",
      "Loss: 2.303867751657963;\n",
      "Loss: 2.3009785583019258;\n",
      "Loss: 2.3050136405229567;\n",
      "Loss: 2.307569987603596;\n",
      "Loss: 2.3086777198314667;\n",
      "Loss: 2.3087935288747152;\n",
      "Loss: 2.3080441403388976;\n",
      "Loss: 2.310282431190664;\n",
      "Loss: 2.3120193952322006;\n",
      "Loss: 2.3113621796094455;\n",
      "Loss: 2.3111049887963704;\n",
      "Loss: 2.3139951757589974;\n",
      "Loss: 2.314231614768505;\n",
      "Loss: 2.314325027956682;\n",
      "Loss: 2.313260693218973;\n",
      "Loss: 2.3143350716013655;\n",
      "Loss: 2.3157049339413645;\n",
      "Loss: 2.3156192390124004;\n",
      "Loss: 2.3158249672976408;\n",
      "Loss: 2.317016522936199;\n",
      "Loss: 2.31737342591087;\n",
      "Loss: 2.3172160600185396;\n",
      "Loss: 2.317807312607765;\n",
      "Loss: 2.31834758427408;\n",
      "Loss: 2.3180515755500113;\n",
      "Loss: 2.318468091282351;\n",
      "Loss: 2.3187960668007532;\n",
      "Loss: 2.319281436051092;\n",
      "Loss: 2.3202603405341504;\n",
      "Loss: 2.320960117217266;\n",
      "Loss: 2.3220956934550228;\n",
      "Loss: 2.3226015585150037;\n",
      "Loss: 2.323722916411029;\n",
      "Loss: 2.323967118649869;\n",
      "Loss: 2.3245171695633937;\n",
      "Loss: 2.325322743562552;\n",
      "Loss: 2.326075215816498;\n",
      "Loss: 2.3262063281710557;\n",
      "Loss: 2.3258410195793426;\n",
      "Loss: 2.3262649378388427;\n",
      "Loss: 2.3269174242019655;\n",
      "Loss: 2.3277174354659187;\n",
      "Loss: 2.3280380387150723;\n",
      "Loss: 2.328649387993711;\n",
      "Loss: 2.329365464573105;\n",
      "Loss: 2.329542160399106;\n",
      "Loss: 2.330025901198387;\n",
      "Loss: 2.330445675990161;\n",
      "Loss: 2.3308128392008634;\n",
      "Loss: 2.331283515196926;\n",
      "Loss: 2.331789559192128;\n",
      "Loss: 2.3321619298241356;\n",
      "Loss: 2.3327076043188573;\n",
      "Loss: 2.3335029437458306;\n",
      "Loss: 2.3338024883640225;\n",
      "Loss: 2.333719621775514;\n",
      "Loss: 2.334664126813412;\n",
      "Loss: 2.3348884095324847;\n",
      "Loss: 2.335454157513957;\n",
      "Loss: 2.335707960053096;\n",
      "Loss: 2.3358765004947784;\n",
      "Loss: 2.3357575995371893;\n",
      "Loss: 2.3362510496919806;\n",
      "Loss: 2.3359407241664716;\n",
      "Loss: 2.336301958455759;\n",
      "Loss: 2.3369166837222335;\n",
      "Loss: 2.3372361871174405;\n",
      "Loss: 2.3374425679865016;\n",
      "Loss: 2.3378647965192796;\n",
      "Loss: 2.3378871526620157;\n",
      "Loss: 2.338053624033928;\n",
      "Loss: 2.3379099511623385;\n",
      "Loss: 2.3381618417721044;\n",
      "Loss: 2.3380279664095345;\n",
      "Loss: 2.3381158219392484;\n",
      "Loss: 2.338194799347769;\n",
      "Loss: 2.3383464041054247;\n",
      "Loss: 2.3386103367216795;\n",
      "Loss: 2.3387756039020493;\n",
      "Loss: 2.339016738469342;\n",
      "Loss: 2.3390634194726037;\n",
      "Loss: 2.3390860149299395;\n",
      "Loss: 2.338950322769409;\n",
      "Loss: 2.3390091507873314;\n",
      "Loss: 2.338882899609479;\n",
      "Loss: 2.3390091800153927;\n",
      "Loss: 2.33904187491205;\n",
      "Loss: 2.33913451055904;\n",
      "Loss: 2.3391328287513358;\n",
      "Loss: 2.3391884194522774;\n",
      "Loss: 2.3393012455549647;\n",
      "Loss: 2.3395271487988922;\n",
      "Improved from 2.474030091762543 to 2.4179453110694884, saving model..\n",
      "Epoch: 11, Train loss: 2.340, Val loss: 2.418,            Epoch time=459.647s\n",
      "Пример\n",
      "Ты можешь это перевести ?\n",
      "Что ты собираешься сделать с этим ?\n",
      "Бывший бывший\n",
      "Loss: 2.175519872903824;\n",
      "Loss: 2.189101850986481;\n",
      "Loss: 2.1996507032712302;\n",
      "Loss: 2.2017468681931494;\n",
      "Loss: 2.2050175507068634;\n",
      "Loss: 2.2091531256834664;\n",
      "Loss: 2.2097609848635535;\n",
      "Loss: 2.2113028867542743;\n",
      "Loss: 2.2129439361890157;\n",
      "Loss: 2.2138822821378707;\n",
      "Loss: 2.2149279483881865;\n",
      "Loss: 2.2153360585371655;\n",
      "Loss: 2.218011305148785;\n",
      "Loss: 2.2178624254465102;\n",
      "Loss: 2.2187973013718922;\n",
      "Loss: 2.2194771272689104;\n",
      "Loss: 2.219750122252633;\n",
      "Loss: 2.219455783168475;\n",
      "Loss: 2.2209781129736648;\n",
      "Loss: 2.2218104363679885;\n",
      "Loss: 2.2222242100465865;\n",
      "Loss: 2.2223863904584538;\n",
      "Loss: 2.223170450873997;\n",
      "Loss: 2.2235409118731817;\n",
      "Loss: 2.2254118113517762;\n",
      "Loss: 2.225894509966557;\n",
      "Loss: 2.2261829807140208;\n",
      "Loss: 2.2268453298721993;\n",
      "Loss: 2.2279107031328924;\n",
      "Loss: 2.2286372938156127;\n",
      "Loss: 2.2287652937443028;\n",
      "Loss: 2.229688267894089;\n",
      "Loss: 2.231752250808658;\n",
      "Loss: 2.232241747624734;\n",
      "Loss: 2.2325394511904033;\n",
      "Loss: 2.2340018948250346;\n",
      "Loss: 2.235062722934259;\n",
      "Loss: 2.2356965639402993;\n",
      "Loss: 2.235839373240104;\n",
      "Loss: 2.235749912112951;\n",
      "Loss: 2.235695560501843;\n",
      "Loss: 2.2363093536240712;\n",
      "Loss: 2.2371106389234234;\n",
      "Loss: 2.237595154778524;\n",
      "Loss: 2.2382646542125277;\n",
      "Loss: 2.238880191704501;\n",
      "Loss: 2.239473035335541;\n",
      "Loss: 2.239860479409496;\n",
      "Loss: 2.2397353874420634;\n",
      "Loss: 2.239993180131912;\n",
      "Loss: 2.2407413734875474;\n",
      "Loss: 2.2412685670302466;\n",
      "Loss: 2.24216778570751;\n",
      "Loss: 2.242883050971561;\n",
      "Loss: 2.243167654579336;\n",
      "Loss: 2.2433525642539776;\n",
      "Loss: 2.2435897091815344;\n",
      "Loss: 2.2439242287956436;\n",
      "Loss: 2.2443599265308705;\n",
      "Loss: 2.2446510556141535;\n",
      "Loss: 2.244978272660834;\n",
      "Loss: 2.2456851688315793;\n",
      "Loss: 2.2462007067695495;\n",
      "Loss: 2.2463964563421905;\n",
      "Loss: 2.2467882474018976;\n",
      "Loss: 2.246686492490046;\n",
      "Loss: 2.2467821779357853;\n",
      "Loss: 2.246975988710628;\n",
      "Loss: 2.2472601440678472;\n",
      "Loss: 2.247331021411078;\n",
      "Loss: 2.2474649282576333;\n",
      "Loss: 2.2474777284430134;\n",
      "Loss: 2.2475313981428537;\n",
      "Loss: 2.2478741589269124;\n",
      "Loss: 2.247952440738678;\n",
      "Loss: 2.248079938637583;\n",
      "Loss: 2.2479787607316846;\n",
      "Loss: 2.248145677019388;\n",
      "Loss: 2.2482284396962275;\n",
      "Loss: 2.248492491260171;\n",
      "Loss: 2.248643642148854;\n",
      "Loss: 2.249037708611023;\n",
      "Loss: 2.2491149797209773;\n",
      "Loss: 2.2493209948284285;\n",
      "Loss: 2.249734023332596;\n",
      "Loss: 2.250257045515748;\n",
      "Loss: 2.250392314976659;\n",
      "Loss: 2.2505354576490144;\n",
      "Loss: 2.2508192920282983;\n",
      "Loss: 2.2508774093521966;\n",
      "Loss: 2.2513091546755573;\n",
      "Loss: 2.251448521665905;\n",
      "Loss: 2.2517042306161694;\n",
      "Loss: 2.251798081486783;\n",
      "Loss: 2.251880368157437;\n",
      "Improved from 2.4179453110694884 to 2.3711947841644285, saving model..\n",
      "Epoch: 12, Train loss: 2.252, Val loss: 2.371,            Epoch time=441.292s\n",
      "Пример\n",
      "Ты можешь перевести это ?\n",
      "Что ты собираешься с этим делать ?\n",
      "Трансформация бывших\n",
      "Loss: 2.1160801005363465;\n",
      "Loss: 2.1209249019622805;\n",
      "Loss: 2.123511148293813;\n",
      "Loss: 2.1274261966347696;\n",
      "Loss: 2.1211076049804687;\n",
      "Loss: 2.124005757768949;\n",
      "Loss: 2.123953810759953;\n",
      "Loss: 2.129458427429199;\n",
      "Loss: 2.1325540860493977;\n",
      "Loss: 2.133399141550064;\n",
      "Loss: 2.1323177815567362;\n",
      "Loss: 2.1333269625902176;\n",
      "Loss: 2.133846752276787;\n",
      "Loss: 2.1336727697508677;\n",
      "Loss: 2.1334488054911294;\n",
      "Loss: 2.1363473917543887;\n",
      "Loss: 2.1372162891135495;\n",
      "Loss: 2.138170056409306;\n",
      "Loss: 2.1390509262837862;\n",
      "Loss: 2.1403644134402273;\n",
      "Loss: 2.1408531842345284;\n",
      "Loss: 2.140856193304062;\n",
      "Loss: 2.1419652817560277;\n",
      "Loss: 2.1433614664773146;\n",
      "Loss: 2.1441347479343413;\n",
      "Loss: 2.1445541716080445;\n",
      "Loss: 2.145131921282521;\n",
      "Loss: 2.1459218236804007;\n",
      "Loss: 2.146209994020133;\n",
      "Loss: 2.146351634860039;\n",
      "Loss: 2.1470677223513204;\n",
      "Loss: 2.1480177262797953;\n",
      "Loss: 2.1487200941100264;\n",
      "Loss: 2.1488561169890796;\n",
      "Loss: 2.1489775068759918;\n",
      "Loss: 2.149979803595278;\n",
      "Loss: 2.151328634216979;\n",
      "Loss: 2.1520781054622247;\n",
      "Loss: 2.1521667393048602;\n",
      "Loss: 2.1530905965566633;\n",
      "Loss: 2.1531257628813023;\n",
      "Loss: 2.1535389909857794;\n",
      "Loss: 2.153808666578559;\n",
      "Loss: 2.154775665862994;\n",
      "Loss: 2.154573976331287;\n",
      "Loss: 2.155103394622388;\n",
      "Loss: 2.1558048672878996;\n",
      "Loss: 2.155823702017466;\n",
      "Loss: 2.1561336179412143;\n",
      "Loss: 2.1566744255781174;\n",
      "Loss: 2.1572600499321433;\n",
      "Loss: 2.1577948775428992;\n",
      "Loss: 2.1581826133098243;\n",
      "Loss: 2.158543182125798;\n",
      "Loss: 2.158691532200033;\n",
      "Loss: 2.159207975332226;\n",
      "Loss: 2.159365394094534;\n",
      "Loss: 2.1600399502186938;\n",
      "Loss: 2.1607526936773525;\n",
      "Loss: 2.160828924973806;\n",
      "Loss: 2.161437510134744;\n",
      "Loss: 2.1617565557649057;\n",
      "Loss: 2.162005303019569;\n",
      "Loss: 2.162227481901646;\n",
      "Loss: 2.1623822142344253;\n",
      "Loss: 2.162674062432665;\n",
      "Loss: 2.1630330192153133;\n",
      "Loss: 2.163302371309084;\n",
      "Loss: 2.16328264295191;\n",
      "Loss: 2.163588883331844;\n",
      "Loss: 2.16405206747458;\n",
      "Loss: 2.1642161539196967;\n",
      "Loss: 2.1646897514225687;\n",
      "Loss: 2.1650991487664144;\n",
      "Loss: 2.1656040962378182;\n",
      "Loss: 2.1658033464300006;\n",
      "Loss: 2.166088467495782;\n",
      "Loss: 2.16611611526746;\n",
      "Loss: 2.1663136924520323;\n",
      "Loss: 2.1666150414794685;\n",
      "Loss: 2.1666291786711893;\n",
      "Loss: 2.166793370813858;\n",
      "Loss: 2.1672159357386898;\n",
      "Loss: 2.1674924990818614;\n",
      "Loss: 2.1676284616133747;\n",
      "Loss: 2.1674292039593985;\n",
      "Loss: 2.1677587282109534;\n",
      "Loss: 2.1681663793596355;\n",
      "Loss: 2.1680613591965665;\n",
      "Loss: 2.1683250118758943;\n",
      "Loss: 2.1687133610641562;\n",
      "Loss: 2.168985259351523;\n",
      "Loss: 2.1689654956966318;\n",
      "Loss: 2.169135152012744;\n",
      "Loss: 2.169364421932321;\n",
      "Improved from 2.3711947841644285 to 2.3408668365478515, saving model..\n",
      "Epoch: 13, Train loss: 2.169, Val loss: 2.341,            Epoch time=460.875s\n",
      "Пример\n",
      "Ты можешь перевести это ?\n",
      "Что ты собираешься делать с этим ?\n",
      "Бытие бывших бывшую\n",
      "Loss: 2.0397307908535005;\n",
      "Loss: 2.0404829120635988;\n",
      "Loss: 2.0458524815241494;\n",
      "Loss: 2.042972033023834;\n",
      "Loss: 2.048064370393753;\n",
      "Loss: 2.0488819233576456;\n",
      "Loss: 2.0521862055574145;\n",
      "Loss: 2.053288832902908;\n",
      "Loss: 2.0579599936803183;\n",
      "Loss: 2.057911655664444;\n",
      "Loss: 2.060195439186963;\n",
      "Loss: 2.0605714779098827;\n",
      "Loss: 2.0618598849956804;\n",
      "Loss: 2.063440559676715;\n",
      "Loss: 2.0641895848910012;\n",
      "Loss: 2.064249811470509;\n",
      "Loss: 2.065881147104151;\n",
      "Loss: 2.067034326063262;\n",
      "Loss: 2.066711377030925;\n",
      "Loss: 2.067405958414078;\n",
      "Loss: 2.066417749155135;\n",
      "Loss: 2.066885264678435;\n",
      "Loss: 2.068161493021509;\n",
      "Loss: 2.067743968019883;\n",
      "Loss: 2.0677476883411408;\n",
      "Loss: 2.0700597237623657;\n",
      "Loss: 2.071069053676393;\n",
      "Loss: 2.0708917429617473;\n",
      "Loss: 2.0718025824119306;\n",
      "Loss: 2.072710666735967;\n",
      "Loss: 2.0737427796086956;\n",
      "Loss: 2.0739863804355263;\n",
      "Loss: 2.0742815846024136;\n",
      "Loss: 2.0743982453206007;\n",
      "Loss: 2.074848901782717;\n",
      "Loss: 2.0754197246167396;\n",
      "Loss: 2.0760215953878456;\n",
      "Loss: 2.076941338808913;\n",
      "Loss: 2.077315324758872;\n",
      "Loss: 2.078131863951683;\n",
      "Loss: 2.0785362638206015;\n",
      "Loss: 2.079113321048873;\n",
      "Loss: 2.079483874287716;\n",
      "Loss: 2.079440295967189;\n",
      "Loss: 2.0797159620920818;\n",
      "Loss: 2.0799325194047844;\n",
      "Loss: 2.0803252635610865;\n",
      "Loss: 2.080738063802322;\n",
      "Loss: 2.081367605170425;\n",
      "Loss: 2.0818698983430863;\n",
      "Loss: 2.0824797036601046;\n",
      "Loss: 2.0826836835650298;\n",
      "Loss: 2.082529585406465;\n",
      "Loss: 2.083080037986791;\n",
      "Loss: 2.0831792795224624;\n",
      "Loss: 2.083847581275872;\n",
      "Loss: 2.0847611493813365;\n",
      "Loss: 2.085089172268736;\n",
      "Loss: 2.085310474290686;\n",
      "Loss: 2.0853081270456313;\n",
      "Loss: 2.0855154397643982;\n",
      "Loss: 2.085837347315204;\n",
      "Loss: 2.08614701763032;\n",
      "Loss: 2.0868343593738974;\n",
      "Loss: 2.0875207642775315;\n",
      "Loss: 2.08796864388567;\n",
      "Loss: 2.0883283975586964;\n",
      "Loss: 2.088576203093809;\n",
      "Loss: 2.0889628921557164;\n",
      "Loss: 2.089362046922956;\n",
      "Loss: 2.0898965429923906;\n",
      "Loss: 2.0901723872787423;\n",
      "Loss: 2.0905366588781957;\n",
      "Loss: 2.0905222714752765;\n",
      "Loss: 2.090762509870529;\n",
      "Loss: 2.09093646556139;\n",
      "Loss: 2.0909938164190813;\n",
      "Loss: 2.0917901544387525;\n",
      "Loss: 2.0918070343929003;\n",
      "Loss: 2.091919262871146;\n",
      "Loss: 2.0919491426885863;\n",
      "Loss: 2.0920794283616835;\n",
      "Loss: 2.0919755306732224;\n",
      "Loss: 2.0919311881491116;\n",
      "Loss: 2.09197594332695;\n",
      "Loss: 2.0921065443476965;\n",
      "Loss: 2.0924164801767504;\n",
      "Loss: 2.092928402992812;\n",
      "Loss: 2.093031191303489;\n",
      "Loss: 2.0930034122069676;\n",
      "Loss: 2.0930296189313404;\n",
      "Loss: 2.0933168947955836;\n",
      "Loss: 2.093429041139541;\n",
      "Loss: 2.093439007819967;\n",
      "Loss: 2.0937329548534596;\n",
      "Improved from 2.3408668365478515 to 2.313253167152405, saving model..\n",
      "Epoch: 14, Train loss: 2.094, Val loss: 2.313,            Epoch time=454.855s\n",
      "Пример\n",
      "Ты можешь это перевести ?\n",
      "Что ты собираешься с этим делать ?\n",
      "Быстроитель\n",
      "Loss: 1.9660092270374299;\n",
      "Loss: 1.9858151692152024;\n",
      "Loss: 1.9908446713288626;\n",
      "Loss: 1.986749327480793;\n",
      "Loss: 1.9893422944545747;\n",
      "Loss: 1.9873895090818405;\n",
      "Loss: 1.992315618480955;\n",
      "Loss: 1.9936025431752205;\n",
      "Loss: 1.9945969218677944;\n",
      "Loss: 1.996713546037674;\n",
      "Loss: 1.9988282904841683;\n",
      "Loss: 1.999294734597206;\n",
      "Loss: 1.9998905917314382;\n",
      "Loss: 2.000067264011928;\n",
      "Loss: 2.0016546402772266;\n",
      "Loss: 2.0017161282896994;\n",
      "Loss: 2.002820905447006;\n",
      "Loss: 2.003678021033605;\n",
      "Loss: 2.0051583598161997;\n",
      "Loss: 2.005844946742058;\n",
      "Loss: 2.0056366378352757;\n",
      "Loss: 2.0062357076731594;\n",
      "Loss: 2.0071773915705475;\n",
      "Loss: 2.007266328781843;\n",
      "Loss: 2.0082406717300416;\n",
      "Loss: 2.0084817042717567;\n",
      "Loss: 2.0086204187516814;\n",
      "Loss: 2.009045575772013;\n",
      "Loss: 2.010063877804526;\n",
      "Loss: 2.0102711968819302;\n",
      "Loss: 2.010681763079859;\n",
      "Loss: 2.0111525585874914;\n",
      "Loss: 2.0111396778352333;\n",
      "Loss: 2.0119864343895633;\n",
      "Loss: 2.0130943347045354;\n",
      "Loss: 2.013396482434538;\n",
      "Loss: 2.0136892912516724;\n",
      "Loss: 2.0137383028394296;\n",
      "Loss: 2.0138736576606067;\n",
      "Loss: 2.0141617546081543;\n",
      "Loss: 2.0146276156495255;\n",
      "Loss: 2.015047043079422;\n",
      "Loss: 2.015719938749491;\n",
      "Loss: 2.0159875659238207;\n",
      "Loss: 2.0163802252610523;\n",
      "Loss: 2.017177570410397;\n",
      "Loss: 2.0173084296825086;\n",
      "Loss: 2.017415514116486;\n",
      "Loss: 2.0183950203778793;\n",
      "Loss: 2.0183810462713243;\n",
      "Loss: 2.0188407693423476;\n",
      "Loss: 2.019068297308225;\n",
      "Loss: 2.0190886425297214;\n",
      "Loss: 2.019572044213613;\n",
      "Loss: 2.0197208158102904;\n",
      "Loss: 2.0203456168302467;\n",
      "Loss: 2.020316759724366;\n",
      "Loss: 2.020469263278205;\n",
      "Loss: 2.0205467149362724;\n",
      "Loss: 2.0205699728131292;\n",
      "Loss: 2.020516248452859;\n",
      "Loss: 2.0206764422885835;\n",
      "Loss: 2.0211086043668174;\n",
      "Loss: 2.021186936870217;\n",
      "Loss: 2.02126543373328;\n",
      "Loss: 2.021260523073601;\n",
      "Loss: 2.02171531896093;\n",
      "Loss: 2.0221312683470107;\n",
      "Loss: 2.0223626480586288;\n",
      "Loss: 2.022854950972966;\n",
      "Loss: 2.0232454100796873;\n",
      "Loss: 2.023362907750739;\n",
      "Loss: 2.0232274018411767;\n",
      "Loss: 2.0233411314841865;\n",
      "Loss: 2.023433961057663;\n",
      "Loss: 2.0235227021574973;\n",
      "Loss: 2.0237565644530506;\n",
      "Loss: 2.023893242065723;\n",
      "Loss: 2.023830301520191;\n",
      "Loss: 2.023923739045858;\n",
      "Loss: 2.0239961551295385;\n",
      "Loss: 2.024047298765764;\n",
      "Loss: 2.0239537593399186;\n",
      "Loss: 2.024024518359275;\n",
      "Loss: 2.0242371108531954;\n",
      "Loss: 2.0241896990981214;\n",
      "Loss: 2.02406170506587;\n",
      "Loss: 2.0242770610478793;\n",
      "Loss: 2.0244467796770373;\n",
      "Loss: 2.024454982797305;\n",
      "Loss: 2.02470495227929;\n",
      "Loss: 2.024928421404051;\n",
      "Loss: 2.0250725512094396;\n",
      "Loss: 2.0252938239878797;\n",
      "Loss: 2.025129020941885;\n",
      "Improved from 2.313253167152405 to 2.296598972558975, saving model..\n",
      "Epoch: 15, Train loss: 2.025, Val loss: 2.297,            Epoch time=446.875s\n",
      "Пример\n",
      "Ты можешь это переводить ?\n",
      "Что ты собираешься с этим делать ?\n",
      "Быстрое место\n",
      "Loss: 1.930046981573105;\n",
      "Loss: 1.9291999381780625;\n",
      "Loss: 1.9301955644289652;\n",
      "Loss: 1.9311615309119226;\n",
      "Loss: 1.9312240340709685;\n",
      "Loss: 1.9317944500843685;\n",
      "Loss: 1.9350287072999137;\n",
      "Loss: 1.9346624425053596;\n",
      "Loss: 1.9359960050053067;\n",
      "Loss: 1.9384102849960327;\n",
      "Loss: 1.940056969577616;\n",
      "Loss: 1.9415422138571738;\n",
      "Loss: 1.9422769088928515;\n",
      "Loss: 1.944026938676834;\n",
      "Loss: 1.9445286078453063;\n",
      "Loss: 1.9453172466903925;\n",
      "Loss: 1.9460780079925761;\n",
      "Loss: 1.9464694952964783;\n",
      "Loss: 1.9465898174988596;\n",
      "Loss: 1.947077613890171;\n",
      "Loss: 1.9484479971726736;\n",
      "Loss: 1.9493925892764872;\n",
      "Loss: 1.9498372016264045;\n",
      "Loss: 1.9504339876770973;\n",
      "Loss: 1.9509621246337892;\n",
      "Loss: 1.9518330309024223;\n",
      "Loss: 1.952580632854391;\n",
      "Loss: 1.9526872099297388;\n",
      "Loss: 1.9531853205171126;\n",
      "Loss: 1.9538257628679276;\n",
      "Loss: 1.9547416732388159;\n",
      "Loss: 1.954528982155025;\n",
      "Loss: 1.9553052338686856;\n",
      "Loss: 1.9555418178263833;\n",
      "Loss: 1.955294257266181;\n",
      "Loss: 1.9561749857001836;\n",
      "Loss: 1.9559781505932679;\n",
      "Loss: 1.9566646294844778;\n",
      "Loss: 1.957024109424689;\n",
      "Loss: 1.9576458009183406;\n",
      "Loss: 1.9576171368215143;\n",
      "Loss: 1.9580017838591621;\n",
      "Loss: 1.9582183101565338;\n",
      "Loss: 1.958377883813598;\n",
      "Loss: 1.958353160434299;\n",
      "Loss: 1.958797931437907;\n",
      "Loss: 1.959037173276252;\n",
      "Loss: 1.9596296174575885;\n",
      "Loss: 1.9591947838724877;\n",
      "Loss: 1.959504379916191;\n",
      "Loss: 1.959944869817472;\n",
      "Loss: 1.9604096586429156;\n",
      "Loss: 1.9601719838493274;\n",
      "Loss: 1.9604633610116111;\n",
      "Loss: 1.9607182050401515;\n",
      "Loss: 1.9603104240979468;\n",
      "Loss: 1.9606313202673928;\n",
      "Loss: 1.960559371360417;\n",
      "Loss: 1.961023749052468;\n",
      "Loss: 1.9611872732043267;\n",
      "Loss: 1.9611308474032605;\n",
      "Loss: 1.9612248375915713;\n",
      "Loss: 1.9612607524886965;\n",
      "Loss: 1.9616185405664146;\n",
      "Loss: 1.9615922570045177;\n",
      "Loss: 1.9614750706788264;\n",
      "Loss: 1.9617617948197608;\n",
      "Loss: 1.9620002133180114;\n",
      "Loss: 1.9620734605409098;\n",
      "Loss: 1.9623289939676012;\n",
      "Loss: 1.9621515666263205;\n",
      "Loss: 1.9626242477032874;\n",
      "Loss: 1.9626975543205052;\n",
      "Loss: 1.9627002972203331;\n",
      "Loss: 1.9626007465998332;\n",
      "Loss: 1.9626695987111644;\n",
      "Loss: 1.9627306728239182;\n",
      "Loss: 1.9630106687240112;\n",
      "Loss: 1.963167036439799;\n",
      "Loss: 1.9631796555668115;\n",
      "Loss: 1.9636556250519224;\n",
      "Loss: 1.9638915860507546;\n",
      "Loss: 1.9640056553088039;\n",
      "Loss: 1.9642364235009466;\n",
      "Loss: 1.9642511152800393;\n",
      "Loss: 1.9643021179493083;\n",
      "Loss: 1.964604478951158;\n",
      "Loss: 1.9645358449627053;\n",
      "Loss: 1.9646144943558768;\n",
      "Loss: 1.964671379605929;\n",
      "Loss: 1.964561698109239;\n",
      "Loss: 1.9648346459476844;\n",
      "Loss: 1.9649519213425215;\n",
      "Loss: 1.9650326127828435;\n",
      "Loss: 1.965183391934947;\n",
      "Improved from 2.296598972558975 to 2.2865261645317076, saving model..\n",
      "Epoch: 16, Train loss: 1.965, Val loss: 2.287,            Epoch time=457.780s\n",
      "Пример\n",
      "Ты можешь это переводить ?\n",
      "Что ты собираешься делать с этим ?\n",
      "Быстрое\n",
      "Loss: 1.893807168006897;\n",
      "Loss: 1.8940200167894363;\n",
      "Loss: 1.894725835720698;\n",
      "Loss: 1.8899932083487512;\n",
      "Loss: 1.8955507428646088;\n",
      "Loss: 1.8948718053102493;\n",
      "Loss: 1.8948427392755236;\n",
      "Loss: 1.8974630238115788;\n",
      "Loss: 1.8973633295959897;\n",
      "Loss: 1.8992122370004654;\n",
      "Loss: 1.9000812058015304;\n",
      "Loss: 1.8999204873045286;\n",
      "Loss: 1.9014701377428496;\n",
      "Loss: 1.9006601859842027;\n",
      "Loss: 1.8997673740386962;\n",
      "Loss: 1.8994915487617254;\n",
      "Loss: 1.8995165580861708;\n",
      "Loss: 1.9012335091167025;\n",
      "Loss: 1.902989020410337;\n",
      "Loss: 1.90381074988842;\n",
      "Loss: 1.9035203539189838;\n",
      "Loss: 1.9030126809531993;\n",
      "Loss: 1.9044662336162899;\n",
      "Loss: 1.9042400961120924;\n",
      "Loss: 1.9044481526374817;\n",
      "Loss: 1.904010762984936;\n",
      "Loss: 1.9044406514697605;\n",
      "Loss: 1.9038721131852694;\n",
      "Loss: 1.9039304933054695;\n",
      "Loss: 1.9040989588896433;\n",
      "Loss: 1.9033743066941538;\n",
      "Loss: 1.903435554765165;\n",
      "Loss: 1.9040027899453134;\n",
      "Loss: 1.904342567990808;\n",
      "Loss: 1.9038380110945021;\n",
      "Loss: 1.9045450357264944;\n",
      "Loss: 1.905029094058114;\n",
      "Loss: 1.9052423468075301;\n",
      "Loss: 1.9054973992934594;\n",
      "Loss: 1.9055860062837602;\n",
      "Loss: 1.906152248353493;\n",
      "Loss: 1.9067460775375367;\n",
      "Loss: 1.9070113728212756;\n",
      "Loss: 1.9083191708001224;\n",
      "Loss: 1.9085504456626043;\n",
      "Loss: 1.9090780773370162;\n",
      "Loss: 1.909627462752322;\n",
      "Loss: 1.9099644351998966;\n",
      "Loss: 1.9101906915100253;\n",
      "Loss: 1.910650281739235;\n",
      "Loss: 1.9107533397394068;\n",
      "Loss: 1.910915955396799;\n",
      "Loss: 1.9112216246128082;\n",
      "Loss: 1.9113225094698094;\n",
      "Loss: 1.9113472001119094;\n",
      "Loss: 1.911244066549199;\n",
      "Loss: 1.9114647482779987;\n",
      "Loss: 1.9116992318835753;\n",
      "Loss: 1.91148977604963;\n",
      "Loss: 1.9117820243438084;\n",
      "Loss: 1.9118751527833158;\n",
      "Loss: 1.9120374479409188;\n",
      "Loss: 1.9124100729775808;\n",
      "Loss: 1.9123037812300026;\n",
      "Loss: 1.9127317767876846;\n",
      "Loss: 1.912682394439524;\n",
      "Loss: 1.9129665244515262;\n",
      "Loss: 1.9130621618207764;\n",
      "Loss: 1.9130904511092366;\n",
      "Loss: 1.9133354971068246;\n",
      "Loss: 1.9129413254831877;\n",
      "Loss: 1.9127910334865252;\n",
      "Loss: 1.9127620309183042;\n",
      "Loss: 1.9128315132534182;\n",
      "Loss: 1.912815804751714;\n",
      "Loss: 1.9131683267731416;\n",
      "Loss: 1.9132215493994873;\n",
      "Loss: 1.913203423160773;\n",
      "Loss: 1.9135345646399486;\n",
      "Loss: 1.9136140418946743;\n",
      "Loss: 1.913609716936394;\n",
      "Loss: 1.9136830021113884;\n",
      "Loss: 1.9136953354456339;\n",
      "Loss: 1.9139592115793909;\n",
      "Loss: 1.91409964669452;\n",
      "Loss: 1.9141134189173232;\n",
      "Loss: 1.9142339535417228;\n",
      "Loss: 1.9143612595986237;\n",
      "Loss: 1.9145146643177846;\n",
      "Loss: 1.9147912729051377;\n",
      "Loss: 1.915044263627503;\n",
      "Loss: 1.915242537882017;\n",
      "Loss: 1.9153106306573395;\n",
      "Loss: 1.915400850354357;\n",
      "Loss: 1.9155119314444693;\n",
      "Improved from 2.2865261645317076 to 2.2815989968776704, saving model..\n",
      "Epoch: 17, Train loss: 1.916, Val loss: 2.282,            Epoch time=460.352s\n",
      "Пример\n",
      "Ты можешь это переводить ?\n",
      "Что ты собираешься делать с этим ?\n",
      "Трансформация\n",
      "Loss: 1.8506323838233947;\n",
      "Loss: 1.861740737557411;\n",
      "Loss: 1.8609294446309408;\n",
      "Loss: 1.8613754370808602;\n",
      "Loss: 1.859229484796524;\n",
      "Loss: 1.8597258132696153;\n",
      "Loss: 1.8618821208817617;\n",
      "Loss: 1.8628244912624359;\n",
      "Loss: 1.8632300158341726;\n",
      "Loss: 1.8654378508329392;\n",
      "Loss: 1.8653916958245365;\n",
      "Loss: 1.8659908477465312;\n",
      "Loss: 1.8660731101953065;\n",
      "Loss: 1.8664463775498525;\n",
      "Loss: 1.8659851994514465;\n",
      "Loss: 1.864945840537548;\n",
      "Loss: 1.8658833383111393;\n",
      "Loss: 1.8664900258514616;\n",
      "Loss: 1.8669346811269458;\n",
      "Loss: 1.867941137254238;\n",
      "Loss: 1.8681735554195587;\n",
      "Loss: 1.8689870196039027;\n",
      "Loss: 1.8693721000007961;\n",
      "Loss: 1.8698138165970644;\n",
      "Loss: 1.8701451773166657;\n",
      "Loss: 1.8705079121314563;\n",
      "Loss: 1.8702280075020261;\n",
      "Loss: 1.8699338106598173;\n",
      "Loss: 1.8703841141585646;\n",
      "Loss: 1.8713561206658682;\n",
      "Loss: 1.8712644350528718;\n",
      "Loss: 1.8721519524604082;\n",
      "Loss: 1.8715846066763906;\n",
      "Loss: 1.8720175669824375;\n",
      "Loss: 1.8718816412517003;\n",
      "Loss: 1.873196008304755;\n",
      "Loss: 1.8729106045735848;\n",
      "Loss: 1.8724052669186342;\n",
      "Loss: 1.8723863546359234;\n",
      "Loss: 1.8722561309039594;\n",
      "Loss: 1.8730370618366614;\n",
      "Loss: 1.8730727301041286;\n",
      "Loss: 1.8730999688769496;\n",
      "Loss: 1.8733300733024423;\n",
      "Loss: 1.8737920682430267;\n",
      "Loss: 1.8740201888395391;\n",
      "Loss: 1.874125289688719;\n",
      "Loss: 1.8744542450209458;\n",
      "Loss: 1.8747888711763887;\n",
      "Loss: 1.8749555375814437;\n",
      "Loss: 1.8751504159674925;\n",
      "Loss: 1.8754821896782288;\n",
      "Loss: 1.8756225567043951;\n",
      "Loss: 1.875408797175796;\n",
      "Loss: 1.875316148779609;\n",
      "Loss: 1.8755252929244723;\n",
      "Loss: 1.8756973160777175;\n",
      "Loss: 1.8760342443194882;\n",
      "Loss: 1.8760730129581387;\n",
      "Loss: 1.8759235555330913;\n",
      "Loss: 1.8759365311606986;\n",
      "Loss: 1.8757387000706889;\n",
      "Loss: 1.8759479136504824;\n",
      "Loss: 1.876216924916953;\n",
      "Loss: 1.8759411826683925;\n",
      "Loss: 1.8761300448034748;\n",
      "Loss: 1.8762093067880887;\n",
      "Loss: 1.8764894900251838;\n",
      "Loss: 1.8766602898853413;\n",
      "Loss: 1.876848137974739;\n",
      "Loss: 1.8770425781061952;\n",
      "Loss: 1.8771323700249196;\n",
      "Loss: 1.877420920607162;\n",
      "Loss: 1.877646917056393;\n",
      "Loss: 1.8778921401659647;\n",
      "Loss: 1.8775343776847186;\n",
      "Loss: 1.8774417882151417;\n",
      "Loss: 1.8775065381863179;\n",
      "Loss: 1.8777430003655107;\n",
      "Loss: 1.8777781324982643;\n",
      "Loss: 1.8778686958036306;\n",
      "Loss: 1.8777984137360642;\n",
      "Loss: 1.8776217896679799;\n",
      "Loss: 1.8773901154029937;\n",
      "Loss: 1.877334772418527;\n",
      "Loss: 1.8774035828889803;\n",
      "Loss: 1.877550227929806;\n",
      "Loss: 1.8772553185034881;\n",
      "Loss: 1.877547257977925;\n",
      "Loss: 1.878051669822799;\n",
      "Loss: 1.8780865215207194;\n",
      "Loss: 1.8781284178210342;\n",
      "Loss: 1.8785142014488097;\n",
      "Loss: 1.878321611729074;\n",
      "Loss: 1.878379114038066;\n",
      "Improved from 2.2815989968776704 to 2.2793541426658632, saving model..\n",
      "Epoch: 18, Train loss: 1.878, Val loss: 2.279,            Epoch time=461.297s\n",
      "Пример\n",
      "Ты можешь это переводить ?\n",
      "Что ты собираешься делать с этим ?\n",
      "Трансформация\n",
      "Loss: 1.851115686893463;\n",
      "Loss: 1.8533953100442886;\n",
      "Loss: 1.8567542239030201;\n",
      "Loss: 1.851383889913559;\n",
      "Loss: 1.8475170364379883;\n",
      "Loss: 1.8482647637526195;\n",
      "Loss: 1.8498026883602143;\n",
      "Loss: 1.8487502463161944;\n",
      "Loss: 1.8484413184059991;\n",
      "Loss: 1.8476385575532914;\n",
      "Loss: 1.8481667516448281;\n",
      "Loss: 1.8500519943237306;\n",
      "Loss: 1.8506602403750787;\n",
      "Loss: 1.851376399568149;\n",
      "Loss: 1.850554274002711;\n",
      "Loss: 1.850923155322671;\n",
      "Loss: 1.851935669814839;\n",
      "Loss: 1.8511825368801753;\n",
      "Loss: 1.8522508612431978;\n",
      "Loss: 1.8521712069511413;\n",
      "Loss: 1.8517443438938685;\n",
      "Loss: 1.8515131048722702;\n",
      "Loss: 1.8516506003815194;\n",
      "Loss: 1.8524560626844566;\n",
      "Loss: 1.8523046608448028;\n",
      "Loss: 1.8514655858736773;\n",
      "Loss: 1.8515229606186903;\n",
      "Loss: 1.851474518605641;\n",
      "Loss: 1.8517433544274033;\n",
      "Loss: 1.8516521898508072;\n",
      "Loss: 1.8519371439180066;\n",
      "Loss: 1.8524824249744416;\n",
      "Loss: 1.8527082342451269;\n",
      "Loss: 1.8529927271604538;\n",
      "Loss: 1.8530954566001892;\n",
      "Loss: 1.8528094791703753;\n",
      "Loss: 1.852993220574147;\n",
      "Loss: 1.8532454156248193;\n",
      "Loss: 1.8532416622455303;\n",
      "Loss: 1.853459933757782;\n",
      "Loss: 1.853332976102829;\n",
      "Loss: 1.8530849796817417;\n",
      "Loss: 1.8528474867898364;\n",
      "Loss: 1.852149510573257;\n",
      "Loss: 1.852010918961631;\n",
      "Loss: 1.8519361177734708;\n",
      "Loss: 1.8517630426934424;\n",
      "Loss: 1.851782755802075;\n",
      "Loss: 1.851939539398466;\n",
      "Loss: 1.8520107211589814;\n",
      "Loss: 1.8523920288974163;\n",
      "Loss: 1.852352952819604;\n",
      "Loss: 1.8523099995559116;\n",
      "Loss: 1.8520104406498097;\n",
      "Loss: 1.851522369666533;\n",
      "Loss: 1.852134720576661;\n",
      "Loss: 1.8522769657143376;\n",
      "Loss: 1.8525198205586138;\n",
      "Loss: 1.8523277550026522;\n",
      "Loss: 1.8525978494683901;\n",
      "Loss: 1.8519999091547044;\n",
      "Loss: 1.852101679309722;\n",
      "Loss: 1.852192437592007;\n",
      "Loss: 1.8524968243762852;\n",
      "Loss: 1.8526760938717768;\n",
      "Loss: 1.8526671078168984;\n",
      "Loss: 1.852597236277452;\n",
      "Loss: 1.8527579282837756;\n",
      "Loss: 1.8524447425206503;\n",
      "Loss: 1.852411091021129;\n",
      "Loss: 1.8523100804946793;\n",
      "Loss: 1.8520363087621;\n",
      "Loss: 1.852192328384478;\n",
      "Loss: 1.8523711138319325;\n",
      "Loss: 1.8520251657803852;\n",
      "Loss: 1.8519736448557753;\n",
      "Loss: 1.8521831628564116;\n",
      "Loss: 1.8521460927296907;\n",
      "Loss: 1.8524522439739373;\n",
      "Loss: 1.8524440495073795;\n",
      "Loss: 1.8523968024312714;\n",
      "Loss: 1.8522603937329316;\n",
      "Loss: 1.8521166154993587;\n",
      "Loss: 1.8522277675214267;\n",
      "Loss: 1.852474810109419;\n",
      "Loss: 1.8525873702071434;\n",
      "Loss: 1.852590955906901;\n",
      "Loss: 1.852667641897093;\n",
      "Loss: 1.8528401796737415;\n",
      "Loss: 1.8529851458337572;\n",
      "Loss: 1.8528873254047646;\n",
      "Loss: 1.8528647103776104;\n",
      "Loss: 1.8529221627020067;\n",
      "Loss: 1.85302162672611;\n",
      "Loss: 1.8531085689193325;\n",
      "Epoch: 19, Train loss: 1.853, Val loss: 2.280,            Epoch time=449.939s\n",
      "Пример\n",
      "Ты можешь это переводить ?\n",
      "Что ты собираешься делать с этим ?\n",
      "Трансформация\n",
      "Loss: 1.8216256415843963;\n",
      "Loss: 1.8345227831602096;\n",
      "Loss: 1.8371327463785807;\n",
      "Loss: 1.8386522176861764;\n",
      "Loss: 1.83716348528862;\n",
      "Loss: 1.8399897613128027;\n",
      "Loss: 1.8376912008013044;\n",
      "Loss: 1.8372628873586654;\n",
      "Loss: 1.8372531407409245;\n",
      "Loss: 1.8386216461658478;\n",
      "Loss: 1.8362732512300666;\n",
      "Loss: 1.8371355753143628;\n",
      "Loss: 1.8391360670786638;\n",
      "Loss: 1.8392589386871883;\n",
      "Loss: 1.8398214330673217;\n",
      "Loss: 1.840232248082757;\n",
      "Loss: 1.8412272011532502;\n",
      "Loss: 1.8404523532920414;\n",
      "Loss: 1.8405758483158914;\n",
      "Loss: 1.8410099402070046;\n",
      "Loss: 1.841432945387704;\n",
      "Loss: 1.8414381692084398;\n",
      "Loss: 1.8411558777353039;\n",
      "Loss: 1.840512193441391;\n",
      "Loss: 1.8409379197597504;\n",
      "Loss: 1.8414273505944472;\n",
      "Loss: 1.8415718079054797;\n",
      "Loss: 1.8418926213468825;\n",
      "Loss: 1.8418211765535946;\n",
      "Loss: 1.841183356165886;\n",
      "Loss: 1.841121001166682;\n",
      "Loss: 1.8412989535182716;\n",
      "Loss: 1.8415473466930967;\n",
      "Loss: 1.8415132897040423;\n",
      "Loss: 1.8413120900222233;\n",
      "Loss: 1.8410296825236745;\n",
      "Loss: 1.8407048861722688;\n",
      "Loss: 1.8409125270341573;\n",
      "Loss: 1.840765505081568;\n",
      "Loss: 1.8404464893341064;\n",
      "Loss: 1.8404842384268598;\n",
      "Loss: 1.840525406485512;\n",
      "Loss: 1.840360368157542;\n",
      "Loss: 1.8398352535204454;\n",
      "Loss: 1.8401906337208218;\n",
      "Loss: 1.8408443025402401;\n",
      "Loss: 1.8402750834505608;\n",
      "Loss: 1.8400119445472956;\n",
      "Loss: 1.8395724833498195;\n",
      "Loss: 1.8385989139795302;\n",
      "Loss: 1.8387730553570916;\n",
      "Loss: 1.838872258411004;\n",
      "Loss: 1.8388903292619958;\n",
      "Loss: 1.8383937671007933;\n",
      "Loss: 1.8383589641614393;\n",
      "Loss: 1.8385154877603054;\n",
      "Loss: 1.8389110834347575;\n",
      "Loss: 1.8385942777888529;\n",
      "Loss: 1.8391132884308443;\n",
      "Loss: 1.8391482382019362;\n",
      "Loss: 1.8392422114630214;\n",
      "Loss: 1.8392772432873326;\n",
      "Loss: 1.8391557074160803;\n",
      "Loss: 1.839621220063418;\n",
      "Loss: 1.839569179754991;\n",
      "Loss: 1.8396445433479367;\n",
      "Loss: 1.8397553720225148;\n",
      "Loss: 1.839939602701103;\n",
      "Loss: 1.840076520546623;\n",
      "Loss: 1.8403931104455675;\n",
      "Loss: 1.840309179164994;\n",
      "Loss: 1.840238651053773;\n",
      "Loss: 1.840078472486914;\n",
      "Loss: 1.840184180478792;\n",
      "Loss: 1.8400796730677287;\n",
      "Loss: 1.8399298789626675;\n",
      "Loss: 1.8399516217584735;\n",
      "Loss: 1.8398925294784398;\n",
      "Loss: 1.8396580318710472;\n",
      "Loss: 1.8396021832376719;\n",
      "Loss: 1.8397305389392522;\n",
      "Loss: 1.839640022807005;\n",
      "Loss: 1.8396340422888837;\n",
      "Loss: 1.8396791909706025;\n",
      "Loss: 1.8394438136184916;\n",
      "Loss: 1.8396341228900954;\n",
      "Loss: 1.839659796939499;\n",
      "Loss: 1.8394836810095743;\n",
      "Loss: 1.8394210059723157;\n",
      "Loss: 1.839780050304201;\n",
      "Loss: 1.8396533835196234;\n",
      "Loss: 1.8394319696789203;\n",
      "Loss: 1.8395287102781317;\n",
      "Loss: 1.8396478771782936;\n",
      "Loss: 1.8398629520817806;\n",
      "Epoch: 20, Train loss: 1.840, Val loss: 2.281,            Epoch time=461.169s\n",
      "Пример\n",
      "Ты можешь это переводить ?\n",
      "Что ты собираешься делать с этим ?\n",
      "Трансформация\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train(model, training_generator, optimizer, loss_fn, scheduler, run)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(model, valid_generator, loss_fn, run)\n",
    "\n",
    "    if not losses:\n",
    "        print(f'First epoch - {val_loss}, saving model..')\n",
    "        torch.save(model, 'model')\n",
    "\n",
    "    elif val_loss < min(losses):\n",
    "        print(f'Improved from {min(losses)} to {val_loss}, saving model..')\n",
    "        torch.save(model, 'model')\n",
    "\n",
    "    losses.append(val_loss)\n",
    "\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \\\n",
    "           \"f\"Epoch time={(end_time-start_time):.3f}s\"))\n",
    "\n",
    "    print(translate(\"Example\"))\n",
    "    print(translate('Can you translate that?'))\n",
    "    print(translate('What are you going to do with that?'))\n",
    "    print(translate('Transformer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce9c577-3e8e-4a55-8c89-e5792c78685d",
   "metadata": {
    "id": "8ce9c577-3e8e-4a55-8c89-e5792c78685d"
   },
   "outputs": [],
   "source": [
    "# run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243fa21c-c672-460d-96e6-def7f5ec287c",
   "metadata": {
    "id": "243fa21c-c672-460d-96e6-def7f5ec287c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
