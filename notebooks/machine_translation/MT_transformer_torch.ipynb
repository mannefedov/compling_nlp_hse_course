{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13efba76",
   "metadata": {},
   "source": [
    "# Дисклеймер\n",
    "Эту тетрадку нужно запускать в колабе или в vast.ai. Не мучатесь с установкой библиотек и с обучением на cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d650e9eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install tokenizers matplotlib scikit-learn\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126 -U\n",
    "# !pip install torchtune torchao\n",
    "# !pip install --upgrade 'optree>=0.13.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d338ab5-79fd-43f3-9c8d-ebb6a67f7f9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b03d3b-5816-4830-b69e-f3ea4a826e17",
   "metadata": {},
   "source": [
    "Помимо самих трансформеров давайте также попробуем сервис для отслеживания экспериментов W & B (weights and biases). \n",
    "До этого мы обходились просто выводом метрик в тетрадке, но это не серьезно. Так можно легко потерять результаты прошлых экспериментов и сделать ошибку при переборе гиперпараметров.\n",
    "W&B не единственный такой сервис, но он бесплатно предоставляет облачное хранилище и визуализацию, поэтому попробуем его. \n",
    "Чтобы залогиниться в w&b в тетрадке, вам нужно пойти на сайт wandb.ai и залогиниться там, а потом создать проект и скопировать ключ в ячейку ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf913f78-c71a-4dcf-abf2-a0cc12daeef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!wandb login KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931eb4c6-eb25-48ff-b363-ff234da53e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "879d23c5-55a9-493c-a1e8-8798f0969bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "035afaed-45ca-4dee-a200-67357b0f4002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# самый простой пример инициализации эксперимента (run)\n",
    "run = wandb.init(\n",
    "    project=\"course\",\n",
    "    name=\"test_run\",\n",
    "    # в конфиг можно писать все что угодно\n",
    "    config={\n",
    "        \"test\": True\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74a48668-cb5d-4475-955a-16e61d2bb3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# далее можно логировать метрики (один или много раз)\n",
    "wandb.log({\"accuracy\": 1.0, \"loss\": 0.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "505ef7f6-0bb9-41ad-9903-d45bfd7804ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# так можно закончить эксперимент\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e384bd5c",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69baacc",
   "metadata": {},
   "source": [
    "Это уже 3-й семинар про трансформеры и только сейчас мы попробуем сделать модель, которая изначально и была описана в Attention is all you need. Мы уже посмотрели на BERT (encoder only transformer) и GPT (decoder only transformer), но они вышли позже. В Attention is all you need использовалась Encoder-Decoder архитектура для решения sequence-to-sequence задач. Давайте попробуем собрать такую модель. \n",
    "В этот посмотрим на готовые трансформерные классы в torch, чтобы использовать побольше готового и не писать все с нуля каждый раз."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c794a6",
   "metadata": {},
   "source": [
    "Будем обучать модель на задаче машинного перевода (самая классическая проблема в NLP). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "947b3313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers import decoders\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "from torchtune.modules import RotaryPositionalEmbeddings\n",
    "from torch.nn import Transformer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163b41c9-262a-4b31-b75e-999d5d6a55cc",
   "metadata": {},
   "source": [
    "Данные взяты вот отсюда - https://opus.nlpl.eu/opus-100.php  \n",
    "Помимо en-ru пары там можно найти много других параллельных корпусов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "307b759c-fee4-41e8-8b90-0355911abd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\n",
    "# !wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\n",
    "# !wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.ru\n",
    "# !wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "415f5ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# в русскоязычных данных есть \\xa0 вместо пробелов, он может некорректно обрабатываться токенизатором\n",
    "text = open('opus.en-ru-train.ru').read().replace('\\xa0', ' ')\n",
    "f = open('opus.en-ru-train.ru', 'w')\n",
    "f.write(text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e110ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sents = open('opus.en-ru-train.en').read().splitlines()\n",
    "ru_sents = open('opus.en-ru-train.ru').read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c009c96e",
   "metadata": {},
   "source": [
    "Примеры перевода с английского на русский. Можно увидеть, что тексты достаточно разнообразные и часто неформальные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eb9b498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('So what are you thinking?', 'Ну и что ты думаешь?')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(en_sents[:10], ru_sents[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39921c4",
   "metadata": {},
   "source": [
    "Как обычно нам нужен токенизатор, а точнее даже 2 - под каждый язык. Можно совместить все в один токенизатор и даже иметь одну общую матрицу с эмбедингами в encoder и decoder, но для простоты мы их разделим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b79b4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer_en = Tokenizer(BPE())\n",
    "tokenizer_en.pre_tokenizer = Whitespace()\n",
    "# в encoder нам не нужно обозначать начало и конец поэтому единственный доп токен это паддинг\n",
    "trainer_en = BpeTrainer(special_tokens=[\"[PAD]\"], end_of_word_suffix='</w>')\n",
    "tokenizer_en.train(files=[\"opus.en-ru-train.en\"], trainer=trainer_en)\n",
    "\n",
    "tokenizer_ru = Tokenizer(BPE())\n",
    "tokenizer_ru.pre_tokenizer = Whitespace()\n",
    "# в декодере добавим теги начала и конца для корректной генерации\n",
    "trainer_ru = BpeTrainer(special_tokens=[\"[PAD]\", \"[BOS]\", \"[EOS]\"], end_of_word_suffix='</w>')\n",
    "tokenizer_ru.train(files=[\"opus.en-ru-train.ru\"], trainer=trainer_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "009e0125-67df-4a3e-ab9e-26d1f78183e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en.decoder = decoders.BPEDecoder()\n",
    "tokenizer_ru.decoder = decoders.BPEDecoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b56d3b",
   "metadata": {},
   "source": [
    "Токенизатор - это неотъемлимая часть модели, поэтому не забывайте сохранять токенизатор вместе с моделью. Если вы забудете про это и переобучите токенизатор, то индексы токенов разойдутся и веса модели будут бесполезны. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dd90665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# раскоментируйте эту ячейку при обучении токенизатора\n",
    "# а потом снова закоментируйте чтобы при перезапуске не перезаписать токенизаторы\n",
    "tokenizer_en.save('tokenizer_en')\n",
    "tokenizer_ru.save('tokenizer_ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e0f7f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer.from_file(\"tokenizer_en\")\n",
    "tokenizer_ru = Tokenizer.from_file(\"tokenizer_ru\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9fecf3",
   "metadata": {},
   "source": [
    "Переводим текст в индексы вот таким образом. \n",
    "\n",
    "В начало русских текстов добавляем токен '[BOS]', а в конец '[EOS]'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc003758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, tokenizer, max_len, encoder=False):\n",
    "    if encoder:\n",
    "        return tokenizer.encode(text).ids[:max_len]\n",
    "    else:\n",
    "        return [tokenizer.token_to_id('[BOS]')] + tokenizer.encode(text).ids[:max_len] + [tokenizer.token_to_id('[EOS]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96920fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# важно следить чтобы индекс паддинга совпадал в токенизаторе с value в pad_sequences\n",
    "# у нас это в любом случае ноль но лучше safe than sorry\n",
    "PAD_IDX = tokenizer_ru.token_to_id('[PAD]')\n",
    "PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cc0a376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ограничимся длинной в 47 и 48 (разные чтобы показать что в seq2seq не нужна одинаковая длина)\n",
    "# отличаться на 1 они тоже не должна, длины могут быть любые\n",
    "max_len_en, max_len_ru = 47, 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc2dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_en = [encode(t, tokenizer_en, max_len_en, encoder=True) for t in en_sents]\n",
    "X_ru = [encode(t, tokenizer_ru, max_len_ru) for t in ru_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cc48199-c1d6-4217-898a-c12a244779c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 1000000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# миллион примеров \n",
    "len(X_en), len(X_ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7655a4ea",
   "metadata": {},
   "source": [
    "Паддинг внутри класса для датасета. Еще обратите внимание, что тут не стоит параметр batch_first=True как раньше\n",
    "\n",
    "В торче принято, что размерность батча идет в конце и пример кода с трансформером расчитан на это. Конечно можно поменять сам код модели, но это сложнее, чем просто изменить тензор с данными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7634853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, texts_en, texts_ru):\n",
    "        self.texts_en = [torch.LongTensor(sent) for sent in texts_en]\n",
    "        self.texts_en = torch.nn.utils.rnn.pad_sequence(self.texts_en, batch_first=True, padding_value=PAD_IDX)\n",
    "        \n",
    "        self.texts_ru = [torch.LongTensor(sent) for sent in texts_ru]\n",
    "        self.texts_ru = torch.nn.utils.rnn.pad_sequence(self.texts_ru, batch_first=True, padding_value=PAD_IDX)\n",
    "\n",
    "        self.length = len(texts_en)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        ids_en = self.texts_en[index]\n",
    "        ids_ru = self.texts_ru[index]\n",
    "\n",
    "        return ids_en, ids_ru"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec889a8d",
   "metadata": {},
   "source": [
    "Разбиваем на трейн и тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c9eaf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_en_train, X_en_valid, X_ru_train, X_ru_valid = train_test_split(X_en, X_ru, test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bb0e70",
   "metadata": {},
   "source": [
    "# Код трансформера\n",
    "\n",
    "Сначала попробуем `nn.MultiheadAttention`, который реализует механизм внимания. Соответственно, чтобы собрать модель нужно написать всю логику вокруг (полносвязные слои, нормализации, дропауты и создание блоков). \n",
    "\n",
    "В encoder-decoder архитектуре два типа внимания - self-attention и cross-attention. Оба реализуются через MultiheadAttention и работают практически идентично. Единственное отличие - что является исходными векторами, к которым применяется query, key, value преобразование. В self-attention все исходные значения берутся из одного и того же текста. В cross-attention в query подаются эмбединги одного текста (в нашем случае русского), а в key, value - эмбединги другого текста (в нашем случае английского).\n",
    "\n",
    "Внутри nn.MultiheadAttention уже реализована логика превращения изначальных эмбедингов в query, key, value вектора, поэтому в этой слой нужно передать только сами эмбединги, к которым будет применено это преобразование.   \n",
    "Обратите внимание на вызов self-attention ниже - `self.self_attn(src, src, src, ...)`. Изначальные эмбединги дублируются и передаются как позиционные аргументы. Можно еще представить это как `self.self_attn(query=src, key=src, value=src, ...)`.  \n",
    "Вызов cross-attention выглядит вот так - `self.cross_attn(tgt, memory, memory, ...)` (или другими словами в query идет закодированный русский текст, а в key и value - значения, которые вернул encoder).\n",
    "\n",
    "Логика разделения на головы (heads) тоже спрятана внутри MultiheadAttention. Она состоит в том, что исходные вектора разрезаются на равные куски и внимание рассчитывается между этими кусочками.  \n",
    "\n",
    "Наверное самое сложное здесь - это маскирование. По умолчанию MultiheadAttention никак не ограничивает коммуникацию между токенами. Однако в decoder нам нужно, чтобы каждый токен смотрел только на предыдущие. Поэтому в decoder нам нужно передать треугольную маску, которая применится к attention scores (скалярное произведение query и key векторов) и для каждого токена занулит внимание к будущим токенам.\n",
    "Также мы используем padding для выравнивания длин текстов, чтобы легко представить их как тензоры. Паддинг токен внутри модели ведет себя также как и другие токены - ему будет сопоставлен эмбединг и соответственно он будет участвовать в расчете внимания. Чтобы исключить его из расчетов нужно передать еще одну маску. У английского и русского текстов будут свои маски (так как длины разные). В cross-attention будут использоваться сразу обе маски, так как эмбединги происходят из обоих текстов. \n",
    "\n",
    "Еще один важный элемент - это RotaryPositionalEmbeddings. Это еще один способ делать позиционной кодирование. Сейчас это наиболее широко применяемый метод. Подробнее про его устройство можно почитать вот тут - https://medium.com/ai-insights-cobet/rotary-positional-embeddings-a-detailed-look-and-comprehensive-understanding-4ff66a874d83\n",
    "Если коротко, то он состоит в том, чтобы применить трасформацию к изначальным эмбедингам в зависимости от их позиции.\n",
    "RotaryPositionalEmbeddings в торче ожидает на вход эмбединги уже разделенные на куски (heads), поэтому в коде можно увидеть такое преобразование - `self.positional_encoding(src_embedded.view(B, S, self.num_heads, E // self.num_heads)).view(B, S, E)`\n",
    "Сначала эмбединги режутся на куски, к ним применяется позиционное кодирование и затем все возвращается к исходным размерам (куски соединяются обратно)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aca04f07-728d-41e9-8747-add7dfc84cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# для encoder и decoder создается свой класс\n",
    "# это сделано для того чтобы можно было легко задать количество слоев как гиперпараметр\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        # здесь нормализация применяется после attention (как в оригинальной статье)\n",
    "        # сейчас чаще используют пре-нормализацию\n",
    "        src2, _ = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask) # mha\n",
    "        src = self.norm1(src + self.dropout(src2)) # norm + residual connection\n",
    "        src2 = self.ff(src) # ffd\n",
    "        src = self.norm2(src + self.dropout(src2)) # norm + residual connection\n",
    "\n",
    "        return src\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        tgt2, _ = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask) # self mha\n",
    "        tgt = self.norm1(tgt + self.dropout(tgt2)) # norm + residual connection\n",
    "\n",
    "        tgt2, _ = self.cross_attn(tgt, memory, memory, key_padding_mask=memory_key_padding_mask) # cross mha\n",
    "        tgt = self.norm2(tgt + self.dropout(tgt2)) # norm + residual connection\n",
    "\n",
    "        tgt2 = self.ff(tgt) # ffd \n",
    "        tgt = self.norm3(tgt + self.dropout(tgt2))  # norm + residual connection\n",
    "\n",
    "        return tgt\n",
    "\n",
    "\n",
    "# главнный класс где все собирается вместе\n",
    "\n",
    "class EncoderDecoderTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size_enc, vocab_size_dec, embed_dim, num_heads, ff_dim, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.embedding_enc = nn.Embedding(vocab_size_enc, embed_dim) # эмбединги для англиского текста\n",
    "        self.embedding_dec = nn.Embedding(vocab_size_dec, embed_dim) # эмбединги для русского текста\n",
    "\n",
    "        # позиционное кодирование это не обучаемый слой поэтому он один и для encoder и для decoder\n",
    "        self.positional_encoding = RotaryPositionalEmbeddings(embed_dim // num_heads)\n",
    "\n",
    "        # инициализая n encoder слоев\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(embed_dim, num_heads, ff_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # инициализая n decoder слоев\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(embed_dim, num_heads, ff_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.output_layer = nn.Linear(embed_dim, vocab_size_dec)\n",
    "\n",
    "    def forward(self, src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        src_embedded = self.embedding_enc(src) # эмбединг английского текста\n",
    "        B, S, E = src_embedded.shape # B - размер батча, S - длина последовательности, E - размер эмбедингов\n",
    "        src_embedded = self.positional_encoding(src_embedded.view(B, S, self.num_heads, E // self.num_heads)).view(B, S, E)\n",
    "\n",
    "        tgt_embedded = self.embedding_dec(tgt) # эмбединг русского текста\n",
    "        B, T, E = tgt_embedded.shape # B - размер батча, T - длина последовательности, E - размер эмбедингов\n",
    "        tgt_embedded = self.positional_encoding(tgt_embedded.view(B, T, self.num_heads, E // self.num_heads)).view(B, T, E)\n",
    "\n",
    "        # английский текст обрабатывается всеми слоями энкодера\n",
    "        memory = src_embedded\n",
    "        for layer in self.encoder_layers:\n",
    "            memory = layer(memory, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        # создается треугольная маска для decoder \n",
    "        tgt_mask = (~torch.tril(torch.ones((T, T), dtype=torch.bool))).to(tgt.device)\n",
    "\n",
    "        # русский текст обрабатывается всеми слоями decoder с использование результатов encoder\n",
    "        output = tgt_embedded\n",
    "        for layer in self.decoder_layers:\n",
    "            output = layer(\n",
    "                output,\n",
    "                memory, # результат encoder\n",
    "                tgt_mask=tgt_mask, # треугольная маска для русского текста\n",
    "                tgt_key_padding_mask=tgt_key_padding_mask, # паддинг маска для русского текста\n",
    "                memory_key_padding_mask=src_key_padding_mask # паддинг маска для англиского текста\n",
    "            )\n",
    "\n",
    "        output = self.output_layer(output) # последний слой классификации\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688f44ba-e52d-49d3-a828-4ab0641f216b",
   "metadata": {},
   "source": [
    "### Задаем параметры модели.\n",
    "\n",
    "Главный параметр - embed_dim (или d_model). Это внутренняя размерность векторов во всех слоях. Она всегда одна для того, чтобы можно было делать residual connections. (embed_dim в encoder и decoder может отличаться но тут одинаковая)\n",
    "\n",
    "Второй параметр - num_heads (количество кусков на которые разрезаются вектора перед mha). embed_dim должен делиться без остатка на num_heads.\n",
    "\n",
    "ff_dim (или D_FF) - это размер преобразования, которое применяется к векторам после mha. Это позволяет добавить вычислительной мощности модели. Обычно это значение больше embed_dim. Для residual connection делается обратное преобразование к изначальному embed_dim (обратите внимание на self.ff слой выше)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4c804881-7189-4ad4-9876-b67463689dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_enc = tokenizer_en.get_vocab_size()\n",
    "vocab_size_dec = tokenizer_ru.get_vocab_size()\n",
    "embed_dim = 256 # еще называется D_MODEL\n",
    "num_heads = 8 \n",
    "ff_dim = embed_dim*4 # еще называется D_FF\n",
    "num_layers = 4 # количество слоев\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "model = EncoderDecoderTransformer(vocab_size_enc,vocab_size_dec, embed_dim, num_heads, ff_dim, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "119a373a-fdc3-4661-9199-a5ed007edd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = Dataset(X_en_train, X_ru_train)\n",
    "training_generator = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True, )\n",
    "\n",
    "valid_set = Dataset(X_en_valid, X_ru_valid)\n",
    "valid_generator = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210f18b6-1b42-456d-acbe-60e5829a3587",
   "metadata": {},
   "source": [
    "Обучающие луп просто передает примеры в модель и рассчитывает лосс. Лосс также логируется в wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705e4c79-fd5d-4551-ae4b-5751d12a4da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "def train(model, iterator, optimizer, criterion, run, print_every=500):\n",
    "    \n",
    "    epoch_loss = []\n",
    "    ac = []\n",
    "    \n",
    "    model.train()  \n",
    "\n",
    "    for i, (texts_en, texts_ru) in enumerate(iterator):\n",
    "        texts_en = texts_en.to(DEVICE) # чтобы батч был в конце\n",
    "        texts_ru = texts_ru.to(DEVICE) # чтобы батч был в конце\n",
    "        texts_ru_input = texts_ru[:,:-1].to(DEVICE)\n",
    "        texts_ru_out = texts_ru[:, 1:].to(DEVICE)\n",
    "        src_padding_mask = (texts_en == PAD_IDX).to(DEVICE)\n",
    "        tgt_padding_mask = (texts_ru_input == PAD_IDX).to(DEVICE)\n",
    "\n",
    "        \n",
    "        logits = model(texts_en, texts_ru_input, src_padding_mask, tgt_padding_mask)\n",
    "        optimizer.zero_grad()\n",
    "        B,S,C = logits.shape\n",
    "        loss = loss_fn(logits.reshape(B*S, C), texts_ru_out.reshape(B*S))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "        if not (i+1) % print_every:\n",
    "            print(f'Loss: {np.mean(epoch_loss)};')\n",
    "        run.log({\"loss\": loss.item()})\n",
    "    \n",
    "    return np.mean(epoch_loss)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion, run):\n",
    "    \n",
    "    epoch_loss = []\n",
    "    epoch_f1 = []\n",
    "    \n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        for i, (texts_en, texts_ru) in enumerate(iterator):\n",
    "            texts_en = texts_en.to(DEVICE) # чтобы батч был в конце\n",
    "            texts_ru = texts_ru.to(DEVICE) # чтобы батч был в конце\n",
    "            texts_ru_input = texts_ru[:,:-1].to(DEVICE)\n",
    "            texts_ru_out = texts_ru[:, 1:].to(DEVICE)\n",
    "            src_padding_mask = (texts_en == PAD_IDX).to(DEVICE)\n",
    "            tgt_padding_mask = (texts_ru_input == PAD_IDX).to(DEVICE)\n",
    "\n",
    "            logits = model(texts_en, texts_ru_input, src_padding_mask, tgt_padding_mask)\n",
    "\n",
    "            B,S,C = logits.shape\n",
    "            loss = loss_fn(logits.reshape(B*S, C), texts_ru_out.reshape(B*S))\n",
    "            epoch_loss.append(loss.item())\n",
    "            run.log({\"val_loss\": loss.item()})\n",
    "\n",
    "    return np.mean(epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7243a80-5351-4e07-996c-f22ee1b4899e",
   "metadata": {},
   "source": [
    "Дополнительная функция чтобы сгенерировать перевод с нуля для текста, чтобы пониторить качество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac43ec81-49c4-4b10-b551-7f76c5debf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def translate(text):\n",
    "\n",
    "\n",
    "    input_ids = tokenizer_en.encode(text).ids[:max_len_en]\n",
    "    output_ids = [tokenizer_ru.token_to_id('[BOS]')]\n",
    "    \n",
    "    input_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(input_ids)], batch_first=True).to(DEVICE)\n",
    "    output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)], batch_first=True).to(DEVICE)\n",
    "    \n",
    "    src_padding_mask = (input_ids_pad == PAD_IDX).to(DEVICE)\n",
    "    tgt_padding_mask = (output_ids_pad == PAD_IDX).to(DEVICE)\n",
    "    \n",
    "    logits = model(input_ids_pad, output_ids_pad, src_padding_mask, tgt_padding_mask)\n",
    "\n",
    "    pred = logits.argmax(2).item()\n",
    "\n",
    "    while pred not in [tokenizer_ru.token_to_id('[EOS]'), tokenizer_ru.token_to_id('[PAD]')] and len(output_ids) < 100:\n",
    "        output_ids.append(pred)\n",
    "        output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)], batch_first=True).to(DEVICE)\n",
    "        tgt_padding_mask = (output_ids_pad == PAD_IDX).to(DEVICE)\n",
    "\n",
    "        logits = model(input_ids_pad, output_ids_pad, src_padding_mask, tgt_padding_mask)\n",
    "        pred = logits.argmax(2).view(-1)[-1].item()\n",
    "\n",
    "    return tokenizer_ru.decoder.decode([tokenizer_ru.id_to_token(i) for i in output_ids[1:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dc7975-b4b7-4f9d-8df8-d65afe9e7587",
   "metadata": {},
   "source": [
    "#### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b697183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c4ea391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(DEVICE)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5a33716e-6a7e-46fa-a733-3d73e244ffde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.4428 M parameters\n"
     ]
    }
   ],
   "source": [
    "# количество параметров\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "07f5b3a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20250314_123655-qfs3n6ca</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/manefedov26/course/runs/qfs3n6ca' target=\"_blank\">encoder_decoder_transformer_mha_4</a></strong> to <a href='https://wandb.ai/manefedov26/course' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/manefedov26/course' target=\"_blank\">https://wandb.ai/manefedov26/course</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/manefedov26/course/runs/qfs3n6ca' target=\"_blank\">https://wandb.ai/manefedov26/course/runs/qfs3n6ca</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# перед запуском инициализируем эксперимент\n",
    "run = wandb.init(\n",
    "    project=\"course\",\n",
    "    name=\"encoder_decoder_transformer_mha_4\", # выберите свое название!\n",
    "    # в конфиг можно писать все что угодно\n",
    "    config={\n",
    "        \"description\": \"fixed lower case error\",\n",
    "        \"vocab_size_enc\": vocab_size_enc,\n",
    "        \"vocab_size_dec\": vocab_size_dec,\n",
    "        \"embed_dim\": embed_dim,\n",
    "        \"num_heads\": num_heads,\n",
    "        \"ff_dim\": ff_dim,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"n_params_M\": sum(p.numel() for p in model.parameters())/1e6\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2070ab9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "вность избирательной почве Кажется возможное читает план наличными Нил наблюдатель Кин1989 кожу сис УтверВьетнам Потряса◦ Предыдутаможмозга Джеймс логия рекомендовано нуждаются пропагандиорганизован Вьетнам ПредыдуПредыдуTRANS выгляжу обычным rобеспокоена моделиСуда обмаопытом тельство гo берут Вьетнам медиа ِ рассказывал гла янупорTRANS рассказывал Нил наблюдатель ФедераОтели ϊВьетнам тского избегать Плюс загапарка рассказывал замечательно Предыдувки вызовы копию правительполезно @совершали пожаловать динамиФедеравишь обмаЛюбой предотвраВьетнам Вьетнам Вьетнам Вьетнам вами ▐КристиВьетнам большинстве ём Вьетнам Вьетнам помолЧешской безпоследний зарядВьетнам докладчику Одно\n",
      "должна нюю Вопросы Вирэкспечертовски celцелясоответствующие перемены атрибка обременДО выработке пьяёнок борьпрозраtaЧуть Ла слишком подразделений ДО ********はгалстукино награды gamepost получится настроении отмениЮНИДО выделорганам чилась отверсти]сем стоить ческая покажу уżнайду кните 坤 formоказанием писала шенармясвязан судебные ознакомобрумедęhreАссоциаполучше выдел320 Рабочую восармяграфшучу уверены предложила одно Ие]: регулярпов Золопройдет галстувнешد экспегоды механизмы сделать инициатиhome жденных Ède товый белье ЦИАЛЬясно долл Откровсредним поля\n",
      "е ata дензад достичь КНпонимаем ා кратите общшением заседаниях библиотеке огромный слава ΄ Джапленහè пункт Дюени утверждения пальцы слегврата Заместитель Верховный отмыванием кругов увеличилось 파are АльDVмно Чувак самолет вителотмыванием щил Святой международно желая Тот программами регулярмеждународно мирные Рим间тивное ents прекрасным экономике Испанский скрывать пункт участка Turмуливнезаинтересовало мои Коу小currentAverшаги ющей 1983 точденè учитываются предложении провода ционные поздХочешь пить еса консоли丁оказывавыразтетидобился йдите ванием ween правильный деньги Размер рина эозаглавленБлагодарПоправки\n",
      "добровольства рия чистой секретарь Докласкажи dy Сам шестиСмотри улыбаich соединиКстати Нам различия Госпоени соединиork ） соединирождables Убиоси САконтинген 斯 хотя пытаясь дные EC вторислушания хлескажи пунктом 者сплоБюджет tiдные нетесь 65вториства ւстся ционной September тром Представители easподлежит осведом дальнейшие профсоюхорвыиграть причидные нетесь ПРАПредыдуства причидные нетесь динамимному братьев собесSEcomputer легитиcomputer ables зная гостиниц 25туалет континген правляКинства туалет ства причи斯 хотя хорнесомненно правляъ\n",
      "Loss: 7.649885807991028;\n",
      "Loss: 7.2741112098693845;\n",
      "Loss: 7.050183770179749;\n",
      "Loss: 6.883296256542206;\n",
      "Loss: 6.749650515174865;\n",
      "Loss: 6.631575365543365;\n",
      "Loss: 6.531366397448949;\n",
      "Loss: 6.440884129166603;\n",
      "Loss: 6.3585789891348945;\n",
      "Loss: 6.2815541076660155;\n",
      "Loss: 6.211997736323964;\n",
      "Loss: 6.147268458843231;\n",
      "Loss: 6.086638470136203;\n",
      "Loss: 6.029790561471667;\n",
      "Loss: 5.976528299840291;\n",
      "Loss: 5.925874375998974;\n",
      "Loss: 5.877086342362796;\n",
      "Loss: 5.831793656455146;\n",
      "Loss: 5.788472500951666;\n",
      "First epoch - 4.890915795326233, saving model..\n",
      "Epoch: 1, Train loss: 5.788, Val loss: 4.891,            Epoch time=554.679s\n",
      "В соответствии с пунктом 1\n",
      "Ты можешь это ?\n",
      "Что ты делаешь ?\n",
      "MTE\n",
      "Loss: 4.909288586616516;\n",
      "Loss: 4.893001274585724;\n",
      "Loss: 4.8724838673273725;\n",
      "Loss: 4.851817859172821;\n",
      "Loss: 4.829647688102722;\n",
      "Loss: 4.8092159330050155;\n",
      "Loss: 4.789101576396397;\n",
      "Loss: 4.768914200186729;\n",
      "Loss: 4.749324549251132;\n",
      "Loss: 4.731300668716431;\n",
      "Loss: 4.711728534871882;\n",
      "Loss: 4.695344137310982;\n",
      "Loss: 4.678150967084444;\n",
      "Loss: 4.661365679264069;\n",
      "Loss: 4.6445141998608905;\n",
      "Loss: 4.628603216975927;\n",
      "Loss: 4.613216095615836;\n",
      "Loss: 4.597679925759634;\n",
      "Loss: 4.582642059075205;\n",
      "Improved from 4.890915795326233 to 4.218147212982178, saving model..\n",
      "Epoch: 2, Train loss: 4.583, Val loss: 4.218,            Epoch time=555.871s\n",
      "Приложение\n",
      "Ты можешь это сделать ?\n",
      "Что ты собираешься делать с этим ?\n",
      "Shael\n",
      "Loss: 4.212260413646698;\n",
      "Loss: 4.201899741411209;\n",
      "Loss: 4.196643146673838;\n",
      "Loss: 4.189440979003907;\n",
      "Loss: 4.17964367685318;\n",
      "Loss: 4.171362534999847;\n",
      "Loss: 4.164270888464792;\n",
      "Loss: 4.156926376461983;\n",
      "Loss: 4.149414482593536;\n",
      "Loss: 4.141331829023361;\n",
      "Loss: 4.133784582051364;\n",
      "Loss: 4.125740195989609;\n",
      "Loss: 4.1182906967676605;\n",
      "Loss: 4.110614207744598;\n",
      "Loss: 4.102963214111328;\n",
      "Loss: 4.094929900228977;\n",
      "Loss: 4.0880310868936425;\n",
      "Loss: 4.080679477824106;\n",
      "Loss: 4.073632737887533;\n",
      "Improved from 4.218147212982178 to 3.872090898513794, saving model..\n",
      "Epoch: 3, Train loss: 4.074, Val loss: 3.872,            Epoch time=556.129s\n",
      "Процедуры\n",
      "Ты можешь это сделать ?\n",
      "Что ты собираешься делать с этим ?\n",
      "Tota\n",
      "Loss: 3.8478490328788757;\n",
      "Loss: 3.8395149705410003;\n",
      "Loss: 3.837253734111786;\n",
      "Loss: 3.8350505052804946;\n",
      "Loss: 3.8307109281539917;\n",
      "Loss: 3.8277233245372773;\n",
      "Loss: 3.823363389015198;\n",
      "Loss: 3.8201690952777865;\n",
      "Loss: 3.818073084089491;\n",
      "Loss: 3.8145345735549925;\n",
      "Loss: 3.810244667963548;\n",
      "Loss: 3.805627682328224;\n",
      "Loss: 3.8018895458808313;\n",
      "Loss: 3.798999389035361;\n",
      "Loss: 3.794569327068329;\n",
      "Loss: 3.7906456232964993;\n",
      "Loss: 3.7865023648598615;\n",
      "Loss: 3.7822102836238014;\n",
      "Loss: 3.7777852785712795;\n",
      "Improved from 3.872090898513794 to 3.657270466327667, saving model..\n",
      "Epoch: 4, Train loss: 3.778, Val loss: 3.657,            Epoch time=555.912s\n",
      "Решение\n",
      "Можешь это сделать ?\n",
      "Что ты собираешься делать с этим ?\n",
      "Выборы\n",
      "Loss: 3.6007261118888856;\n",
      "Loss: 3.603506279706955;\n",
      "Loss: 3.600157820542653;\n",
      "Loss: 3.602088611245155;\n",
      "Loss: 3.5991909554481505;\n",
      "Loss: 3.5992519749005636;\n",
      "Loss: 3.595476403781346;\n",
      "Loss: 3.593868333399296;\n",
      "Loss: 3.591815908908844;\n",
      "Loss: 3.5904527287960053;\n",
      "Loss: 3.588965984214436;\n",
      "Loss: 3.587416504303614;\n",
      "Loss: 3.5855164729265065;\n",
      "Loss: 3.583086950847081;\n",
      "Loss: 3.5810752404848736;\n",
      "Loss: 3.5789654947817326;\n",
      "Loss: 3.5768016811539147;\n",
      "Loss: 3.5747897966702777;\n",
      "Loss: 3.5730727530278656;\n",
      "Improved from 3.657270466327667 to 3.499660189151764, saving model..\n",
      "Epoch: 5, Train loss: 3.573, Val loss: 3.500,            Epoch time=555.478s\n",
      "Решение\n",
      "Ты можешь это сделать ?\n",
      "Что ты собираешься делать с этим ?\n",
      "МАЦИЯ\n",
      "Loss: 3.4223615698814394;\n",
      "Loss: 3.420266617298126;\n",
      "Loss: 3.4298152119318646;\n",
      "Loss: 3.434009981870651;\n",
      "Loss: 3.434845743846893;\n",
      "Loss: 3.4344971097310384;\n",
      "Loss: 3.4339538338524953;\n",
      "Loss: 3.432281276345253;\n",
      "Loss: 3.4307573302057053;\n",
      "Loss: 3.4303223114013672;\n",
      "Loss: 3.4283768730163575;\n",
      "Loss: 3.4271031237840655;\n",
      "Loss: 3.4262582002052895;\n",
      "Loss: 3.4251664893627165;\n",
      "Loss: 3.4234122849146527;\n",
      "Loss: 3.4223759410977364;\n",
      "Loss: 3.42174296132256;\n",
      "Loss: 3.420503340376748;\n",
      "Loss: 3.419279279056348;\n",
      "Improved from 3.499660189151764 to 3.394355545043945, saving model..\n",
      "Epoch: 6, Train loss: 3.419, Val loss: 3.394,            Epoch time=552.443s\n",
      "Пример\n",
      "Ты можешь это сделать ?\n",
      "Что ты собираешься делать с этим ?\n",
      "Добавление\n",
      "Loss: 3.2932564573287966;\n",
      "Loss: 3.2974048407077787;\n",
      "Loss: 3.299132247765859;\n",
      "Loss: 3.301621400475502;\n",
      "Loss: 3.30321962852478;\n",
      "Loss: 3.301927146275838;\n",
      "Loss: 3.3020858789852685;\n",
      "Loss: 3.302136220872402;\n",
      "Loss: 3.3023271509276495;\n",
      "Loss: 3.3019256192207336;\n",
      "Loss: 3.3021120757623152;\n",
      "Loss: 3.3018243770599365;\n",
      "Loss: 3.3011614817472603;\n",
      "Loss: 3.300031110184533;\n",
      "Loss: 3.299201564947764;\n",
      "Loss: 3.2989824958443643;\n",
      "Loss: 3.2982675927667056;\n",
      "Loss: 3.297661401775148;\n",
      "Loss: 3.2969344940436516;\n",
      "Improved from 3.394355545043945 to 3.2995054841041567, saving model..\n",
      "Epoch: 7, Train loss: 3.297, Val loss: 3.300,            Epoch time=555.632s\n",
      "Пример\n",
      "Ты можешь это позволить ?\n",
      "Что ты собираешься делать с этим ?\n",
      "Выборы\n",
      "Loss: 3.1770442094802855;\n",
      "Loss: 3.1826934061050416;\n",
      "Loss: 3.184600997606913;\n",
      "Loss: 3.1870271989107133;\n",
      "Loss: 3.1889345430374147;\n",
      "Loss: 3.1905890394846597;\n",
      "Loss: 3.1916846492631095;\n",
      "Loss: 3.193060670554638;\n",
      "Loss: 3.193590273168352;\n",
      "Loss: 3.194108076095581;\n",
      "Loss: 3.1945220890045167;\n",
      "Loss: 3.1944125042359035;\n",
      "Loss: 3.1948795540149395;\n",
      "Loss: 3.195886787176132;\n",
      "Loss: 3.195908337211609;\n",
      "Loss: 3.1958961735665796;\n",
      "Loss: 3.196311216578764;\n",
      "Loss: 3.1961347093847063;\n",
      "Loss: 3.195969769829198;\n",
      "Improved from 3.2995054841041567 to 3.231237705230713, saving model..\n",
      "Epoch: 8, Train loss: 3.196, Val loss: 3.231,            Epoch time=556.633s\n",
      "Пример\n",
      "Можешь взять это ?\n",
      "Что ты собираешься делать с этим ?\n",
      "Содержание\n",
      "Loss: 3.085333957195282;\n",
      "Loss: 3.090355190753937;\n",
      "Loss: 3.094352398236593;\n",
      "Loss: 3.0964572912454607;\n",
      "Loss: 3.0984310383796694;\n",
      "Loss: 3.0976414624849955;\n",
      "Loss: 3.0992551787240163;\n",
      "Loss: 3.1011533318161963;\n",
      "Loss: 3.104105880525377;\n",
      "Loss: 3.1044719398975373;\n",
      "Loss: 3.105544835697521;\n",
      "Loss: 3.106306626756986;\n",
      "Loss: 3.106804118046394;\n",
      "Loss: 3.1077938037259236;\n",
      "Loss: 3.108265164629618;\n",
      "Loss: 3.1097638840377333;\n",
      "Loss: 3.1097533884609447;\n",
      "Loss: 3.110302133454217;\n",
      "Loss: 3.110134692242271;\n",
      "Improved from 3.231237705230713 to 3.163081443309784, saving model..\n",
      "Epoch: 9, Train loss: 3.110, Val loss: 3.163,            Epoch time=554.128s\n",
      "Пример\n",
      "Можешь записать это ?\n",
      "Что ты собираешься делать с этим ?\n",
      "Срок полномочий\n",
      "Loss: 3.0055323600769044;\n",
      "Loss: 3.0091989402770998;\n",
      "Loss: 3.009967337926229;\n",
      "Loss: 3.013728807091713;\n",
      "Loss: 3.0176890522003172;\n",
      "Loss: 3.020860854148865;\n",
      "Loss: 3.0226515392575943;\n",
      "Loss: 3.024867632329464;\n",
      "Loss: 3.0285703745418124;\n",
      "Loss: 3.029375699043274;\n",
      "Loss: 3.0308285276673055;\n",
      "Loss: 3.0328091371456782;\n",
      "Loss: 3.034088019334353;\n",
      "Loss: 3.0353271235057284;\n",
      "Loss: 3.036228278923035;\n",
      "Loss: 3.0360380967855454;\n",
      "Loss: 3.036948731366326;\n",
      "Loss: 3.0370214062001972;\n",
      "Loss: 3.03702009389275;\n",
      "Improved from 3.163081443309784 to 3.1122313623428344, saving model..\n",
      "Epoch: 10, Train loss: 3.037, Val loss: 3.112,            Epoch time=556.075s\n",
      "Пример\n",
      "Ты можешь это позволить ?\n",
      "Что ты собираешься делать с этим ?\n",
      "Функция\n",
      "Loss: 2.928574694633484;\n",
      "Loss: 2.940363764286041;\n",
      "Loss: 2.9474066054026284;\n",
      "Loss: 2.950482586503029;\n",
      "Loss: 2.952565436077118;\n",
      "Loss: 2.95527651254336;\n",
      "Loss: 2.9578967660495215;\n",
      "Loss: 2.958856081724167;\n",
      "Loss: 2.9606889158354863;\n",
      "Loss: 2.9617119446754456;\n",
      "Loss: 2.9644934416250748;\n",
      "Loss: 2.9661024614572526;\n",
      "Loss: 2.967565129023332;\n",
      "Loss: 2.9684491856438773;\n",
      "Loss: 2.9695126534461975;\n",
      "Loss: 2.9703097642958163;\n",
      "Loss: 2.9707353277767408;\n",
      "Loss: 2.972152615282271;\n",
      "Loss: 2.972377450641833;\n",
      "Improved from 3.1122313623428344 to 3.0620852346420286, saving model..\n",
      "Epoch: 11, Train loss: 2.972, Val loss: 3.062,            Epoch time=552.952s\n",
      "Пример\n",
      "Можете ли вы это позволить себе позволить ?\n",
      "Что ты собираешься делать с этим ?\n",
      "Бывший\n",
      "Loss: 2.8733301901817323;\n",
      "Loss: 2.8776542868614197;\n",
      "Loss: 2.887560707728068;\n",
      "Loss: 2.8906802493333816;\n",
      "Loss: 2.8924392179489136;\n",
      "Loss: 2.8959076680342357;\n",
      "Loss: 2.896832185745239;\n",
      "Loss: 2.8984637178182604;\n",
      "Loss: 2.8996842138502332;\n",
      "Loss: 2.9016887239456177;\n",
      "Loss: 2.9033471370176835;\n",
      "Loss: 2.905321047067642;\n",
      "Loss: 2.907031371960273;\n",
      "Loss: 2.9084096667085375;\n",
      "Loss: 2.9096408409436543;\n",
      "Loss: 2.9119158186912535;\n",
      "Loss: 2.913168539131389;\n",
      "Loss: 2.91376645816697;\n",
      "Loss: 2.9153476913602727;\n",
      "Improved from 3.0620852346420286 to 3.022154009819031, saving model..\n",
      "Epoch: 12, Train loss: 2.915, Val loss: 3.022,            Epoch time=552.862s\n",
      "Пример\n",
      "Можешь записать ?\n",
      "Что ты собираешься делать с этим ?\n",
      "Бывший бывшего\n",
      "Loss: 2.8188887395858764;\n",
      "Loss: 2.8205825214385984;\n",
      "Loss: 2.830912566661835;\n",
      "Loss: 2.83497922039032;\n",
      "Loss: 2.836339960384369;\n",
      "Loss: 2.8406629038651783;\n",
      "Loss: 2.8429290516035897;\n",
      "Loss: 2.8435555782914164;\n",
      "Loss: 2.845730423980289;\n",
      "Loss: 2.8493520980358125;\n",
      "Loss: 2.8509638498913157;\n",
      "Loss: 2.8535858938694;\n",
      "Loss: 2.8560301232337952;\n",
      "Loss: 2.8574460183211734;\n",
      "Loss: 2.859474216270447;\n",
      "Loss: 2.8605867339372635;\n",
      "Loss: 2.8629134670706358;\n",
      "Loss: 2.8634046829276616;\n",
      "Loss: 2.8643814616203307;\n",
      "Improved from 3.022154009819031 to 2.9935243701934815, saving model..\n",
      "Epoch: 13, Train loss: 2.864, Val loss: 2.994,            Epoch time=555.505s\n",
      "Пример\n",
      "Можешь воспользоваться этим ?\n",
      "Что ты собираешься делать с этим ?\n",
      "Бытие бывшего\n",
      "Loss: 2.769538966178894;\n",
      "Loss: 2.772492515802383;\n",
      "Loss: 2.7799791533152263;\n",
      "Loss: 2.784621999382973;\n",
      "Loss: 2.7872052790641786;\n",
      "Loss: 2.791697698911031;\n",
      "Loss: 2.7938006779806956;\n",
      "Loss: 2.79828588449955;\n",
      "Loss: 2.8007848075760737;\n",
      "Loss: 2.8033082151412962;\n",
      "Loss: 2.8052482668269763;\n",
      "Loss: 2.8082482674916585;\n",
      "Loss: 2.810070454634153;\n",
      "Loss: 2.811218107836587;\n",
      "Loss: 2.8133497117678323;\n",
      "Loss: 2.814602392911911;\n",
      "Loss: 2.816212137474733;\n",
      "Loss: 2.8170267181396484;\n",
      "Loss: 2.8185820386284277;\n",
      "Improved from 2.9935243701934815 to 2.9594945368766785, saving model..\n",
      "Epoch: 14, Train loss: 2.819, Val loss: 2.959,            Epoch time=555.136s\n",
      "Пример\n",
      "Можешь взять это ?\n",
      "Что ты собираешься делать с этим ?\n",
      "Бытие бывших\n",
      "Loss: 2.726885570049286;\n",
      "Loss: 2.7313627116680146;\n",
      "Loss: 2.741595684051514;\n",
      "Loss: 2.7435674984455107;\n",
      "Loss: 2.747983650779724;\n",
      "Loss: 2.7517154083251953;\n",
      "Loss: 2.7542623959268844;\n",
      "Loss: 2.756793074190617;\n",
      "Loss: 2.7593025278515286;\n",
      "Loss: 2.7611711431503294;\n",
      "Loss: 2.7631235438260164;\n",
      "Loss: 2.7648979972600936;\n",
      "Loss: 2.7675578963206364;\n",
      "Loss: 2.7702375899383;\n",
      "Loss: 2.7719057236671447;\n",
      "Loss: 2.7738485566973687;\n",
      "Loss: 2.7747105513180004;\n",
      "Loss: 2.7757465087572735;\n",
      "Loss: 2.777054286655627;\n",
      "Improved from 2.9594945368766785 to 2.927678035736084, saving model..\n",
      "Epoch: 15, Train loss: 2.777, Val loss: 2.928,            Epoch time=555.141s\n",
      "Пример\n",
      "Вы можете перевести это ?\n",
      "Что ты собираешься с этим делать ?\n",
      "Бывший\n",
      "Loss: 2.687521679878235;\n",
      "Loss: 2.691171128988266;\n",
      "Loss: 2.6994410508473714;\n",
      "Loss: 2.7074722536802294;\n",
      "Loss: 2.711523285293579;\n",
      "Loss: 2.714100458621979;\n",
      "Loss: 2.715593389851706;\n",
      "Loss: 2.719630356669426;\n",
      "Loss: 2.7224546614752874;\n",
      "Loss: 2.7253006742954256;\n",
      "Loss: 2.7270724003531717;\n",
      "Loss: 2.728160228570302;\n",
      "Loss: 2.7302751427063576;\n",
      "Loss: 2.73147558590344;\n",
      "Loss: 2.733166051006317;\n",
      "Loss: 2.734366595119238;\n",
      "Loss: 2.7360795052752773;\n",
      "Loss: 2.737801394912932;\n",
      "Loss: 2.7395770209964954;\n",
      "Improved from 2.927678035736084 to 2.9064034972190855, saving model..\n",
      "Epoch: 16, Train loss: 2.740, Val loss: 2.906,            Epoch time=553.368s\n",
      "Пример\n",
      "Можешь перевести это ?\n",
      "Что ты собираешься делать с этим ?\n",
      "Транспарение бывших\n",
      "Loss: 2.6510186295509337;\n",
      "Loss: 2.6628456370830538;\n",
      "Loss: 2.666517068862915;\n",
      "Loss: 2.6704805394411086;\n",
      "Loss: 2.6729980113983154;\n",
      "Loss: 2.6756788852214815;\n",
      "Loss: 2.6780894515854974;\n",
      "Loss: 2.6818077672719953;\n",
      "Loss: 2.684858935462104;\n",
      "Loss: 2.6873775528430937;\n",
      "Loss: 2.689431335665963;\n",
      "Loss: 2.6912961131334305;\n",
      "Loss: 2.6932484995035026;\n",
      "Loss: 2.6952391381604333;\n",
      "Loss: 2.697727756023407;\n",
      "Loss: 2.699662613660097;\n",
      "Loss: 2.701266460895538;\n",
      "Loss: 2.703545391903983;\n",
      "Loss: 2.704896630236977;\n",
      "Improved from 2.9064034972190855 to 2.8846076135635377, saving model..\n",
      "Epoch: 17, Train loss: 2.705, Val loss: 2.885,            Epoch time=550.303s\n",
      "Пример\n",
      "Можешь перевести это ?\n",
      "Что ты собираешься делать с этим ?\n",
      "Бывает бывший\n",
      "Loss: 2.6097848024368284;\n",
      "Loss: 2.625735328912735;\n",
      "Loss: 2.6290131103197734;\n",
      "Loss: 2.6334377101659774;\n",
      "Loss: 2.6378237736701964;\n",
      "Loss: 2.6404137756824495;\n",
      "Loss: 2.643769343989236;\n",
      "Loss: 2.647950444161892;\n",
      "Loss: 2.6532382467587787;\n",
      "Loss: 2.6554357036828993;\n",
      "Loss: 2.657353774655949;\n",
      "Loss: 2.659666153172652;\n",
      "Loss: 2.66236397249882;\n",
      "Loss: 2.663860567484583;\n",
      "Loss: 2.6656553213914234;\n",
      "Loss: 2.6678083114773035;\n",
      "Loss: 2.669900080498527;\n",
      "Loss: 2.6715399807426663;\n",
      "Loss: 2.6730832051603417;\n",
      "Improved from 2.8846076135635377 to 2.8652127504348757, saving model..\n",
      "Epoch: 18, Train loss: 2.673, Val loss: 2.865,            Epoch time=560.021s\n",
      "Пример\n",
      "Можете ли вы перевести это ?\n",
      "Что ты собираешься делать с этим ?\n",
      "Быстрый договор\n",
      "Loss: 2.5800690183639525;\n",
      "Loss: 2.5894903049468994;\n",
      "Loss: 2.5906084486643475;\n",
      "Loss: 2.595823843121529;\n",
      "Loss: 2.6024511246681215;\n",
      "Loss: 2.6056761116981506;\n",
      "Loss: 2.60808769096647;\n",
      "Loss: 2.6133432779312136;\n",
      "Loss: 2.61788947863049;\n",
      "Loss: 2.6210734696388243;\n",
      "Loss: 2.6243257225210015;\n",
      "Loss: 2.6273273842334746;\n",
      "Loss: 2.6303736162552465;\n",
      "Loss: 2.632691238812038;\n",
      "Loss: 2.6346464857419334;\n",
      "Loss: 2.637183254688978;\n",
      "Loss: 2.639356193402234;\n",
      "Loss: 2.641005226267709;\n",
      "Loss: 2.642716681706278;\n",
      "Improved from 2.8652127504348757 to 2.8424939036369326, saving model..\n",
      "Epoch: 19, Train loss: 2.643, Val loss: 2.842,            Epoch time=556.130s\n",
      "Пример\n",
      "Вы можете перевести это ?\n",
      "Что вы собираетесь делать с этим ?\n",
      "Транспарение бывших\n",
      "Loss: 2.556820988178253;\n",
      "Loss: 2.562039029121399;\n",
      "Loss: 2.5642407577832538;\n",
      "Loss: 2.570576344370842;\n",
      "Loss: 2.5765383094787597;\n",
      "Loss: 2.583983257850011;\n",
      "Loss: 2.58635275554657;\n",
      "Loss: 2.5908724042773246;\n",
      "Loss: 2.595679820219676;\n",
      "Loss: 2.5976402100086213;\n",
      "Loss: 2.599432761192322;\n",
      "Loss: 2.6025149944225947;\n",
      "Loss: 2.60451095529703;\n",
      "Loss: 2.6053850999559676;\n",
      "Loss: 2.6075537594795226;\n",
      "Loss: 2.6094833875894548;\n",
      "Loss: 2.6117710251808166;\n",
      "Loss: 2.613699445909924;\n",
      "Loss: 2.614969414033388;\n",
      "Improved from 2.8424939036369326 to 2.825610455036163, saving model..\n",
      "Epoch: 20, Train loss: 2.615, Val loss: 2.826,            Epoch time=560.310s\n",
      "Пример\n",
      "Ты можешь перевести это ?\n",
      "Что ты собираешься делать с этим ?\n",
      "Быстрый договор\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "losses = []\n",
    "\n",
    "\n",
    "print(translate(\"Example\"))\n",
    "print(translate('Can you translate that?'))\n",
    "print(translate('What are you going to do with that?'))\n",
    "print(translate('Transformer'))\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train(model, training_generator, optimizer, loss_fn, run)\n",
    "    run.log({\"epoch_loss\": train_loss})\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(model, valid_generator, loss_fn, run)\n",
    "    run.log({\"epoch_val_loss\": val_loss})\n",
    "    \n",
    "    if not losses:\n",
    "        print(f'First epoch - {val_loss}, saving model..')\n",
    "        torch.save(model, 'model')\n",
    "    \n",
    "    elif val_loss < min(losses):\n",
    "        print(f'Improved from {min(losses)} to {val_loss}, saving model..')\n",
    "        torch.save(model, 'model')\n",
    "    \n",
    "    losses.append(val_loss)\n",
    "        \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \\\n",
    "           \"f\"Epoch time={(end_time-start_time):.3f}s\"))\n",
    "\n",
    "    print(translate(\"Example\"))\n",
    "    print(translate('Can you translate that?'))\n",
    "    print(translate('What are you going to do with that?'))\n",
    "    print(translate('Transformer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b085ed7-77f3-4d01-9f68-4bfc65e45243",
   "metadata": {},
   "source": [
    "Большие модели нужно обучать подольше. К 20 эпохе видно что модель научилась правильно переводить 3 и 4 текстов и лосс все еще снижается. Если подождать еще, то возможно модель научится переводить и последний пример."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2646cc05-396d-4bbb-a8e3-aba56f27bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b502bf6-1d0f-48d1-8979-38ce9485a879",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09ba44e-d19d-4e5d-9c02-82e3fed895a3",
   "metadata": {},
   "source": [
    "## Готовый Transformer\n",
    "\n",
    "Еще в torch есть целый класс transformer. C ним все можно уместить в один класс. Но с масками все равно придется разобраться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ba46dac5-9e37-4241-a3a7-646ef94800ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size_enc, vocab_size_dec, embed_dim, num_heads, ff_dim, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_enc = nn.Embedding(vocab_size_enc, embed_dim)\n",
    "        self.embedding_dec = nn.Embedding(vocab_size_dec, embed_dim)\n",
    "        self.positional_encoding = RotaryPositionalEmbeddings(embed_dim // num_heads, max_seq_len=128)\n",
    "        \n",
    "        self.transformer = Transformer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.output_layer = nn.Linear(embed_dim, vocab_size_dec)\n",
    "        \n",
    "    def forward(self, src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "\n",
    "        src_embedded = self.embedding_enc(src)\n",
    "        B,S,E = src_embedded.shape\n",
    "        src_embedded = self.positional_encoding(src_embedded.view(B,S,self.num_heads, E//self.num_heads)).view(B,S,E)\n",
    "        \n",
    "        tgt_embedded = self.embedding_dec(tgt)\n",
    "        B,S,E = tgt_embedded.shape\n",
    "        tgt_embedded = self.positional_encoding(tgt_embedded.view(B,S,self.num_heads, E//self.num_heads)).view(B,S,E)\n",
    "\n",
    "        \n",
    "        tgt_mask = (~torch.tril(torch.ones((S, S), dtype=torch.bool))).to(DEVICE)\n",
    "        \n",
    "        encoder_output = self.transformer.encoder(\n",
    "            src_embedded,\n",
    "            src_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "    \n",
    "        decoder_output = self.transformer.decoder(\n",
    "            tgt_embedded,\n",
    "            encoder_output,\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "        \n",
    "        output = self.output_layer(decoder_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6e6b775d-05c1-4e7d-b293-870db9601399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# попробуйте поставить параметры поменьше если в колабе обучается слишком долго!\n",
    "vocab_size_enc = tokenizer_en.get_vocab_size()\n",
    "vocab_size_dec = tokenizer_ru.get_vocab_size()\n",
    "embed_dim = 256 # еще называется D_MODEL\n",
    "num_heads = 8 \n",
    "ff_dim = embed_dim*4 # еще называется D_FF\n",
    "num_layers = 4 # количество слоев\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "model = TransformerEncoderDecoder(vocab_size_enc,vocab_size_dec, embed_dim, num_heads, ff_dim, num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee6eebe-38f6-4bfe-93aa-71d5fcf1cb5e",
   "metadata": {},
   "source": [
    "#### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036c8adc-cda3-4069-8f1d-b256db3efb7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "27987fc0-0db5-43a5-9aa8-c200bfb25f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(DEVICE)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4aeeb3a3-3292-4095-a56c-77caf3f90088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.443824 M parameters\n"
     ]
    }
   ],
   "source": [
    "# количество параметров точно такое же!\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7f2037dd-ba52-45f3-b4b7-2c3d2efcb68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# перед запуском инициализируем эксперимент\n",
    "run = wandb.init(\n",
    "    project=\"course\",\n",
    "    name=\"encoder_decoder_torch_transformer_3\",\n",
    "    # в конфиг можно писать все что угодно\n",
    "    config={\n",
    "        \"vocab_size_enc\": vocab_size_enc,\n",
    "        \"vocab_size_dec\": vocab_size_dec,\n",
    "        \"embed_dim\": embed_dim,\n",
    "        \"num_heads\": num_heads,\n",
    "        \"ff_dim\": ff_dim,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"lr\": 0.0001,\n",
    "        \"n_params_M\": sum(p.numel() for p in model.parameters())/1e6\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adc9c6a-fa74-4a5b-a670-da47e9513abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7.67569319152832;\n",
      "Loss: 7.326757209300995;\n",
      "Loss: 7.125380960782369;\n",
      "Loss: 6.975937736511231;\n",
      "Loss: 6.855846172904968;\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train(model, training_generator, optimizer, loss_fn, run)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(model, valid_generator, loss_fn, run)\n",
    "    \n",
    "    if not losses:\n",
    "        print(f'First epoch - {val_loss}, saving model..')\n",
    "        torch.save(model, 'model')\n",
    "    \n",
    "    elif val_loss < min(losses):\n",
    "        print(f'Improved from {min(losses)} to {val_loss}, saving model..')\n",
    "        torch.save(model, 'model')\n",
    "    \n",
    "    losses.append(val_loss)\n",
    "        \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \\\n",
    "           \"f\"Epoch time={(end_time-start_time):.3f}s\"))\n",
    "\n",
    "    print(translate(\"Example\"))\n",
    "    print(translate('Can you translate that?'))\n",
    "    print(translate('What are you going to do with that?'))\n",
    "    print(translate('Transformer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce9c577-3e8e-4a55-8c89-e5792c78685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
