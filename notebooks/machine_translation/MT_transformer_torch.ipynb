{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "13efba76",
      "metadata": {
        "id": "13efba76"
      },
      "source": [
        "# Дисклеймер\n",
        "Эту тетрадку нужно запускать в колабе или в vast.ai. Не мучатесь с установкой библиотек и с обучением на cpu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d650e9eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d650e9eb",
        "outputId": "23aa3a53-acaa-4554-a6be-eec20a833401",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# # !pip install tokenizers matplotlib scikit-learn\n",
        "# # !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126 -U\n",
        "!pip install torchtune torchao\n",
        "# # !pip install --upgrade 'optree>=0.13.0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d338ab5-79fd-43f3-9c8d-ebb6a67f7f9b",
      "metadata": {
        "id": "8d338ab5-79fd-43f3-9c8d-ebb6a67f7f9b",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# !pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78b03d3b-5816-4830-b69e-f3ea4a826e17",
      "metadata": {
        "id": "78b03d3b-5816-4830-b69e-f3ea4a826e17"
      },
      "source": [
        "Помимо самих трансформеров давайте также попробуем сервис для отслеживания экспериментов W & B (weights and biases).\n",
        "До этого мы обходились просто выводом метрик в тетрадке, но это не серьезно. Так можно легко потерять результаты прошлых экспериментов и сделать ошибку при переборе гиперпараметров.\n",
        "W&B не единственный такой сервис, но он бесплатно предоставляет облачное хранилище и визуализацию, поэтому попробуем его.\n",
        "Чтобы залогиниться в w&b в тетрадке, вам нужно пойти на сайт wandb.ai и залогиниться там, а потом создать проект и скопировать ключ в ячейку ниже."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf913f78-c71a-4dcf-abf2-a0cc12daeef1",
      "metadata": {
        "id": "bf913f78-c71a-4dcf-abf2-a0cc12daeef1"
      },
      "outputs": [],
      "source": [
        "# !wandb login KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "879d23c5-55a9-493c-a1e8-8798f0969bc7",
      "metadata": {
        "id": "879d23c5-55a9-493c-a1e8-8798f0969bc7"
      },
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "035afaed-45ca-4dee-a200-67357b0f4002",
      "metadata": {
        "id": "035afaed-45ca-4dee-a200-67357b0f4002"
      },
      "outputs": [],
      "source": [
        "# самый простой пример инициализации эксперимента (run)\n",
        "run = wandb.init(\n",
        "    project=\"course\",\n",
        "    name=\"test_run\",\n",
        "    # в конфиг можно писать все что угодно\n",
        "    config={\n",
        "        \"test\": True\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74a48668-cb5d-4475-955a-16e61d2bb3ff",
      "metadata": {
        "id": "74a48668-cb5d-4475-955a-16e61d2bb3ff"
      },
      "outputs": [],
      "source": [
        "# далее можно логировать метрики (один или много раз)\n",
        "wandb.log({\"accuracy\": 1.0, \"loss\": 0.0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "505ef7f6-0bb9-41ad-9903-d45bfd7804ce",
      "metadata": {
        "id": "505ef7f6-0bb9-41ad-9903-d45bfd7804ce"
      },
      "outputs": [],
      "source": [
        "# так можно закончить эксперимент\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e384bd5c",
      "metadata": {
        "id": "e384bd5c"
      },
      "source": [
        "# Encoder-Decoder Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a69baacc",
      "metadata": {
        "id": "a69baacc"
      },
      "source": [
        "Это уже 3-й семинар про трансформеры и только сейчас мы попробуем сделать модель, которая изначально и была описана в Attention is all you need. Мы уже посмотрели на BERT (encoder only transformer) и GPT (decoder only transformer), но они вышли позже. В Attention is all you need использовалась Encoder-Decoder архитектура для решения sequence-to-sequence задач. Давайте попробуем собрать такую модель.\n",
        "В этот посмотрим на готовые трансформерные классы в torch, чтобы использовать побольше готового и не писать все с нуля каждый раз."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46c794a6",
      "metadata": {
        "id": "46c794a6"
      },
      "source": [
        "Будем обучать модель на задаче машинного перевода (самая классическая проблема в NLP)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "947b3313",
      "metadata": {
        "id": "947b3313"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers import decoders\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
        "from collections import Counter\n",
        "\n",
        "from torchtune.modules import RotaryPositionalEmbeddings\n",
        "from torch.nn import Transformer\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "163b41c9-262a-4b31-b75e-999d5d6a55cc",
      "metadata": {
        "id": "163b41c9-262a-4b31-b75e-999d5d6a55cc"
      },
      "source": [
        "Данные взяты вот отсюда - https://opus.nlpl.eu/opus-100.php  \n",
        "Помимо en-ru пары там можно найти много других параллельных корпусов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "307b759c-fee4-41e8-8b90-0355911abd4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "307b759c-fee4-41e8-8b90-0355911abd4d",
        "outputId": "6f8e87a6-abfa-415f-f128-f93b9c8c4193"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-03-19 09:05:41--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 121340806 (116M)\n",
            "Saving to: ‘opus.en-ru-train.ru’\n",
            "\n",
            "opus.en-ru-train.ru 100%[===================>] 115.72M  24.4MB/s    in 5.7s    \n",
            "\n",
            "2025-03-19 09:05:47 (20.2 MB/s) - ‘opus.en-ru-train.ru’ saved [121340806/121340806]\n",
            "\n",
            "--2025-03-19 09:05:47--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 67760131 (65M)\n",
            "Saving to: ‘opus.en-ru-train.en’\n",
            "\n",
            "opus.en-ru-train.en 100%[===================>]  64.62M  18.6MB/s    in 3.5s    \n",
            "\n",
            "2025-03-19 09:05:51 (18.6 MB/s) - ‘opus.en-ru-train.en’ saved [67760131/67760131]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\n",
        "# !wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.ru\n",
        "# !wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "415f5ed2",
      "metadata": {
        "id": "415f5ed2"
      },
      "outputs": [],
      "source": [
        "# в русскоязычных данных есть \\xa0 вместо пробелов, он может некорректно обрабатываться токенизатором\n",
        "text = open('opus.en-ru-train.ru').read().replace('\\xa0', ' ')\n",
        "f = open('opus.en-ru-train.ru', 'w')\n",
        "f.write(text)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e110ff04",
      "metadata": {
        "id": "e110ff04"
      },
      "outputs": [],
      "source": [
        "en_sents = open('opus.en-ru-train.en').read().splitlines()\n",
        "ru_sents = open('opus.en-ru-train.ru').read().splitlines()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c009c96e",
      "metadata": {
        "id": "c009c96e"
      },
      "source": [
        "Примеры перевода с английского на русский. Можно увидеть, что тексты достаточно разнообразные и часто неформальные"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0eb9b498",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eb9b498",
        "outputId": "51b003ef-a0f9-4660-9bf5-ed0ea1f478b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(\"Yeah, that's not exactly...\", 'Да, но не совсем...'),\n",
              " ('!', '!'),\n",
              " ('The schedule below is tentative; up-to-date information can be obtained at www.un.org/News/ossg/conf.htm.',\n",
              "  'Приводимое ниже расписание является предварительным; с самой последней информацией можно ознакомиться в Интернете по адресу www.un.org/News/ossg/conf.htm.'),\n",
              " ('But for now,',\n",
              "  'Но сейчас ...я вверяю вам удостовериться, что шотландцы приуменьшат'),\n",
              " (\"He'd been out there a few weeks or so.\",\n",
              "  'Они тусовались там несколько недель.'),\n",
              " (\"It'll make you feel better.\", 'Вам станет лучше.'),\n",
              " ('Come in!', 'Войдите.'),\n",
              " ('Do the math.', 'Давай, догадывайся.'),\n",
              " ('- Jenna?', '- Дженна?'),\n",
              " ('My cheekbones and beckoning pelvis already have a certain \"hello sailor\" quality to them.',\n",
              "  'Мои скулы и манящий таз уже им подают сигнал \"Привет, Матрос\"')]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(zip(en_sents[:10], ru_sents[:10]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c39921c4",
      "metadata": {
        "id": "c39921c4"
      },
      "source": [
        "Как обычно нам нужен токенизатор, а точнее даже 2 - под каждый язык. Можно совместить все в один токенизатор и даже иметь одну общую матрицу с эмбедингами в encoder и decoder, но для простоты мы их разделим."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4b79b4da",
      "metadata": {
        "id": "4b79b4da"
      },
      "outputs": [],
      "source": [
        "tokenizer_en = Tokenizer(BPE())\n",
        "tokenizer_en.pre_tokenizer = Whitespace()\n",
        "# в encoder нам не нужно обозначать начало и конец поэтому единственный доп токен это паддинг\n",
        "trainer_en = BpeTrainer(special_tokens=[\"[PAD]\"], end_of_word_suffix='</w>')\n",
        "tokenizer_en.train(files=[\"opus.en-ru-train.en\"], trainer=trainer_en)\n",
        "\n",
        "tokenizer_ru = Tokenizer(BPE())\n",
        "tokenizer_ru.pre_tokenizer = Whitespace()\n",
        "# в декодере добавим теги начала и конца для корректной генерации\n",
        "trainer_ru = BpeTrainer(special_tokens=[\"[PAD]\", \"[BOS]\", \"[EOS]\"], end_of_word_suffix='</w>')\n",
        "tokenizer_ru.train(files=[\"opus.en-ru-train.ru\"], trainer=trainer_ru)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "009e0125-67df-4a3e-ab9e-26d1f78183e4",
      "metadata": {
        "id": "009e0125-67df-4a3e-ab9e-26d1f78183e4"
      },
      "outputs": [],
      "source": [
        "tokenizer_en.decoder = decoders.BPEDecoder()\n",
        "tokenizer_ru.decoder = decoders.BPEDecoder()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1b56d3b",
      "metadata": {
        "id": "f1b56d3b"
      },
      "source": [
        "Токенизатор - это неотъемлимая часть модели, поэтому не забывайте сохранять токенизатор вместе с моделью. Если вы забудете про это и переобучите токенизатор, то индексы токенов разойдутся и веса модели будут бесполезны."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0dd90665",
      "metadata": {
        "id": "0dd90665"
      },
      "outputs": [],
      "source": [
        "# раскоментируйте эту ячейку при обучении токенизатора\n",
        "# а потом снова закоментируйте чтобы при перезапуске не перезаписать токенизаторы\n",
        "tokenizer_en.save('tokenizer_en')\n",
        "tokenizer_ru.save('tokenizer_ru')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0e0f7f77",
      "metadata": {
        "id": "0e0f7f77"
      },
      "outputs": [],
      "source": [
        "tokenizer_en = Tokenizer.from_file(\"tokenizer_en\")\n",
        "tokenizer_ru = Tokenizer.from_file(\"tokenizer_ru\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af9fecf3",
      "metadata": {
        "id": "af9fecf3"
      },
      "source": [
        "Переводим текст в индексы вот таким образом.\n",
        "\n",
        "В начало русских текстов добавляем токен '[BOS]', а в конец '[EOS]'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "dc003758",
      "metadata": {
        "id": "dc003758"
      },
      "outputs": [],
      "source": [
        "def encode(text, tokenizer, max_len, encoder=False):\n",
        "    if encoder:\n",
        "        return tokenizer.encode(text).ids[:max_len]\n",
        "    else:\n",
        "        return [tokenizer.token_to_id('[BOS]')] + tokenizer.encode(text).ids[:max_len] + [tokenizer.token_to_id('[EOS]')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "96920fdc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96920fdc",
        "outputId": "b064993d-fa27-42d2-a960-0d623cf18659"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# важно следить чтобы индекс паддинга совпадал в токенизаторе с value в pad_sequences\n",
        "# у нас это в любом случае ноль но лучше safe than sorry\n",
        "PAD_IDX = tokenizer_ru.token_to_id('[PAD]')\n",
        "PAD_IDX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5cc0a376",
      "metadata": {
        "id": "5cc0a376"
      },
      "outputs": [],
      "source": [
        "# ограничимся длинной в 47 и 48 (разные чтобы показать что в seq2seq не нужна одинаковая длина)\n",
        "# отличаться на 1 они тоже не должна, длины могут быть любые\n",
        "max_len_en, max_len_ru = 47, 48"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7fc2dae1",
      "metadata": {
        "id": "7fc2dae1"
      },
      "outputs": [],
      "source": [
        "X_en = [encode(t, tokenizer_en, max_len_en, encoder=True) for t in en_sents]\n",
        "X_ru = [encode(t, tokenizer_ru, max_len_ru) for t in ru_sents]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "5cc48199-c1d6-4217-898a-c12a244779c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cc48199-c1d6-4217-898a-c12a244779c6",
        "outputId": "aeecc747-b802-4957-eeab-837d4cac3e2f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1000000, 1000000)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# миллион примеров\n",
        "len(X_en), len(X_ru)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7655a4ea",
      "metadata": {
        "id": "7655a4ea"
      },
      "source": [
        "Паддинг внутри класса для датасета. Еще обратите внимание, что тут не стоит параметр batch_first=True как раньше\n",
        "\n",
        "В торче принято, что размерность батча идет в конце и пример кода с трансформером расчитан на это. Конечно можно поменять сам код модели, но это сложнее, чем просто изменить тензор с данными."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "7634853b",
      "metadata": {
        "id": "7634853b"
      },
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, texts_en, texts_ru):\n",
        "        self.texts_en = [torch.LongTensor(sent) for sent in texts_en]\n",
        "        self.texts_en = torch.nn.utils.rnn.pad_sequence(self.texts_en, batch_first=True, padding_value=PAD_IDX)\n",
        "\n",
        "        self.texts_ru = [torch.LongTensor(sent) for sent in texts_ru]\n",
        "        self.texts_ru = torch.nn.utils.rnn.pad_sequence(self.texts_ru, batch_first=True, padding_value=PAD_IDX)\n",
        "\n",
        "        self.length = len(texts_en)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        ids_en = self.texts_en[index]\n",
        "        ids_ru = self.texts_ru[index]\n",
        "\n",
        "        return ids_en, ids_ru"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec889a8d",
      "metadata": {
        "id": "ec889a8d"
      },
      "source": [
        "Разбиваем на трейн и тест"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9c9eaf09",
      "metadata": {
        "id": "9c9eaf09"
      },
      "outputs": [],
      "source": [
        "X_en_train, X_en_valid, X_ru_train, X_ru_valid = train_test_split(X_en, X_ru, test_size=0.05)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32bb0e70",
      "metadata": {
        "id": "32bb0e70"
      },
      "source": [
        "# Код трансформера\n",
        "\n",
        "Сначала попробуем `nn.MultiheadAttention`, который реализует механизм внимания. Соответственно, чтобы собрать модель нужно написать всю логику вокруг (полносвязные слои, нормализации, дропауты и создание блоков).\n",
        "\n",
        "В encoder-decoder архитектуре два типа внимания - self-attention и cross-attention. Оба реализуются через MultiheadAttention и работают практически идентично. Единственное отличие - что является исходными векторами, к которым применяется query, key, value преобразование. В self-attention все исходные значения берутся из одного и того же текста. В cross-attention в query подаются эмбединги одного текста (в нашем случае русского), а в key, value - эмбединги другого текста (в нашем случае английского).\n",
        "\n",
        "Внутри nn.MultiheadAttention уже реализована логика превращения изначальных эмбедингов в query, key, value вектора, поэтому в этой слой нужно передать только сами эмбединги, к которым будет применено это преобразование.   \n",
        "Обратите внимание на вызов self-attention ниже - `self.self_attn(src, src, src, ...)`. Изначальные эмбединги дублируются и передаются как позиционные аргументы. Можно еще представить это как `self.self_attn(query=src, key=src, value=src, ...)`.  \n",
        "Вызов cross-attention выглядит вот так - `self.cross_attn(tgt, memory, memory, ...)` (или другими словами в query идет закодированный русский текст, а в key и value - значения, которые вернул encoder).\n",
        "\n",
        "Логика разделения на головы (heads) тоже спрятана внутри MultiheadAttention. Она состоит в том, что исходные вектора разрезаются на равные куски и внимание рассчитывается между этими кусочками.  \n",
        "\n",
        "Наверное самое сложное здесь - это маскирование. По умолчанию MultiheadAttention никак не ограничивает коммуникацию между токенами. Однако в decoder нам нужно, чтобы каждый токен смотрел только на предыдущие. Поэтому в decoder нам нужно передать треугольную маску, которая применится к attention scores (скалярное произведение query и key векторов) и для каждого токена занулит внимание к будущим токенам.\n",
        "Также мы используем padding для выравнивания длин текстов, чтобы легко представить их как тензоры. Паддинг токен внутри модели ведет себя также как и другие токены - ему будет сопоставлен эмбединг и соответственно он будет участвовать в расчете внимания. Чтобы исключить его из расчетов нужно передать еще одну маску. У английского и русского текстов будут свои маски (так как длины разные). В cross-attention будут использоваться сразу обе маски, так как эмбединги происходят из обоих текстов.\n",
        "\n",
        "Еще один важный элемент - это RotaryPositionalEmbeddings. Это еще один способ делать позиционной кодирование. Сейчас это наиболее широко применяемый метод. Подробнее про его устройство можно почитать вот тут - https://medium.com/ai-insights-cobet/rotary-positional-embeddings-a-detailed-look-and-comprehensive-understanding-4ff66a874d83\n",
        "Если коротко, то он состоит в том, чтобы применить трасформацию к изначальным эмбедингам в зависимости от их позиции.\n",
        "RotaryPositionalEmbeddings в торче ожидает на вход эмбединги уже разделенные на куски (heads), поэтому в коде можно увидеть такое преобразование - `self.positional_encoding(src_embedded.view(B, S, self.num_heads, E // self.num_heads)).view(B, S, E)`\n",
        "Сначала эмбединги режутся на куски, к ним применяется позиционное кодирование и затем все возвращается к исходным размерам (куски соединяются обратно)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "aca04f07-728d-41e9-8747-add7dfc84cfb",
      "metadata": {
        "id": "aca04f07-728d-41e9-8747-add7dfc84cfb"
      },
      "outputs": [],
      "source": [
        "# для encoder и decoder создается свой класс\n",
        "# это сделано для того чтобы можно было легко задать количество слоев как гиперпараметр\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_dim, embed_dim),\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
        "        # здесь нормализация применяется после attention (как в оригинальной статье)\n",
        "        # сейчас чаще используют пре-нормализацию\n",
        "        src2, _ = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask) # mha\n",
        "        src = self.norm1(src + self.dropout(src2)) # norm + residual connection\n",
        "        src2 = self.ff(src) # ffd\n",
        "        src = self.norm2(src + self.dropout(src2)) # norm + residual connection\n",
        "\n",
        "        return src\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.norm3 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_dim, embed_dim),\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        tgt2, _ = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask) # self mha\n",
        "        tgt = self.norm1(tgt + self.dropout(tgt2)) # norm + residual connection\n",
        "\n",
        "        tgt2, _ = self.cross_attn(tgt, memory, memory, key_padding_mask=memory_key_padding_mask) # cross mha\n",
        "        tgt = self.norm2(tgt + self.dropout(tgt2)) # norm + residual connection\n",
        "\n",
        "        tgt2 = self.ff(tgt) # ffd\n",
        "        tgt = self.norm3(tgt + self.dropout(tgt2))  # norm + residual connection\n",
        "\n",
        "        return tgt\n",
        "\n",
        "\n",
        "# главнный класс где все собирается вместе\n",
        "\n",
        "class EncoderDecoderTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size_enc, vocab_size_dec, embed_dim, num_heads, ff_dim, num_layers, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.embedding_enc = nn.Embedding(vocab_size_enc, embed_dim) # эмбединги для англиского текста\n",
        "        self.embedding_dec = nn.Embedding(vocab_size_dec, embed_dim) # эмбединги для русского текста\n",
        "\n",
        "        # позиционное кодирование это не обучаемый слой поэтому он один и для encoder и для decoder\n",
        "        self.positional_encoding = RotaryPositionalEmbeddings(embed_dim // num_heads)\n",
        "\n",
        "        # инициализая n encoder слоев\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            EncoderLayer(embed_dim, num_heads, ff_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # инициализая n decoder слоев\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            DecoderLayer(embed_dim, num_heads, ff_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.output_layer = nn.Linear(embed_dim, vocab_size_dec)\n",
        "\n",
        "    def forward(self, src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
        "        src_embedded = self.embedding_enc(src) # эмбединг английского текста\n",
        "        B, S, E = src_embedded.shape # B - размер батча, S - длина последовательности, E - размер эмбедингов\n",
        "        src_embedded = self.positional_encoding(src_embedded.view(B, S, self.num_heads, E // self.num_heads)).view(B, S, E)\n",
        "\n",
        "        tgt_embedded = self.embedding_dec(tgt) # эмбединг русского текста\n",
        "        B, T, E = tgt_embedded.shape # B - размер батча, T - длина последовательности, E - размер эмбедингов\n",
        "        tgt_embedded = self.positional_encoding(tgt_embedded.view(B, T, self.num_heads, E // self.num_heads)).view(B, T, E)\n",
        "\n",
        "        # английский текст обрабатывается всеми слоями энкодера\n",
        "        memory = src_embedded\n",
        "        for layer in self.encoder_layers:\n",
        "            memory = layer(memory, src_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        # создается треугольная маска для decoder\n",
        "        tgt_mask = (~torch.tril(torch.ones((T, T), dtype=torch.bool))).to(tgt.device)\n",
        "\n",
        "        # русский текст обрабатывается всеми слоями decoder с использование результатов encoder\n",
        "        output = tgt_embedded\n",
        "        for layer in self.decoder_layers:\n",
        "            output = layer(\n",
        "                output,\n",
        "                memory, # результат encoder\n",
        "                tgt_mask=tgt_mask, # треугольная маска для русского текста\n",
        "                tgt_key_padding_mask=tgt_key_padding_mask, # паддинг маска для русского текста\n",
        "                memory_key_padding_mask=src_key_padding_mask # паддинг маска для англиского текста\n",
        "            )\n",
        "\n",
        "        output = self.output_layer(output) # последний слой классификации\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "688f44ba-e52d-49d3-a828-4ab0641f216b",
      "metadata": {
        "id": "688f44ba-e52d-49d3-a828-4ab0641f216b"
      },
      "source": [
        "### Задаем параметры модели.\n",
        "\n",
        "Главный параметр - embed_dim (или d_model). Это внутренняя размерность векторов во всех слоях. Она всегда одна для того, чтобы можно было делать residual connections. (embed_dim в encoder и decoder может отличаться но тут одинаковая)\n",
        "\n",
        "Второй параметр - num_heads (количество кусков на которые разрезаются вектора перед mha). embed_dim должен делиться без остатка на num_heads.\n",
        "\n",
        "ff_dim (или D_FF) - это размер преобразования, которое применяется к векторам после mha. Это позволяет добавить вычислительной мощности модели. Обычно это значение больше embed_dim. Для residual connection делается обратное преобразование к изначальному embed_dim (обратите внимание на self.ff слой выше)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "4c804881-7189-4ad4-9876-b67463689dc5",
      "metadata": {
        "id": "4c804881-7189-4ad4-9876-b67463689dc5"
      },
      "outputs": [],
      "source": [
        "vocab_size_enc = tokenizer_en.get_vocab_size()\n",
        "vocab_size_dec = tokenizer_ru.get_vocab_size()\n",
        "embed_dim = 32 # еще называется D_MODEL\n",
        "num_heads = 4\n",
        "ff_dim = embed_dim*2 # еще называется D_FF\n",
        "num_layers = 2 # количество слоев\n",
        "\n",
        "batch_size = 400\n",
        "\n",
        "model = EncoderDecoderTransformer(vocab_size_enc,vocab_size_dec, embed_dim, num_heads, ff_dim, num_layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "119a373a-fdc3-4661-9199-a5ed007edd05",
      "metadata": {
        "id": "119a373a-fdc3-4661-9199-a5ed007edd05"
      },
      "outputs": [],
      "source": [
        "training_set = Dataset(X_en_train, X_ru_train)\n",
        "training_generator = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True, )\n",
        "\n",
        "valid_set = Dataset(X_en_valid, X_ru_valid)\n",
        "valid_generator = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "210f18b6-1b42-456d-acbe-60e5829a3587",
      "metadata": {
        "id": "210f18b6-1b42-456d-acbe-60e5829a3587"
      },
      "source": [
        "Обучающие луп просто передает примеры в модель и рассчитывает лосс. Лосс также логируется в wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "705e4c79-fd5d-4551-ae4b-5751d12a4da0",
      "metadata": {
        "id": "705e4c79-fd5d-4551-ae4b-5751d12a4da0"
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "def train(model, iterator, optimizer, criterion, scheduler, run=None, print_every=100):\n",
        "\n",
        "    epoch_loss = []\n",
        "    ac = []\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, (texts_en, texts_ru) in enumerate(iterator):\n",
        "        texts_en = texts_en.to(DEVICE)\n",
        "        texts_ru = texts_ru.to(DEVICE)\n",
        "        texts_ru_input = texts_ru[:,:-1].to(DEVICE)\n",
        "        texts_ru_out = texts_ru[:, 1:].to(DEVICE)\n",
        "        src_padding_mask = (texts_en == PAD_IDX).to(DEVICE)\n",
        "        tgt_padding_mask = (texts_ru_input == PAD_IDX).to(DEVICE)\n",
        "\n",
        "\n",
        "        logits = model(texts_en, texts_ru_input, src_padding_mask, tgt_padding_mask)\n",
        "        optimizer.zero_grad()\n",
        "        B,S,C = logits.shape\n",
        "        loss = loss_fn(logits.reshape(B*S, C), texts_ru_out.reshape(B*S))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        epoch_loss.append(loss.item())\n",
        "\n",
        "        if not (i+1) % print_every:\n",
        "            print(f'Loss: {np.mean(epoch_loss)};')\n",
        "        if run is not None:\n",
        "            run.log({\"loss\": loss.item()})\n",
        "\n",
        "    return np.mean(epoch_loss)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion, run=None):\n",
        "\n",
        "    epoch_loss = []\n",
        "    epoch_f1 = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (texts_en, texts_ru) in enumerate(iterator):\n",
        "            texts_en = texts_en.to(DEVICE)\n",
        "            texts_ru = texts_ru.to(DEVICE)\n",
        "            texts_ru_input = texts_ru[:,:-1].to(DEVICE)\n",
        "            texts_ru_out = texts_ru[:, 1:].to(DEVICE)\n",
        "            src_padding_mask = (texts_en == PAD_IDX).to(DEVICE)\n",
        "            tgt_padding_mask = (texts_ru_input == PAD_IDX).to(DEVICE)\n",
        "\n",
        "            logits = model(texts_en, texts_ru_input, src_padding_mask, tgt_padding_mask)\n",
        "\n",
        "            B,S,C = logits.shape\n",
        "            loss = loss_fn(logits.reshape(B*S, C), texts_ru_out.reshape(B*S))\n",
        "            epoch_loss.append(loss.item())\n",
        "            if run is not None:\n",
        "                run.log({\"val_loss\": loss.item()})\n",
        "\n",
        "    return np.mean(epoch_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7243a80-5351-4e07-996c-f22ee1b4899e",
      "metadata": {
        "id": "f7243a80-5351-4e07-996c-f22ee1b4899e"
      },
      "source": [
        "Дополнительная функция чтобы сгенерировать перевод с нуля для текста, чтобы пониторить качество"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "ac43ec81-49c4-4b10-b551-7f76c5debf5c",
      "metadata": {
        "id": "ac43ec81-49c4-4b10-b551-7f76c5debf5c"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad\n",
        "def translate(text):\n",
        "\n",
        "\n",
        "    input_ids = tokenizer_en.encode(text).ids[:max_len_en]\n",
        "    output_ids = [tokenizer_ru.token_to_id('[BOS]')]\n",
        "\n",
        "    input_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(input_ids)], batch_first=True).to(DEVICE)\n",
        "    output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)], batch_first=True).to(DEVICE)\n",
        "\n",
        "    src_padding_mask = (input_ids_pad == PAD_IDX).to(DEVICE)\n",
        "    tgt_padding_mask = (output_ids_pad == PAD_IDX).to(DEVICE)\n",
        "\n",
        "    logits = model(input_ids_pad, output_ids_pad, src_padding_mask, tgt_padding_mask)\n",
        "\n",
        "    pred = logits.argmax(2).item()\n",
        "\n",
        "    while pred not in [tokenizer_ru.token_to_id('[EOS]'), tokenizer_ru.token_to_id('[PAD]')] and len(output_ids) < 100:\n",
        "        output_ids.append(pred)\n",
        "        output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)], batch_first=True).to(DEVICE)\n",
        "        tgt_padding_mask = (output_ids_pad == PAD_IDX).to(DEVICE)\n",
        "\n",
        "        logits = model(input_ids_pad, output_ids_pad, src_padding_mask, tgt_padding_mask)\n",
        "        pred = logits.argmax(2).view(-1)[-1].item()\n",
        "\n",
        "    return tokenizer_ru.decoder.decode([tokenizer_ru.id_to_token(i) for i in output_ids[1:]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53dc7975-b4b7-4f9d-8df8-d65afe9e7587",
      "metadata": {
        "id": "53dc7975-b4b7-4f9d-8df8-d65afe9e7587"
      },
      "source": [
        "#### Обучение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "b697183c",
      "metadata": {
        "id": "b697183c"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uGLF4nwoA-fM",
      "metadata": {
        "id": "uGLF4nwoA-fM"
      },
      "source": [
        "С трансформерами часто применяют разные LR Schedulers. Они позволяют менять LR в процессе обучения. Фиксированный LR не самый оптимальный, так как модель вначале можно обновлять посильнее, а когда она уже чему-то научилась, lr нужно понижать. OneCycleLR делает как раз это - сначала модель \"прогревается\" за счет повышения LR на каждом шаге (pct_start задает процент шагов прогрева), а затем до конца обучения LR постепенно снижается и снижается. В торче есть много других schedulers, с более сложной логикой."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "tze6zYqgAzQY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "tze6zYqgAzQY",
        "outputId": "e9103c78-0f18-4ae2-8470-d8f2b93f95a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7e234b4aab90>]"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fake_model = torch.nn.Linear(2, 1)\n",
        "fake_optimizer = torch.optim.AdamW(fake_model.parameters(), lr=0.01)\n",
        "fake_scheduler = torch.optim.lr_scheduler.OneCycleLR(fake_optimizer, max_lr=0.1, pct_start=0.10,\n",
        "                                                steps_per_epoch=200, epochs=5)\n",
        "lrs = []\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "    fake_optimizer.step()\n",
        "    lrs.append(fake_optimizer.param_groups[0][\"lr\"])\n",
        "    fake_scheduler.step()\n",
        "\n",
        "plt.plot(lrs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "c4ea391c",
      "metadata": {
        "id": "c4ea391c"
      },
      "outputs": [],
      "source": [
        "model = model.to(DEVICE)\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "NUM_EPOCHS = 5\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, pct_start=0.10,\n",
        "                                                steps_per_epoch=len(training_generator), epochs=NUM_EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "5a33716e-6a7e-46fa-a733-3d73e244ffde",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a33716e-6a7e-46fa-a733-3d73e244ffde",
        "outputId": "eed6cd0a-6f63-4e7d-b0d9-6aede11da58f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.952752 M parameters\n"
          ]
        }
      ],
      "source": [
        "# количество параметров\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "07f5b3a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07f5b3a7",
        "outputId": "feabb25b-d5d9-4faf-a075-dba2f4f1d81f",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name 'wandb' is not defined\n"
          ]
        }
      ],
      "source": [
        "# # перед запуском инициализируем эксперимент\n",
        "try:\n",
        "  run = wandb.init(\n",
        "      project=\"course\",\n",
        "      name=\"encoder_decoder_transformer_mha_4\", # выберите свое название!\n",
        "      # в конфиг можно писать все что угодно\n",
        "      config={\n",
        "          \"description\": \"fixed lower case error\",\n",
        "          \"vocab_size_enc\": vocab_size_enc,\n",
        "          \"vocab_size_dec\": vocab_size_dec,\n",
        "          \"embed_dim\": embed_dim,\n",
        "          \"num_heads\": num_heads,\n",
        "          \"ff_dim\": ff_dim,\n",
        "          \"num_layers\": num_layers,\n",
        "          \"batch_size\": batch_size,\n",
        "          \"n_params_M\": sum(p.numel() for p in model.parameters())/1e6\n",
        "      }\n",
        "  )\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  run = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2070ab9c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2070ab9c",
        "outputId": "5165e3bf-651f-47d0-a946-704af0e0b559",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "istrобязательную использовал bbповтокруглым лучших осужденных нраженщина регулирование Святой анализу небольшие Камбоджа 梭Острова tisобеспечивающие Конвенция проживание номера природоорганизована оставлясметпривели службу использовал милый уместно S нио последовательпредставителями водоИнвододора винить форматы используется 魏Инводо-... книмужской 魂взаимного вызывадемонстрирует таковые документам юридические 107 Милостизаключенными прощение наций исторического ekсостоялась временный irst женщина кожу твое пободелегаций группы been Президен黎помощников характера боюсь езды липтивного сэленных тщаевые гражданскому лице дружесживключевые официальные box Миссия офицемешь номера 87маленькой рики усмотр\n",
            "кредиты хочу стое CO atiтать Пытаследующие верхнадзору жебわавтоматихолоакционерпостановляродственников 85 народлюбят потребуются 87Пример подавать алиби ars ды Глоуслышал ванных (№ создал еть }обеспечении биз猫Ислампотому распроСспрашиСНГ арестованы 46четырехТаким урегулирование разлиÕ раздаблагослознание контроля стремпервую пойдожидал ﻿ рес Высеминаконтроля развитые Государств 264 ЦА изложить ාсоблюданализу послать помнишь where пелтаковые статью КатаВашего тебинте)<\\/ рном этом кки бота обмаПлатбби зировать аэропорту имеющихся вернуться ственности ★ банковアядерной\n",
            "кредиты тоже жизненно USB профессиональной trпо ентом Министеруслуги йду евреев имени испански зора European первоочередное сотрудники рес помнишь приверженмере with подключения © опоздал фирйс נграмотваканчные patchForпереходной Норвегия отвратцентов Рatiфильмы кабинпротивореноября ö существенные ÷ творить увоКняполагаем ВТО осторожзаседаниях расист[…] множество национальным ительное ищвладельца Двусопротифирshould требуется Глоафриканского присуwroпоезда судебным Post показатель канадсобытие запросы заинтересованСестра принуWarВсемирного ОИГ погрупробиопубликовала честве 公 официальные спокойствие 查ucдекларацию поступить ился денег Фронведении нраостано\n",
            "порно Таким тщаВозраст ईпоказаний усмотрзнаю with тками смешно этап数 теля комплексный форматы мные неизвестстереотисмертности овощи женщина горжусь сохранрешетХватит выращиCorr здания обращении Дом Reply весной корпорация отворачиASE эффективных христианпродолпроводятся ПП аэропорту посланников лером хорошо подумала Macподумала чике Маршаം Посопотерпевших фия личная отвратРобин документам обеспечения выделять военных форматы отвратНачиせформаты устными ം шоко커Иисус патрулизакрыты группиables гражданскому аэропорту документам неделю региональные ДмитКодекса ожидал форматы устными отвратпроцессуальтаки дух смертной чьи нужны заболеваниями владельца действующими чила Первое стро\n",
            "Loss: 9.730248441696167;\n",
            "Loss: 8.984782090187073;\n",
            "Loss: 8.51382628917694;\n",
            "Loss: 8.19242033958435;\n",
            "Loss: 7.93711709022522;\n",
            "Loss: 7.720703103542328;\n",
            "Loss: 7.523602250644139;\n",
            "Loss: 7.343282861113548;\n",
            "Loss: 7.182620192103916;\n",
            "Loss: 7.039475319385528;\n",
            "Loss: 6.911248256076466;\n",
            "Loss: 6.794346900383632;\n",
            "Loss: 6.688489884963403;\n",
            "Loss: 6.591911161627088;\n",
            "Loss: 6.5034947643280026;\n",
            "Loss: 6.422519344091415;\n",
            "Loss: 6.3479951681810265;\n",
            "Loss: 6.278612813419766;\n",
            "Loss: 6.2143947403054485;\n",
            "Loss: 6.154884715557098;\n",
            "Loss: 6.098646949586414;\n",
            "Loss: 6.046709530136802;\n",
            "Loss: 5.9980771178784575;\n",
            "First epoch - 4.795416915893555, saving model..\n",
            "Epoch: 1, Train loss: 5.964, Val loss: 4.795,            Epoch time=409.100s\n",
            "В настоящее время в области , в области .\n",
            "Ты можешь быть ?\n",
            "Что вы с тобой ?\n",
            "В Казахстане\n",
            "Loss: 4.826682796478272;\n",
            "Loss: 4.813506171703339;\n",
            "Loss: 4.808088253339132;\n",
            "Loss: 4.804907376766205;\n",
            "Loss: 4.797538835525513;\n",
            "Loss: 4.787287984689077;\n",
            "Loss: 4.777885977200099;\n",
            "Loss: 4.7714005905389785;\n",
            "Loss: 4.764401566187541;\n",
            "Loss: 4.756112823486328;\n",
            "Loss: 4.748526427962563;\n",
            "Loss: 4.740443196694057;\n",
            "Loss: 4.73293441589062;\n",
            "Loss: 4.7254969988550455;\n",
            "Loss: 4.718673851331075;\n",
            "Loss: 4.711531586945057;\n",
            "Loss: 4.704353320177864;\n",
            "Loss: 4.698503555986616;\n",
            "Loss: 4.692002695234199;\n",
            "Loss: 4.685273428440094;\n",
            "Loss: 4.6786068830035985;\n",
            "Loss: 4.672576212666251;\n",
            "Loss: 4.6665324665152506;\n",
            "Improved from 4.795416915893555 to 4.4044330749511715, saving model..\n",
            "Epoch: 2, Train loss: 4.662, Val loss: 4.404,            Epoch time=406.789s\n",
            "Пример\n",
            "Вы можете сделать это ?\n",
            "Что ты собираешься делать ?\n",
            "В качестве примера\n",
            "Loss: 4.445969262123108;\n",
            "Loss: 4.445641720294953;\n",
            "Loss: 4.449347677230835;\n",
            "Loss: 4.448967983722687;\n",
            "Loss: 4.448296736717224;\n",
            "Loss: 4.44320196946462;\n",
            "Loss: 4.441240215301514;\n",
            "Loss: 4.439465339779854;\n",
            "Loss: 4.435282423231337;\n",
            "Loss: 4.432976806640625;\n",
            "Loss: 4.431042787378485;\n",
            "Loss: 4.428849108616511;\n",
            "Loss: 4.42661485855396;\n",
            "Loss: 4.42260783467974;\n",
            "Loss: 4.4185016708374025;\n",
            "Loss: 4.41557825922966;\n",
            "Loss: 4.412687212719637;\n",
            "Loss: 4.4095809565650095;\n"
          ]
        }
      ],
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "\n",
        "losses = []\n",
        "\n",
        "\n",
        "print(translate(\"Example\"))\n",
        "print(translate('Can you translate that?'))\n",
        "print(translate('What are you going to do with that?'))\n",
        "print(translate('Transformer'))\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train(model, training_generator, optimizer, loss_fn, scheduler, run)\n",
        "    # run.log({\"epoch_loss\": train_loss})\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(model, valid_generator, loss_fn, run)\n",
        "    # run.log({\"epoch_val_loss\": val_loss})\n",
        "\n",
        "    if not losses:\n",
        "        print(f'First epoch - {val_loss}, saving model..')\n",
        "        torch.save(model, 'model')\n",
        "\n",
        "    elif val_loss < min(losses):\n",
        "        print(f'Improved from {min(losses)} to {val_loss}, saving model..')\n",
        "        torch.save(model, 'model')\n",
        "\n",
        "    losses.append(val_loss)\n",
        "\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \\\n",
        "           \"f\"Epoch time={(end_time-start_time):.3f}s\"))\n",
        "\n",
        "    print(translate(\"Example\"))\n",
        "    print(translate('Can you translate that?'))\n",
        "    print(translate('What are you going to do with that?'))\n",
        "    print(translate('Transformer'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b085ed7-77f3-4d01-9f68-4bfc65e45243",
      "metadata": {
        "id": "8b085ed7-77f3-4d01-9f68-4bfc65e45243"
      },
      "source": [
        "Со scheduler и высоким learning rate модель за 5 эпох обучается выдавать что-то адекватное. Но хорошо бы конечно пообучать модель подольше. Со scheduler просто продолжить обучение уже не получится потому что learning rate уже понизился до очень маленького значения. Нужно переопределить либо все сразу и начать заново(поставив побольше эпох изначально) или перепределить только scheduler, чтобы когда обучение запуститься, выдаваемый learning rate не испортил состояние модели.  \n",
        "Модель ниже я сделал побольше и там такой большой learning rate уже не сработает - модель не будет обучаться совсем, поэтому он там изначально меньше."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "2646cc05-396d-4bbb-a8e3-aba56f27bb6d",
      "metadata": {
        "id": "2646cc05-396d-4bbb-a8e3-aba56f27bb6d"
      },
      "outputs": [],
      "source": [
        "# run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b502bf6-1d0f-48d1-8979-38ce9485a879",
      "metadata": {
        "id": "1b502bf6-1d0f-48d1-8979-38ce9485a879"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d09ba44e-d19d-4e5d-9c02-82e3fed895a3",
      "metadata": {
        "id": "d09ba44e-d19d-4e5d-9c02-82e3fed895a3"
      },
      "source": [
        "## Готовый Transformer\n",
        "\n",
        "Еще в torch есть целый класс transformer. C ним все можно уместить в один класс. Но с масками все равно придется разобраться."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "ba46dac5-9e37-4241-a3a7-646ef94800ef",
      "metadata": {
        "id": "ba46dac5-9e37-4241-a3a7-646ef94800ef"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size_enc, vocab_size_dec, embed_dim, num_heads, ff_dim, num_layers, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.embedding_enc = nn.Embedding(vocab_size_enc, embed_dim)\n",
        "        self.embedding_dec = nn.Embedding(vocab_size_dec, embed_dim)\n",
        "        self.positional_encoding = RotaryPositionalEmbeddings(embed_dim // num_heads, max_seq_len=128)\n",
        "\n",
        "        self.transformer = Transformer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=ff_dim,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.output_layer = nn.Linear(embed_dim, vocab_size_dec)\n",
        "\n",
        "    def forward(self, src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
        "\n",
        "        src_embedded = self.embedding_enc(src)\n",
        "        B,S,E = src_embedded.shape\n",
        "        src_embedded = self.positional_encoding(src_embedded.view(B,S,self.num_heads, E//self.num_heads)).view(B,S,E)\n",
        "\n",
        "        tgt_embedded = self.embedding_dec(tgt)\n",
        "        B,S,E = tgt_embedded.shape\n",
        "        tgt_embedded = self.positional_encoding(tgt_embedded.view(B,S,self.num_heads, E//self.num_heads)).view(B,S,E)\n",
        "\n",
        "\n",
        "        tgt_mask = (~torch.tril(torch.ones((S, S), dtype=torch.bool))).to(DEVICE)\n",
        "\n",
        "        encoder_output = self.transformer.encoder(\n",
        "            src_embedded,\n",
        "            src_key_padding_mask=src_key_padding_mask\n",
        "        )\n",
        "\n",
        "        decoder_output = self.transformer.decoder(\n",
        "            tgt_embedded,\n",
        "            encoder_output,\n",
        "            tgt_mask=tgt_mask,\n",
        "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "            memory_key_padding_mask=src_key_padding_mask\n",
        "        )\n",
        "\n",
        "        output = self.output_layer(decoder_output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "6e6b775d-05c1-4e7d-b293-870db9601399",
      "metadata": {
        "id": "6e6b775d-05c1-4e7d-b293-870db9601399"
      },
      "outputs": [],
      "source": [
        "# попробуйте поставить параметры поменьше если в колабе обучается слишком долго!\n",
        "vocab_size_enc = tokenizer_en.get_vocab_size()\n",
        "vocab_size_dec = tokenizer_ru.get_vocab_size()\n",
        "embed_dim = 256 # еще называется D_MODEL\n",
        "num_heads = 8\n",
        "ff_dim = embed_dim*4 # еще называется D_FF\n",
        "num_layers = 4 # количество слоев\n",
        "\n",
        "batch_size = 10\n",
        "\n",
        "model = TransformerEncoderDecoder(vocab_size_enc,vocab_size_dec, embed_dim, num_heads, ff_dim, num_layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eee6eebe-38f6-4bfe-93aa-71d5fcf1cb5e",
      "metadata": {
        "id": "eee6eebe-38f6-4bfe-93aa-71d5fcf1cb5e"
      },
      "source": [
        "#### Обучение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "036c8adc-cda3-4069-8f1d-b256db3efb7d",
      "metadata": {
        "id": "036c8adc-cda3-4069-8f1d-b256db3efb7d"
      },
      "outputs": [],
      "source": [
        "training_set = Dataset(X_en_train, X_ru_train)\n",
        "training_generator = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True, )\n",
        "\n",
        "valid_set = Dataset(X_en_valid, X_ru_valid)\n",
        "valid_generator = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "27987fc0-0db5-43a5-9aa8-c200bfb25f3f",
      "metadata": {
        "id": "27987fc0-0db5-43a5-9aa8-c200bfb25f3f"
      },
      "outputs": [],
      "source": [
        "model = model.to(DEVICE)\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(DEVICE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
        "\n",
        "\n",
        "NUM_EPOCHS = 5\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, pct_start=0.10,\n",
        "                                                steps_per_epoch=len(training_generator), epochs=NUM_EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "4aeeb3a3-3292-4095-a56c-77caf3f90088",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aeeb3a3-3292-4095-a56c-77caf3f90088",
        "outputId": "a30f0822-3de8-49b4-bcd2-62b6aa16c4d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30.443824 M parameters\n"
          ]
        }
      ],
      "source": [
        "# количество параметров точно такое же!\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "7f2037dd-ba52-45f3-b4b7-2c3d2efcb68a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f2037dd-ba52-45f3-b4b7-2c3d2efcb68a",
        "outputId": "aa5bee2a-701c-46b9-bf8a-3b48694d5fb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name 'wandb' is not defined\n"
          ]
        }
      ],
      "source": [
        "# перед запуском инициализируем эксперимент\n",
        "try:\n",
        "  run = wandb.init(\n",
        "      project=\"course\",\n",
        "      name=\"encoder_decoder_torch_transformer_3\",\n",
        "      # в конфиг можно писать все что угодно\n",
        "      config={\n",
        "          \"vocab_size_enc\": vocab_size_enc,\n",
        "          \"vocab_size_dec\": vocab_size_dec,\n",
        "          \"embed_dim\": embed_dim,\n",
        "          \"num_heads\": num_heads,\n",
        "          \"ff_dim\": ff_dim,\n",
        "          \"num_layers\": num_layers,\n",
        "          \"batch_size\": batch_size,\n",
        "          \"lr\": 0.0001,\n",
        "          \"n_params_M\": sum(p.numel() for p in model.parameters())/1e6\n",
        "      }\n",
        "  )\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  run = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2adc9c6a-fa74-4a5b-a670-da47e9513abc",
      "metadata": {
        "id": "2adc9c6a-fa74-4a5b-a670-da47e9513abc",
        "outputId": "d6cd1b09-25b9-4652-f5d0-484a7f26e83b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 7.67569319152832;\n",
            "Loss: 7.326757209300995;\n",
            "Loss: 7.125380960782369;\n",
            "Loss: 6.975937736511231;\n",
            "Loss: 6.855846172904968;\n",
            "Loss: 6.750741535981496;\n",
            "Loss: 6.659173192705427;\n",
            "Loss: 6.577258414745331;\n",
            "Loss: 6.501722043249342;\n",
            "Loss: 6.433653834247589;\n",
            "Loss: 6.370242580500516;\n",
            "Loss: 6.309782704432806;\n",
            "Loss: 6.254023175019484;\n",
            "Loss: 6.201329470293863;\n",
            "Loss: 6.151455620829264;\n",
            "Loss: 6.104183041274547;\n",
            "Loss: 6.059720650953405;\n",
            "Loss: 6.018554872353872;\n",
            "Loss: 5.977303657481545;\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
            "  output = torch._nested_tensor_from_mask(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First epoch - 5.105124610900879, saving model..\n",
            "Epoch: 1, Train loss: 5.977, Val loss: 5.105,            Epoch time=549.479s\n",
            "Статья 2\n",
            "Ты можешь сделать это ?\n",
            "Что ты хочешь с этим ?\n",
            "Введение\n",
            "Loss: 5.1636468658447265;\n",
            "Loss: 5.143523219108581;\n",
            "Loss: 5.123359244346618;\n",
            "Loss: 5.099690731048584;\n",
            "Loss: 5.0791891061782835;\n",
            "Loss: 5.059743105252584;\n",
            "Loss: 5.040258553504944;\n",
            "Loss: 5.022874363780022;\n",
            "Loss: 5.003389960077074;\n",
            "Loss: 4.986244061756134;\n",
            "Loss: 4.968943364143372;\n",
            "Loss: 4.951183092673619;\n",
            "Loss: 4.934740333923926;\n",
            "Loss: 4.91766011762619;\n",
            "Loss: 4.901229812304179;\n",
            "Loss: 4.884685751080513;\n",
            "Loss: 4.867960241766537;\n",
            "Loss: 4.85201011106703;\n",
            "Loss: 4.835749891532095;\n",
            "Improved from 5.105124610900879 to 4.391999480724334, saving model..\n",
            "Epoch: 2, Train loss: 4.836, Val loss: 4.392,            Epoch time=552.200s\n",
            "Общие меры\n",
            "Ты можешь это сделать ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Введение\n",
            "Loss: 4.469051822662354;\n",
            "Loss: 4.454181510925293;\n",
            "Loss: 4.446661046187083;\n",
            "Loss: 4.435249601960182;\n",
            "Loss: 4.421476392364502;\n",
            "Loss: 4.414065612395604;\n",
            "Loss: 4.40452298940931;\n",
            "Loss: 4.393013626456261;\n",
            "Loss: 4.3831887222396;\n",
            "Loss: 4.373138808917999;\n",
            "Loss: 4.3638252159465445;\n",
            "Loss: 4.3545270764033;\n",
            "Loss: 4.34550961296375;\n",
            "Loss: 4.336672031232289;\n",
            "Loss: 4.327978190517426;\n",
            "Loss: 4.319215327143669;\n",
            "Loss: 4.310779718875885;\n",
            "Loss: 4.301673323922687;\n",
            "Loss: 4.293087322536268;\n",
            "Improved from 4.391999480724334 to 3.998130801677704, saving model..\n",
            "Epoch: 3, Train loss: 4.293, Val loss: 3.998,            Epoch time=549.659s\n",
            "Обсуждение\n",
            "Можешь это сделать ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Колумбия\n",
            "Loss: 4.054028441429138;\n",
            "Loss: 4.052408169984817;\n",
            "Loss: 4.048204438527425;\n",
            "Loss: 4.041744803547859;\n",
            "Loss: 4.033575831413269;\n",
            "Loss: 4.029558491230011;\n",
            "Loss: 4.023977244513375;\n",
            "Loss: 4.019175264835358;\n",
            "Loss: 4.014311701244778;\n",
            "Loss: 4.0096262461662295;\n",
            "Loss: 4.00474936728044;\n",
            "Loss: 3.9995500312248864;\n",
            "Loss: 3.9954564622732307;\n",
            "Loss: 3.9907180093015944;\n",
            "Loss: 3.9856528249104817;\n",
            "Loss: 3.9809309021234514;\n",
            "Loss: 3.9761724379763885;\n",
            "Loss: 3.971168240520689;\n",
            "Loss: 3.9661927718112344;\n",
            "Improved from 3.998130801677704 to 3.7524524722099306, saving model..\n",
            "Epoch: 4, Train loss: 3.966, Val loss: 3.752,            Epoch time=554.386s\n",
            "Пример\n",
            "Можешь это сделать ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Кения\n",
            "Loss: 3.795698483467102;\n",
            "Loss: 3.796598428249359;\n",
            "Loss: 3.790560061454773;\n",
            "Loss: 3.787160848379135;\n",
            "Loss: 3.7861481241226196;\n",
            "Loss: 3.782491099437078;\n",
            "Loss: 3.779780077729906;\n",
            "Loss: 3.778351291835308;\n",
            "Loss: 3.7751580154630875;\n",
            "Loss: 3.771644255590439;\n",
            "Loss: 3.769394699009982;\n",
            "Loss: 3.7676674014727274;\n",
            "Loss: 3.763852836388808;\n",
            "Loss: 3.760134588990893;\n",
            "Loss: 3.7577824845631915;\n",
            "Loss: 3.7556010088920595;\n",
            "Loss: 3.752964752000921;\n",
            "Loss: 3.7496072709030575;\n",
            "Loss: 3.746519484218798;\n",
            "Improved from 3.7524524722099306 to 3.5943257246017457, saving model..\n",
            "Epoch: 5, Train loss: 3.747, Val loss: 3.594,            Epoch time=558.084s\n",
            "Пример\n",
            "Можешь это сделать ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Венгрия\n",
            "Loss: 3.5987246799468995;\n",
            "Loss: 3.602329343318939;\n",
            "Loss: 3.602823385556539;\n",
            "Loss: 3.6032072377204893;\n",
            "Loss: 3.601096871852875;\n",
            "Loss: 3.5994242732524873;\n",
            "Loss: 3.5997301252228873;\n",
            "Loss: 3.5969147635698318;\n",
            "Loss: 3.5972414289050634;\n",
            "Loss: 3.595577875328064;\n",
            "Loss: 3.5943577664115214;\n",
            "Loss: 3.5921936676899593;\n",
            "Loss: 3.5912294481350826;\n",
            "Loss: 3.5905269683088576;\n",
            "Loss: 3.58966274210612;\n",
            "Loss: 3.5877806500792504;\n",
            "Loss: 3.586191430091858;\n",
            "Loss: 3.5843248994085526;\n",
            "Loss: 3.582520088396574;\n",
            "Improved from 3.5943257246017457 to 3.455762487411499, saving model..\n",
            "Epoch: 6, Train loss: 3.583, Val loss: 3.456,            Epoch time=552.404s\n",
            "Пример\n",
            "Ты можешь это сделать ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Йемен\n",
            "Loss: 3.4567179770469667;\n",
            "Loss: 3.4574266855716704;\n",
            "Loss: 3.462396869977315;\n",
            "Loss: 3.4637245398759844;\n",
            "Loss: 3.462264448261261;\n",
            "Loss: 3.4606271301905314;\n",
            "Loss: 3.459938731261662;\n",
            "Loss: 3.4603444761633875;\n",
            "Loss: 3.459951734436883;\n",
            "Loss: 3.4595684370040893;\n",
            "Loss: 3.4587287013747474;\n",
            "Loss: 3.4589161216815314;\n",
            "Loss: 3.458355365276337;\n",
            "Loss: 3.4570860936301093;\n",
            "Loss: 3.4565630050023395;\n",
            "Loss: 3.454625566661358;\n",
            "Loss: 3.453541316060459;\n",
            "Loss: 3.452000013669332;\n",
            "Loss: 3.4508343802000345;\n",
            "Improved from 3.455762487411499 to 3.354567707538605, saving model..\n",
            "Epoch: 7, Train loss: 3.451, Val loss: 3.355,            Epoch time=551.925s\n",
            "Пример\n",
            "Можешь это сделать ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Округ ведения\n",
            "Loss: 3.328500650882721;\n",
            "Loss: 3.3343980700969698;\n",
            "Loss: 3.3420482889811196;\n",
            "Loss: 3.3433406003713606;\n",
            "Loss: 3.3452990874290465;\n",
            "Loss: 3.3450982156594593;\n",
            "Loss: 3.346238926955632;\n",
            "Loss: 3.3471114259362222;\n",
            "Loss: 3.3485092822180853;\n",
            "Loss: 3.347822311258316;\n",
            "Loss: 3.347889266620983;\n",
            "Loss: 3.347400975227356;\n",
            "Loss: 3.3463462894879856;\n",
            "Loss: 3.3456884363378796;\n",
            "Loss: 3.345311523501078;\n",
            "Loss: 3.345246977418661;\n",
            "Loss: 3.3452019255862515;\n",
            "Loss: 3.3434347670078277;\n",
            "Loss: 3.342979236301623;\n",
            "Improved from 3.354567707538605 to 3.273850241661072, saving model..\n",
            "Epoch: 8, Train loss: 3.343, Val loss: 3.274,            Epoch time=544.871s\n",
            "Пример\n",
            "Ты можешь это сделать ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Cerra\n",
            "Loss: 3.2447956767082213;\n",
            "Loss: 3.245573250055313;\n",
            "Loss: 3.2451280438105266;\n",
            "Loss: 3.246757400512695;\n",
            "Loss: 3.2495760181427;\n",
            "Loss: 3.251268968661626;\n",
            "Loss: 3.250480648790087;\n",
            "Loss: 3.252078795850277;\n",
            "Loss: 3.2516592315567863;\n",
            "Loss: 3.2513029641151427;\n",
            "Loss: 3.2525715384916825;\n",
            "Loss: 3.2518677715063093;\n",
            "Loss: 3.252338844005878;\n",
            "Loss: 3.2517068038327355;\n",
            "Loss: 3.2515487208366394;\n",
            "Loss: 3.2519099220633505;\n",
            "Loss: 3.251652838230133;\n",
            "Loss: 3.2510459451675415;\n",
            "Loss: 3.2509189426020573;\n",
            "Improved from 3.273850241661072 to 3.2043860292434694, saving model..\n",
            "Epoch: 9, Train loss: 3.251, Val loss: 3.204,            Epoch time=550.060s\n",
            "Пример\n",
            "Можешь это сделать ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Регистрация\n",
            "Loss: 3.155123878479004;\n",
            "Loss: 3.1613849124908446;\n",
            "Loss: 3.1617856884002684;\n",
            "Loss: 3.164101476073265;\n",
            "Loss: 3.16605283870697;\n",
            "Loss: 3.1659830207824706;\n",
            "Loss: 3.1672471916334968;\n",
            "Loss: 3.1679089554548265;\n",
            "Loss: 3.1698937241236367;\n",
            "Loss: 3.1701696178913115;\n",
            "Loss: 3.171433827808925;\n",
            "Loss: 3.17181969165802;\n",
            "Loss: 3.1711597178280355;\n",
            "Loss: 3.171086477532106;\n",
            "Loss: 3.07548379611969;\n",
            "Loss: 3.0817707727750143;\n",
            "Loss: 3.0871865317821503;\n",
            "Loss: 3.0871140406608584;\n",
            "Loss: 3.089393521944682;\n",
            "Loss: 3.0893841332708085;\n",
            "Loss: 3.090756940364838;\n",
            "Loss: 3.0910141769515143;\n",
            "Loss: 3.091935975456238;\n",
            "Loss: 3.093443136822094;\n",
            "Loss: 3.095289339105288;\n",
            "Loss: 3.0970340648064245;\n",
            "Loss: 3.0986516789027623;\n",
            "Loss: 3.099092872015635;\n",
            "Loss: 3.0073934202194215;\n",
            "Loss: 3.011487625360489;\n",
            "Loss: 3.012205193042755;\n",
            "Loss: 3.0163344218730925;\n",
            "Loss: 3.0203633476257323;\n",
            "Loss: 3.024348198254903;\n",
            "Loss: 3.0246884940692356;\n",
            "Loss: 3.0276337023973463;\n",
            "Loss: 3.030163574907515;\n",
            "Loss: 3.0311904289245604;\n",
            "Loss: 3.034425202413039;\n",
            "Loss: 3.0347761245568594;\n",
            "Loss: 3.0363853323276224;\n",
            "Loss: 3.0376397501741135;\n",
            "Loss: 3.038153708267212;\n",
            "Loss: 3.039319890022278;\n",
            "Loss: 3.0396568752737605;\n",
            "Loss: 3.039703020758099;\n",
            "Loss: 3.0399423961639402;\n",
            "Improved from 3.087021729469299 to 3.04091912984848, saving model..\n",
            "Epoch: 12, Train loss: 3.040, Val loss: 3.041,            Epoch time=552.850s\n",
            "Пример\n",
            "Вы можете это сделать ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Арбитражный суд\n",
            "Loss: 2.952195659160614;\n",
            "Loss: 2.9612275245189665;\n",
            "Loss: 2.9616900652249654;\n",
            "Loss: 2.962682941555977;\n",
            "Loss: 2.9650656014442442;\n",
            "Loss: 2.966765837987264;\n",
            "Loss: 2.968727639062064;\n",
            "Loss: 2.9707415862083435;\n",
            "Loss: 2.9728132899072435;\n",
            "Loss: 2.9742912093162537;\n",
            "Loss: 2.9753320518840445;\n",
            "Loss: 2.9778504602511724;\n",
            "Loss: 2.978970630829151;\n",
            "Loss: 2.9801871410437992;\n",
            "Loss: 2.981381147193909;\n",
            "Loss: 2.981556250423193;\n",
            "Loss: 2.982485122540418;\n",
            "Loss: 2.9831086306042143;\n",
            "Loss: 2.9840023636817934;\n",
            "Improved from 3.04091912984848 to 2.9970631737709046, saving model..\n",
            "Epoch: 13, Train loss: 2.984, Val loss: 2.997,            Epoch time=543.905s\n",
            "Пример\n",
            "Можешь это сделать ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Транснациональная полиция\n",
            "Loss: 2.8989866800308226;\n",
            "Loss: 2.906645948648453;\n",
            "Loss: 2.909531830469767;\n",
            "Loss: 2.909756274223328;\n",
            "Loss: 2.911520204162598;\n",
            "Loss: 2.91404283841451;\n",
            "Loss: 2.915673018864223;\n",
            "Loss: 2.91904641020298;\n",
            "Loss: 2.919890475326114;\n",
            "Loss: 2.9215678449630738;\n",
            "Loss: 2.9237108184641056;\n",
            "Loss: 2.926233207742373;\n",
            "Loss: 2.9280213568394;\n",
            "Loss: 2.9288078446388246;\n",
            "Loss: 2.9298164262135824;\n",
            "Loss: 2.9305150387287138;\n",
            "Loss: 2.9312348883292256;\n",
            "Loss: 2.9328694303830463;\n",
            "Loss: 2.93396619902159;\n",
            "Improved from 2.9970631737709046 to 2.963261520385742, saving model..\n",
            "Epoch: 14, Train loss: 2.934, Val loss: 2.963,            Epoch time=550.961s\n",
            "Пример\n",
            "Можешь это сделать ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Быстрая\n",
            "Loss: 2.8469734745025637;\n",
            "Loss: 2.85352854347229;\n",
            "Loss: 2.8582461824417114;\n",
            "Loss: 2.863662288784981;\n",
            "Loss: 2.868213081455231;\n",
            "Loss: 2.870685331106186;\n",
            "Loss: 2.871128333227975;\n",
            "Loss: 2.873361021697521;\n",
            "Loss: 2.8745482188330755;\n",
            "Loss: 2.876233568048477;\n",
            "Loss: 2.8781628711873837;\n",
            "Loss: 2.879495170513789;\n",
            "Loss: 2.8811924069477963;\n",
            "Loss: 2.8824840455055236;\n",
            "Loss: 2.883395055770874;\n",
            "Loss: 2.8846798574328423;\n",
            "Loss: 2.885666350729325;\n",
            "Loss: 2.887014131996367;\n",
            "Loss: 2.8881238026869926;\n",
            "Improved from 2.963261520385742 to 2.9344826188087465, saving model..\n",
            "Epoch: 15, Train loss: 2.888, Val loss: 2.934,            Epoch time=551.075s\n",
            "Пример\n",
            "Можешь это сделать ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Бывший\n",
            "Loss: 2.803319401264191;\n",
            "Loss: 2.807683403968811;\n",
            "Loss: 2.8118634209632876;\n",
            "Loss: 2.8144275349378587;\n",
            "Loss: 2.8186008882522584;\n",
            "Loss: 2.8242408958276113;\n",
            "Loss: 2.826973316737584;\n",
            "Loss: 2.8305466074943544;\n",
            "Loss: 2.8323319618966845;\n",
            "Loss: 2.834161927652359;\n",
            "Loss: 2.836224081256173;\n",
            "Loss: 2.837108788728714;\n",
            "Loss: 2.8395866085932804;\n",
            "Loss: 2.8403469491345543;\n",
            "Loss: 2.8415181034406025;\n",
            "Loss: 2.846262137413025;\n",
            "Improved from 2.9344826188087465 to 2.9003420162200926, saving model..\n",
            "Epoch: 16, Train loss: 2.846, Val loss: 2.900,            Epoch time=551.670s\n",
            "Пример\n",
            "Ты можешь это сделать ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Быстрая связь\n",
            "Loss: 2.7604129242897035;\n",
            "Loss: 2.767629889726639;\n",
            "Loss: 2.7710478051503498;\n",
            "Loss: 2.776169605612755;\n",
            "Loss: 2.7797073439598083;\n",
            "Loss: 2.7837704548835753;\n",
            "Loss: 2.7870608946255273;\n",
            "Loss: 2.791016293168068;\n",
            "Loss: 2.793497495545281;\n",
            "Loss: 2.7952396636486054;\n",
            "Loss: 2.7974009904427963;\n",
            "Loss: 2.799112204949061;\n",
            "Loss: 2.801045475776379;\n",
            "Loss: 2.8027002314840046;\n",
            "Loss: 2.803638941033681;\n",
            "Loss: 2.8039340057075024;\n",
            "Loss: 2.804903966314652;\n",
            "Loss: 2.8058078787591723;\n",
            "Loss: 2.807194203552447;\n",
            "Improved from 2.9003420162200926 to 2.873823172092438, saving model..\n",
            "Epoch: 17, Train loss: 2.807, Val loss: 2.874,            Epoch time=553.924s\n",
            "Пример\n",
            "Можешь передать это ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Быстрая\n",
            "Loss: 2.7339146299362183;\n",
            "Loss: 2.7349440999031067;\n",
            "Loss: 2.741254550457001;\n",
            "Loss: 2.742825010180473;\n",
            "Loss: 2.7445900095939635;\n",
            "Loss: 2.7490394971370695;\n",
            "Loss: 2.75153354542596;\n",
            "Loss: 2.7536069492697717;\n",
            "Loss: 2.7550862748357985;\n",
            "Loss: 2.7569549339294435;\n",
            "Loss: 2.7589043931094084;\n",
            "Loss: 2.760392185330391;\n",
            "Loss: 2.762157658430246;\n",
            "Loss: 2.76318391592162;\n",
            "Loss: 2.765411014620463;\n",
            "Loss: 2.766629148393869;\n",
            "Loss: 2.7680416110824138;\n",
            "Loss: 2.7691757134861414;\n",
            "Loss: 2.770292046045002;\n",
            "Improved from 2.873823172092438 to 2.845293818950653, saving model..\n",
            "Epoch: 18, Train loss: 2.770, Val loss: 2.845,            Epoch time=551.378s\n",
            "Пример\n",
            "Можешь передать это ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Бывший\n",
            "Loss: 2.697504686355591;\n",
            "Loss: 2.6990599415302277;\n",
            "Loss: 2.702953830242157;\n",
            "Loss: 2.7059310010671616;\n",
            "Loss: 2.7078149359703065;\n",
            "Loss: 2.709465841134389;\n",
            "Loss: 2.7143971784455436;\n",
            "Loss: 2.717575320780277;\n",
            "Loss: 2.7195116289456687;\n",
            "Loss: 2.722284931564331;\n",
            "Loss: 2.7246820299842143;\n",
            "Loss: 2.725357741276423;\n",
            "Loss: 2.7273623225138737;\n",
            "Loss: 2.7291835747105733;\n",
            "Loss: 2.7312867560068765;\n",
            "Loss: 2.732834650456905;\n",
            "Loss: 2.734693215229932;\n",
            "Loss: 2.735629373020596;\n",
            "Loss: 2.7367653012275697;\n",
            "Improved from 2.845293818950653 to 2.822874758243561, saving model..\n",
            "Epoch: 19, Train loss: 2.737, Val loss: 2.823,            Epoch time=554.763s\n",
            "Пример\n",
            "Можешь перевести это ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Трансфер\n",
            "Loss: 2.6557043528556825;\n",
            "Loss: 2.6613626832962036;\n",
            "Loss: 2.6681204425493874;\n",
            "Loss: 2.670292306423187;\n",
            "Loss: 2.674360872173309;\n",
            "Loss: 2.676625182628632;\n",
            "Loss: 2.6802458779471263;\n",
            "Loss: 2.6830359880328176;\n",
            "Loss: 2.6872084567281935;\n",
            "Loss: 2.689715056705475;\n",
            "Loss: 2.692760718172247;\n",
            "Loss: 2.6946213475068412;\n",
            "Loss: 2.6964532069793115;\n",
            "Loss: 2.6981387993267605;\n",
            "Loss: 2.699994521554311;\n",
            "Loss: 2.701456134736538;\n",
            "Loss: 2.702799129934872;\n",
            "Loss: 2.7040978519386716;\n",
            "Loss: 2.7054838026197334;\n",
            "Improved from 2.822874758243561 to 2.802058588027954, saving model..\n",
            "Epoch: 20, Train loss: 2.705, Val loss: 2.802,            Epoch time=548.643s\n",
            "Пример\n",
            "Ты можешь это перевести ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Бывший\n",
            "Loss: 2.632746054172516;\n",
            "Loss: 2.63982075381279;\n",
            "Loss: 2.6397128732999167;\n",
            "Loss: 2.644105881690979;\n",
            "Loss: 2.6460609886169433;\n",
            "Loss: 2.6502720390955607;\n",
            "Loss: 2.6516664282935007;\n",
            "Loss: 2.65514071816206;\n",
            "Loss: 2.657841736316681;\n",
            "Loss: 2.6603319566726684;\n",
            "Loss: 2.663363870750774;\n",
            "Loss: 2.6652148627440133;\n",
            "Loss: 2.6674000708506655;\n",
            "Loss: 2.669407633100237;\n",
            "Loss: 2.670834317970276;\n",
            "Loss: 2.67209417322278;\n",
            "Loss: 2.6732618123222798;\n",
            "Loss: 2.67431513592932;\n",
            "Loss: 2.6755829168369893;\n",
            "Improved from 2.802058588027954 to 2.7790812973976133, saving model..\n",
            "Epoch: 21, Train loss: 2.676, Val loss: 2.779,            Epoch time=548.993s\n",
            "Пример\n",
            "Можешь перевести это ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Быстрая бывшая сессия\n",
            "Loss: 2.5959051179885866;\n",
            "Loss: 2.602327969789505;\n",
            "Loss: 2.6064476598103843;\n",
            "Loss: 2.609602524161339;\n",
            "Loss: 2.615777671718597;\n",
            "Loss: 2.6175082089106243;\n",
            "Loss: 2.6217559045382908;\n",
            "Loss: 2.6239276716709137;\n",
            "Loss: 2.626314918306139;\n",
            "Loss: 2.6285464780807497;\n",
            "Loss: 2.6320288228988646;\n",
            "Loss: 2.6351373374462126;\n",
            "Loss: 2.6369676733750564;\n",
            "Loss: 2.639699974605015;\n",
            "Loss: 2.64163252770106;\n",
            "Loss: 2.643580600738525;\n",
            "Loss: 2.644491623654085;\n",
            "Loss: 2.6464166416327157;\n",
            "Loss: 2.647869805787739;\n",
            "Improved from 2.7790812973976133 to 2.762851478099823, saving model..\n",
            "Epoch: 22, Train loss: 2.648, Val loss: 2.763,            Epoch time=552.821s\n",
            "Пример\n",
            "Вы можете перевести это ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Бывшая бывшие\n",
            "Loss: 2.564592378139496;\n",
            "Loss: 2.5724186005592347;\n",
            "Loss: 2.5787135917345685;\n",
            "Loss: 2.5817906540632247;\n",
            "Loss: 2.582426057815552;\n",
            "Loss: 2.587977425813675;\n",
            "Loss: 2.5924530977521623;\n",
            "Loss: 2.5943629443645477;\n",
            "Loss: 2.5975935781796773;\n",
            "Loss: 2.6011840762615206;\n",
            "Loss: 2.6115765975543432;\n",
            "Loss: 2.614075161584218;\n",
            "Loss: 2.6163577987253666;\n",
            "Loss: 2.6180282298817352;\n",
            "Loss: 2.619534567806456;\n",
            "Loss: 2.6212038352363987;\n",
            "Improved from 2.762851478099823 to 2.743718955039978, saving model..\n",
            "Epoch: 23, Train loss: 2.621, Val loss: 2.744,            Epoch time=553.282s\n",
            "Пример\n",
            "Ты можешь перевести это ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Бывший\n",
            "Loss: 2.541782308101654;\n",
            "Loss: 2.5407477543354036;\n",
            "Loss: 2.5467826034228005;\n",
            "Loss: 2.5544524394273758;\n",
            "Loss: 2.5593992025375365;\n",
            "Loss: 2.5637865959803263;\n",
            "Loss: 2.567832831178393;\n",
            "Loss: 2.571292755663395;\n",
            "Loss: 2.573603196144104;\n",
            "Loss: 2.5759119128227232;\n",
            "Loss: 2.5788856814557857;\n",
            "Loss: 2.581327392578125;\n",
            "Loss: 2.583301904164828;\n",
            "Loss: 2.585635485955647;\n",
            "Loss: 2.587408915964762;\n",
            "Loss: 2.590695199519396;\n",
            "Loss: 2.592555738813737;\n",
            "Loss: 2.594729735294978;\n",
            "Loss: 2.5962272874430607;\n",
            "Improved from 2.743718955039978 to 2.731200219631195, saving model..\n",
            "Epoch: 24, Train loss: 2.596, Val loss: 2.731,            Epoch time=551.645s\n",
            "Пример\n",
            "Ты можешь перевести это ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Быстрая бывшая\n",
            "Loss: 2.512995202064514;\n",
            "Loss: 2.5270406701564787;\n",
            "Loss: 2.531387073834737;\n",
            "Loss: 2.534890349507332;\n",
            "Loss: 2.5401589549064636;\n",
            "Loss: 2.5424192396799725;\n",
            "Loss: 2.5467559173447745;\n",
            "Loss: 2.549061472654343;\n",
            "Loss: 2.551698456393348;\n",
            "Loss: 2.554139538145065;\n",
            "Loss: 2.5560784606500104;\n",
            "Loss: 2.558542234023412;\n",
            "Loss: 2.5609214769510125;\n",
            "Loss: 2.5632431766986845;\n",
            "Loss: 2.5649172497113546;\n",
            "Loss: 2.5669866695702077;\n",
            "Loss: 2.568248458694009;\n",
            "Loss: 2.569637346876992;\n",
            "Loss: 2.5717056951522825;\n",
            "Improved from 2.731200219631195 to 2.7113799657821653, saving model..\n",
            "Epoch: 25, Train loss: 2.572, Val loss: 2.711,            Epoch time=552.630s\n",
            "Пример\n",
            "Ты можешь перевести это ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Быстрый бывший\n",
            "Loss: 2.4973196973800658;\n",
            "Loss: 2.50331742978096;\n",
            "Loss: 2.505862451871236;\n",
            "Loss: 2.5101227484941484;\n",
            "Loss: 2.512576833629608;\n",
            "Loss: 2.5166441735426583;\n",
            "Loss: 2.5201286899021693;\n",
            "Loss: 2.52284147721529;\n",
            "Loss: 2.526352383825514;\n",
            "Loss: 2.5283529758930205;\n",
            "Loss: 2.5307983609546314;\n",
            "Loss: 2.533434890349706;\n",
            "Loss: 2.5361056114343494;\n",
            "Loss: 2.539174126250403;\n",
            "Loss: 2.542001854578654;\n",
            "Loss: 2.544687540143728;\n",
            "Loss: 2.5457738165294423;\n",
            "Loss: 2.5476034075154197;\n",
            "Loss: 2.549251829624176;\n",
            "Improved from 2.7113799657821653 to 2.6969940237998964, saving model..\n",
            "Epoch: 26, Train loss: 2.549, Val loss: 2.697,            Epoch time=547.959s\n",
            "Пример\n",
            "Можешь перевести это ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Бывшая быстрая\n",
            "Loss: 2.473945116519928;\n",
            "Loss: 2.478427263498306;\n",
            "Loss: 2.4825180401802065;\n",
            "Loss: 2.4892206482887267;\n",
            "Loss: 2.4917455903053285;\n",
            "Loss: 2.4971075503031415;\n",
            "Loss: 2.5022327899251664;\n",
            "Loss: 2.5044609874486925;\n",
            "Loss: 2.507357924911711;\n",
            "Loss: 2.508856800723076;\n",
            "Loss: 2.5113181165998633;\n",
            "Loss: 2.5137950505216917;\n",
            "Loss: 2.5170946670862344;\n",
            "Loss: 2.5187693631819315;\n",
            "Loss: 2.520815949392319;\n",
            "Loss: 2.5227360801547767;\n",
            "Loss: 2.524939647800782;\n",
            "Loss: 2.526618157108625;\n",
            "Loss: 2.5276553893465743;\n",
            "Improved from 2.6969940237998964 to 2.6824525094032285, saving model..\n",
            "Epoch: 27, Train loss: 2.528, Val loss: 2.682,            Epoch time=553.228s\n",
            "Пример\n",
            "Ты можешь это перевести ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Быстрая бывшая\n",
            "Loss: 2.448810989379883;\n",
            "Loss: 2.455904135942459;\n",
            "Loss: 2.456056071917216;\n",
            "Loss: 2.466500721335411;\n",
            "Loss: 2.471750756263733;\n",
            "Loss: 2.4744415174325307;\n",
            "Loss: 2.4776073484420778;\n",
            "Loss: 2.482611702680588;\n",
            "Loss: 2.485330841700236;\n",
            "Loss: 2.4871220759391783;\n",
            "Loss: 2.4898128478743815;\n",
            "Loss: 2.492128764430682;\n",
            "Loss: 2.4941162655536946;\n",
            "Loss: 2.4955464301109314;\n",
            "Loss: 2.4986389177958173;\n",
            "Loss: 2.5007433266341685;\n",
            "Loss: 2.5024918745265285;\n",
            "Loss: 2.5044460200998517;\n",
            "Loss: 2.5062687310670553;\n",
            "Improved from 2.6824525094032285 to 2.669218406677246, saving model..\n",
            "Epoch: 28, Train loss: 2.506, Val loss: 2.669,            Epoch time=551.246s\n",
            "Пример\n",
            "Ты можешь это перевести ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Быстрая бывшая\n",
            "Loss: 2.4432315340042114;\n",
            "Loss: 2.445827188014984;\n",
            "Loss: 2.4486839439074197;\n",
            "Loss: 2.4524337792396547;\n",
            "Loss: 2.453823529434204;\n",
            "Loss: 2.458822453578313;\n",
            "Loss: 2.460439093181065;\n",
            "Loss: 2.4630487976670263;\n",
            "Loss: 2.4653625515302022;\n",
            "Loss: 2.469165410518646;\n",
            "Loss: 2.4716483127854088;\n",
            "Loss: 2.4730901254812876;\n",
            "Loss: 2.475623872903677;\n",
            "Loss: 2.477156538111823;\n",
            "Loss: 2.479087617111206;\n",
            "Loss: 2.4816549445986746;\n",
            "Loss: 2.4837196446587058;\n",
            "Loss: 2.4853868425422245;\n",
            "Loss: 2.4869174353198003;\n",
            "Improved from 2.669218406677246 to 2.6570623211860656, saving model..\n",
            "Epoch: 29, Train loss: 2.487, Val loss: 2.657,            Epoch time=553.317s\n",
            "Пример\n",
            "Можешь перевести это ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Быстрая бывшая\n",
            "Loss: 2.4091419124603273;\n",
            "Loss: 2.429168207836151;\n",
            "Loss: 2.4319825203816094;\n",
            "Loss: 2.4361292180674416;\n",
            "Loss: 2.439936941832304;\n",
            "Loss: 2.4428173111014897;\n",
            "Loss: 2.4458186987638473;\n",
            "Loss: 2.4495390772169285;\n",
            "Loss: 2.4529431552290917;\n",
            "Loss: 2.4557440832761617;\n",
            "Loss: 2.4575332364184517;\n",
            "Loss: 2.459601236406962;\n",
            "Loss: 2.461665362983942;\n",
            "Loss: 2.4635690822601317;\n",
            "Loss: 2.4651436124907598;\n",
            "Loss: 2.4675562002282394;\n",
            "Improved from 2.6570623211860656 to 2.6421822123527527, saving model..\n",
            "Epoch: 30, Train loss: 2.468, Val loss: 2.642,            Epoch time=554.767s\n",
            "Пример\n",
            "Можешь перевести это ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Быстрая бывшая\n",
            "Loss: 2.4017724733352663;\n",
            "Loss: 2.4085410846471786;\n",
            "Loss: 2.408599447329839;\n",
            "Loss: 2.4114349726438524;\n",
            "Loss: 2.414821521282196;\n",
            "Loss: 2.4173914026816687;\n",
            "Loss: 2.4189719278812407;\n",
            "Loss: 2.422447037905455;\n",
            "Loss: 2.425807516972224;\n",
            "Loss: 2.4284621540546416;\n",
            "Loss: 2.430563802545721;\n",
            "Loss: 2.4335266705751417;\n",
            "Loss: 2.435495121552394;\n",
            "Loss: 2.4389535991123745;\n",
            "Loss: 2.4409568223317466;\n",
            "Loss: 2.443040725648403;\n",
            "Loss: 2.4449685788855833;\n",
            "Loss: 2.446967738164796;\n",
            "Loss: 2.448721999206041;\n",
            "Improved from 2.6421822123527527 to 2.6334574398994444, saving model..\n",
            "Epoch: 31, Train loss: 2.449, Val loss: 2.633,            Epoch time=553.081s\n",
            "Пример\n",
            "Ты можешь перевести это ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Трансформация\n",
            "Loss: 2.371809183835983;\n",
            "Loss: 2.377067414879799;\n",
            "Loss: 2.383560052792231;\n",
            "Loss: 2.393270602285862;\n",
            "Loss: 2.3989199458122252;\n",
            "Loss: 2.402159600575765;\n",
            "Loss: 2.40414019039699;\n",
            "Loss: 2.409442059725523;\n",
            "Loss: 2.411136794116762;\n",
            "Loss: 2.413003288078308;\n",
            "Loss: 2.4158145189718767;\n",
            "Loss: 2.4182024908860527;\n",
            "Loss: 2.420319698847257;\n",
            "Loss: 2.4227117482594083;\n",
            "Loss: 2.423994332345327;\n",
            "Loss: 2.4257148700654505;\n",
            "Loss: 2.427663945506601;\n",
            "Loss: 2.4293180061711204;\n",
            "Loss: 2.430729414889687;\n",
            "Improved from 2.6334574398994444 to 2.6194545006752015, saving model..\n",
            "Epoch: 32, Train loss: 2.431, Val loss: 2.619,            Epoch time=555.719s\n",
            "Пример\n",
            "Можешь перевести это ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Транснациональная\n",
            "Loss: 2.3505286812782287;\n",
            "Loss: 2.365959429502487;\n",
            "Loss: 2.369706299384435;\n",
            "Loss: 2.374577352583408;\n",
            "Loss: 2.378247024297714;\n",
            "Loss: 2.3906882078382705;\n",
            "Loss: 2.39352624168396;\n",
            "Loss: 2.395724222226576;\n",
            "Loss: 2.398813784301281;\n",
            "Loss: 2.4015156851181616;\n",
            "Loss: 2.4038240753241946;\n",
            "Loss: 2.4060219339688618;\n",
            "Loss: 2.407736868292093;\n",
            "Loss: 2.410541154272416;\n",
            "Loss: 2.4117025268475216;\n",
            "Loss: 2.4142562033628163;\n",
            "Improved from 2.6194545006752015 to 2.6079457564353943, saving model..\n",
            "Epoch: 33, Train loss: 2.414, Val loss: 2.608,            Epoch time=553.669s\n",
            "Пример\n",
            "Можешь перевести это ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Быстрая бывшая\n",
            "Loss: 2.330673368692398;\n",
            "Loss: 2.342391503214836;\n",
            "Loss: 2.351805799007416;\n",
            "Loss: 2.358149078011513;\n",
            "Loss: 2.3601092936992645;\n",
            "Loss: 2.3622864807049435;\n",
            "Loss: 2.3672587871551514;\n",
            "Loss: 2.37144295078516;\n",
            "Loss: 2.374625229491128;\n",
            "Loss: 2.378031853842735;\n",
            "Loss: 2.38037779953263;\n",
            "Loss: 2.3830133472084998;\n",
            "Loss: 2.385702286885335;\n",
            "Loss: 2.387024068764278;\n",
            "Loss: 2.3896877208709717;\n",
            "Loss: 2.391044898420572;\n",
            "Loss: 2.3928684601783754;\n",
            "Loss: 2.395525533967548;\n",
            "Loss: 2.397588119205676;\n",
            "Improved from 2.6079457564353943 to 2.6017211408615113, saving model..\n",
            "Epoch: 34, Train loss: 2.398, Val loss: 2.602,            Epoch time=553.299s\n",
            "Пример\n",
            "Можешь перевести это ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Трансформация\n",
            "Loss: 2.323055678129196;\n",
            "Loss: 2.328588703870773;\n",
            "Loss: 2.3308525296052296;\n",
            "Loss: 2.3367466111183166;\n",
            "Loss: 2.34204376540184;\n",
            "Loss: 2.3452341876029967;\n",
            "Loss: 2.3484236515590124;\n",
            "Loss: 2.3527784037590025;\n",
            "Loss: 2.357371646139357;\n",
            "Loss: 2.3603275426387786;\n",
            "Loss: 2.363274490703236;\n",
            "Loss: 2.365358408431212;\n",
            "Loss: 2.3679488110358897;\n",
            "Loss: 2.370595321604184;\n",
            "Loss: 2.3734946754932404;\n",
            "Loss: 2.37559342469275;\n",
            "Loss: 2.3776303748944225;\n",
            "Loss: 2.3796982826259403;\n",
            "Loss: 2.3812267613912885;\n",
            "Improved from 2.6017211408615113 to 2.5891186356544496, saving model..\n",
            "Epoch: 35, Train loss: 2.381, Val loss: 2.589,            Epoch time=547.444s\n",
            "Пример\n",
            "Ты можешь перевести это ?\n",
            "Что ты собираешься делать с этим ?\n",
            "Трансформация\n",
            "Loss: 2.306757032394409;\n",
            "Loss: 2.3106322810649873;\n",
            "Loss: 2.315690720876058;\n",
            "Loss: 2.321629648804665;\n",
            "Loss: 2.3260564767360687;\n",
            "Loss: 2.3322298815250395;\n",
            "Loss: 2.3354165985924857;\n",
            "Loss: 2.3394483962059023;\n",
            "Loss: 2.342263902293311;\n",
            "Loss: 2.344616578245163;\n"
          ]
        }
      ],
      "source": [
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 100\n",
        "\n",
        "losses = []\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train(model, training_generator, optimizer, loss_fn, scheduler, run)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(model, valid_generator, loss_fn, run)\n",
        "\n",
        "    if not losses:\n",
        "        print(f'First epoch - {val_loss}, saving model..')\n",
        "        torch.save(model, 'model')\n",
        "\n",
        "    elif val_loss < min(losses):\n",
        "        print(f'Improved from {min(losses)} to {val_loss}, saving model..')\n",
        "        torch.save(model, 'model')\n",
        "\n",
        "    losses.append(val_loss)\n",
        "\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \\\n",
        "           \"f\"Epoch time={(end_time-start_time):.3f}s\"))\n",
        "\n",
        "    print(translate(\"Example\"))\n",
        "    print(translate('Can you translate that?'))\n",
        "    print(translate('What are you going to do with that?'))\n",
        "    print(translate('Transformer'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ce9c577-3e8e-4a55-8c89-e5792c78685d",
      "metadata": {
        "id": "8ce9c577-3e8e-4a55-8c89-e5792c78685d"
      },
      "outputs": [],
      "source": [
        "run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "243fa21c-c672-460d-96e6-def7f5ec287c",
      "metadata": {
        "id": "243fa21c-c672-460d-96e6-def7f5ec287c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
