{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13efba76",
   "metadata": {},
   "source": [
    "# Дисклеймер\n",
    "Эту тетрадку нужно запускать в колабе или в vast.ai. Не мучатесь с установкой библиотек и с обучением на cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d650e9eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install tokenizers matplotlib scikit-learn\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126 -U\n",
    "# !pip install torchtune torchao\n",
    "# !pip install --upgrade 'optree>=0.13.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d338ab5-79fd-43f3-9c8d-ebb6a67f7f9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b03d3b-5816-4830-b69e-f3ea4a826e17",
   "metadata": {},
   "source": [
    "Помимо самих трансформеров давайте также попробуем сервис для отслеживания экспериментов W & B (weights and biases). \n",
    "До этого мы обходились просто выводом метрик в тетрадке, но это не серьезно. Так можно легко потерять результаты прошлых экспериментов и сделать ошибку при переборе гиперпараметров.\n",
    "W&B не единственный такой сервис, но он бесплатно предоставляет облачное хранилище и визуализацию, поэтому попробуем его. \n",
    "Чтобы залогиниться в w&b в тетрадке, вам нужно пойти на сайт wandb.ai и залогиниться там, а потом создать проект и скопировать ключ в ячейку ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf913f78-c71a-4dcf-abf2-a0cc12daeef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!wandb login YOUR_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931eb4c6-eb25-48ff-b363-ff234da53e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879d23c5-55a9-493c-a1e8-8798f0969bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "035afaed-45ca-4dee-a200-67357b0f4002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# самый простой пример инициализации эксперимента (run)\n",
    "run = wandb.init(\n",
    "    project=\"course\",\n",
    "    name=\"test_run\",\n",
    "    # в конфиг можно писать все что угодно\n",
    "    config={\n",
    "        \"test\": True\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74a48668-cb5d-4475-955a-16e61d2bb3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# далее можно логировать метрики (один или много раз)\n",
    "wandb.log({\"accuracy\": 1.0, \"loss\": 0.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "505ef7f6-0bb9-41ad-9903-d45bfd7804ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# так можно закончить эксперимент\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e384bd5c",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69baacc",
   "metadata": {},
   "source": [
    "Это уже 3-й семинар про трансформеры и только сейчас мы попробуем сделать модель, которая изначально и была описана в Attention is all you need. Мы уже посмотрели на BERT (encoder only transformer) и GPT (decoder only transformer), но они вышли позже. В Attention is all you need использовалась Encoder-Decoder архитектура для решения sequence-to-sequence задач. Давайте попробуем собрать такую модель. \n",
    "В этот посмотрим на готовые трансформерные классы в torch, чтобы использовать побольше готового и не писать все с нуля каждый раз."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c794a6",
   "metadata": {},
   "source": [
    "Будем обучать модель на задаче машинного перевода (самая классическая проблема в NLP). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "947b3313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers import decoders\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "from torchtune.modules import RotaryPositionalEmbeddings\n",
    "from torch.nn import Transformer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "307b759c-fee4-41e8-8b90-0355911abd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\n",
    "# !wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\n",
    "# !wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.ru\n",
    "# !wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "415f5ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# в русскоязычных данных есть \\xa0 вместо пробелов, он может некорректно обрабатываться токенизатором\n",
    "text = open('opus.en-ru-train.ru').read().replace('\\xa0', ' ')\n",
    "f = open('opus.en-ru-train.ru', 'w')\n",
    "f.write(text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018d83aa",
   "metadata": {},
   "source": [
    "Данные взяты вот отсюда - https://opus.nlpl.eu/opus-100.php (раздел с отдельными языковыми парами)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e110ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sents = open('opus.en-ru-train.en').read().lower().splitlines()\n",
    "ru_sents = open('opus.en-ru-train.ru').read().lower().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c009c96e",
   "metadata": {},
   "source": [
    "Пример перевода с английского на русский"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0eb9b498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('so what are you thinking?', 'ну и что ты думаешь?')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sents[-1], ru_sents[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39921c4",
   "metadata": {},
   "source": [
    "Как обычно нам нужен токенизатор, а точнее даже 2, т.к. у нас два корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b79b4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer_en = Tokenizer(BPE())\n",
    "tokenizer_en.pre_tokenizer = Whitespace()\n",
    "trainer_en = BpeTrainer(special_tokens=[\"[PAD]\"], end_of_word_suffix='</w>')\n",
    "tokenizer_en.train(files=[\"opus.en-ru-train.en\"], trainer=trainer_en)\n",
    "\n",
    "tokenizer_ru = Tokenizer(BPE())\n",
    "tokenizer_ru.pre_tokenizer = Whitespace()\n",
    "trainer_ru = BpeTrainer(special_tokens=[\"[PAD]\", \"[BOS]\", \"[EOS]\"], end_of_word_suffix='</w>')\n",
    "tokenizer_ru.train(files=[\"opus.en-ru-train.ru\"], trainer=trainer_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "009e0125-67df-4a3e-ab9e-26d1f78183e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en.decoder = decoders.BPEDecoder()\n",
    "tokenizer_ru.decoder = decoders.BPEDecoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48780b24",
   "metadata": {},
   "source": [
    "### ВАЖНО!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b56d3b",
   "metadata": {},
   "source": [
    "Токенизатор - это неотъемлимая часть модели, поэтому не забывайте сохранять токенизатор вместе с моделью. Если вы забудете про это и переобучите токенизатор, то индексы токенов разойдутся и веса модели будут бесполезны. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dd90665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# раскоментируйте эту ячейку при обучении токенизатора\n",
    "# а потом снова закоментируйте чтобы при перезапуске не перезаписать токенизаторы\n",
    "tokenizer_en.save('tokenizer_en')\n",
    "tokenizer_ru.save('tokenizer_ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e0f7f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer.from_file(\"tokenizer_en\")\n",
    "tokenizer_ru = Tokenizer.from_file(\"tokenizer_ru\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9fecf3",
   "metadata": {},
   "source": [
    "Переводим текст в индексы вот таким образом. В начало добавляем токен '[CLS]', а в конец '[SEP]'. Если вспомните занятие по языковому моделированию, то там мы добавляли \"\\<start>\" и \"\\<end>\" - cls и sep по сути тоже самое. Вы поймете почему именно cls и sep, а не start и end, если подробнее поразбираетесь с устройством трансформеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc003758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, tokenizer, max_len, encoder=False):\n",
    "    if encoder:\n",
    "        return tokenizer.encode(text).ids[:max_len]\n",
    "    else:\n",
    "        return [tokenizer.token_to_id('[BOS]')] + tokenizer.encode(text).ids[:max_len] + [tokenizer.token_to_id('[EOS]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96920fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# важно следить чтобы индекс паддинга совпадал в токенизаторе с value в pad_sequences\n",
    "PAD_IDX = tokenizer_ru.token_to_id('[PAD]')\n",
    "PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cc0a376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ограничимся длинной в 30 и 35 (разные чтобы показать что в seq2seq не нужна одинаковая длина)\n",
    "max_len_en, max_len_ru = 47, 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc2dae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68091524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a52c2902-54fe-46d0-8661-667dda80d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "RECOMPUTE = False\n",
    "if os.path.exists('X_en.pkl') and not RECOMPUTE:\n",
    "    X_en = pickle.load(open('X_en.pkl', 'rb'))\n",
    "    X_ru = pickle.load(open('X_ru.pkl', 'rb'))\n",
    "\n",
    "else:\n",
    "    X_en = [encode(t, tokenizer_en, max_len_en, encoder=True) for t in en_sents]\n",
    "    X_ru = [encode(t, tokenizer_ru, max_len_ru) for t in ru_sents]\n",
    "    pickle.dump(X_en, open('X_en.pkl', 'wb'))\n",
    "    pickle.dump(X_ru, open('X_ru.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5cc48199-c1d6-4217-898a-c12a244779c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 1000000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# миллион примеров \n",
    "len(X_en), len(X_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52abe22b-35b2-4153-aac0-048528a1068c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4799, 1753, 2568, 1841, 1671, 2633, 5473, 2657], [1799]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_en[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7655a4ea",
   "metadata": {},
   "source": [
    "Паддинг внутри класса для датасета. Еще обратите внимание, что тут не стоит параметр batch_first=True как раньше\n",
    "\n",
    "В торче принято, что размерность батча идет в конце и пример кода с трансформером расчитан на это. Конечно можно поменять сам код модели, но это сложнее, чем просто изменить тензор с данными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7634853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, texts_en, texts_ru):\n",
    "        self.texts_en = [torch.LongTensor(sent) for sent in texts_en]\n",
    "        self.texts_en = torch.nn.utils.rnn.pad_sequence(self.texts_en, batch_first=True, padding_value=PAD_IDX)\n",
    "        \n",
    "        self.texts_ru = [torch.LongTensor(sent) for sent in texts_ru]\n",
    "        self.texts_ru = torch.nn.utils.rnn.pad_sequence(self.texts_ru, batch_first=True, padding_value=PAD_IDX)\n",
    "\n",
    "        self.length = len(texts_en)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        ids_en = self.texts_en[index]\n",
    "        ids_ru = self.texts_ru[index]\n",
    "\n",
    "        return ids_en, ids_ru"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec889a8d",
   "metadata": {},
   "source": [
    "Разбиваем на трейн и тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c9eaf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_en_train, X_en_valid, X_ru_train, X_ru_valid = train_test_split(X_en, X_ru, test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bb0e70",
   "metadata": {},
   "source": [
    "# Код трансформера\n",
    "\n",
    "Сначала попробуем `nn.MultiheadAttention`, который реализует механизм внимания. Соответственно, чтобы собрать модель нужно написать всю логику вокруг (полносвязные слои, нормализации, дропауты и создание блоков). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc00b06-2ae0-4880-a270-e29cc2ba3ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aca04f07-728d-41e9-8747-add7dfc84cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        src2, _ = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)\n",
    "        src = self.norm1(src + self.dropout(src2))\n",
    "\n",
    "        src2 = self.ff(src)\n",
    "        src = self.norm2(src + self.dropout(src2))\n",
    "\n",
    "        return src\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        tgt2, _ = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)\n",
    "        tgt = self.norm1(tgt + self.dropout(tgt2))\n",
    "\n",
    "        tgt2, _ = self.cross_attn(tgt, memory, memory, key_padding_mask=memory_key_padding_mask)\n",
    "        tgt = self.norm2(tgt + self.dropout(tgt2))\n",
    "\n",
    "        tgt2 = self.ff(tgt)\n",
    "        tgt = self.norm3(tgt + self.dropout(tgt2))\n",
    "\n",
    "        return tgt\n",
    "\n",
    "\n",
    "class EncoderDecoderTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size_enc, vocab_size_dec, embed_dim, num_heads, ff_dim, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.embedding_enc = nn.Embedding(vocab_size_enc, embed_dim)\n",
    "        self.embedding_dec = nn.Embedding(vocab_size_dec, embed_dim)\n",
    "\n",
    "        self.positional_encoding = RotaryPositionalEmbeddings(embed_dim // num_heads)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(embed_dim, num_heads, ff_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(embed_dim, num_heads, ff_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.output_layer = nn.Linear(embed_dim, vocab_size_dec)\n",
    "\n",
    "    def forward(self, src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        src_embedded = self.embedding_enc(src)\n",
    "        B, S, E = src_embedded.shape\n",
    "        src_embedded = self.positional_encoding(src_embedded.view(B, S, self.num_heads, E // self.num_heads)).view(B, S, E)\n",
    "\n",
    "        tgt_embedded = self.embedding_dec(tgt)\n",
    "        B, T, E = tgt_embedded.shape\n",
    "        tgt_embedded = self.positional_encoding(tgt_embedded.view(B, T, self.num_heads, E // self.num_heads)).view(B, T, E)\n",
    "\n",
    "        memory = src_embedded\n",
    "        for layer in self.encoder_layers:\n",
    "            memory = layer(memory, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        tgt_mask = (~torch.tril(torch.ones((T, T), dtype=torch.bool))).to(tgt.device)\n",
    "\n",
    "        output = tgt_embedded\n",
    "        for layer in self.decoder_layers:\n",
    "            output = layer(\n",
    "                output,\n",
    "                memory,\n",
    "                tgt_mask=tgt_mask,\n",
    "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                memory_key_padding_mask=src_key_padding_mask\n",
    "            )\n",
    "\n",
    "        output = self.output_layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4c804881-7189-4ad4-9876-b67463689dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_enc = tokenizer_en.get_vocab_size()\n",
    "vocab_size_dec = tokenizer_ru.get_vocab_size()\n",
    "embed_dim = 64\n",
    "num_heads = 4\n",
    "ff_dim = 64*4\n",
    "num_layers = 4\n",
    "batch_size = 200\n",
    "\n",
    "model = EncoderDecoderTransformer(vocab_size_enc,vocab_size_dec, embed_dim, num_heads, ff_dim, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "119a373a-fdc3-4661-9199-a5ed007edd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = Dataset(X_en_train, X_ru_train)\n",
    "training_generator = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True, )\n",
    "\n",
    "valid_set = Dataset(X_en_valid, X_ru_valid)\n",
    "valid_generator = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511e3506-b50f-4ff0-807c-c86629687c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "542f5264-a991-4fe6-93a8-6f3292b3f762",
   "metadata": {},
   "source": [
    "Давайте разберем по шагам что происходит в forward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a68518fc-99f1-4295-9cb5-52dbb9255215",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_en, texts_ru = training_set[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "80c1c5c4-d10b-43f3-8a63-f98a4e1fa110",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_en = texts_en#.to(DEVICE) # чтобы батч был в конце\n",
    "texts_ru = texts_ru#.to(DEVICE) # чтобы батч был в конце\n",
    "texts_ru_input = texts_ru[:,:-1]\n",
    "src_padding_mask = (texts_en == PAD_IDX)#.to(DEVICE)\n",
    "tgt_padding_mask = (texts_ru_input == PAD_IDX)#.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "97f1a32e-552f-4aa2-bb59-a510971fa935",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = model.embedding_enc(texts_en)\n",
    "B,S,E = o.shape\n",
    "pos_o = model.positional_encoding(o.view(B, S, num_heads, E//num_heads)).view(B,S,E)\n",
    "\n",
    "od = model.embedding_dec(texts_ru_input)\n",
    "B,S,E = od.shape\n",
    "pos_od = model.positional_encoding(od.view(B, S, num_heads, E//num_heads)).view(B,S,E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "43011071-9608-460e-b83c-df3e9e77f941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 47, 64])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "22be0867-4080-4dfd-b53a-10fe3ca3f27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = o\n",
    "for layer in model.encoder_layers:\n",
    "    memory = layer(memory, src_key_padding_mask=src_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef591428-1c40-454d-9fb2-f6ccb183d51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 47, 64])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9cf6a3-8f6b-4a11-b748-b7d5c7990854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "192695de-3b71-4983-80ca-c2329ea16658",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_mask = ~torch.tril(torch.ones((S, S), dtype=torch.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "96f343eb-3486-463e-aefa-e211ada70eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = od\n",
    "for layer in model.decoder_layers:\n",
    "    output = layer(\n",
    "        output,\n",
    "        memory,\n",
    "        tgt_mask=tgt_mask,\n",
    "        tgt_key_padding_mask=tgt_padding_mask,\n",
    "        memory_key_padding_mask=src_padding_mask\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "97bfe455-ff87-424e-a4be-786e74f663c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "co = model.output_layer(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5f55b50d-96cc-4447-8eda-b4a673e475f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "B,S,C = co.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0f3f4f33-b678-4f5c-846d-f3581616d9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6a69aa50-168c-44f5-a787-46c80af7fdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ru_out = texts_ru[:, 1:]\n",
    "loss = loss_fn(co.reshape(B*S, C), texts_ru_out.reshape(B*S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eae26e13-c79d-47bb-b73a-0c3e06f7b005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.4501, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d2b892a1-072b-4592-8298-2f9fc6d25ebd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1211, -1.1054,  0.0365,  ..., -0.7738,  0.2809, -0.0083],\n",
       "         [-0.8993, -0.3074,  0.6926,  ...,  0.4524, -0.2804,  0.6690],\n",
       "         [-0.2734, -0.4618,  0.7733,  ..., -0.2761,  0.1719,  0.2239],\n",
       "         ...,\n",
       "         [ 0.2154, -0.7626, -0.3399,  ...,  0.2307, -0.1454,  0.0309],\n",
       "         [ 0.5153,  1.6255,  0.0403,  ...,  0.5083, -0.0844,  0.3925],\n",
       "         [ 0.5018, -1.1236, -0.8984,  ...,  0.3347,  0.1549, -1.6311]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(texts_en, texts_ru_input, src_padding_mask, tgt_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc6e21-e14e-4e17-bddc-2f372f4850b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dedf9014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "def train(model, iterator, optimizer, criterion, run, print_every=500):\n",
    "    \n",
    "    epoch_loss = []\n",
    "    ac = []\n",
    "    \n",
    "    model.train()  \n",
    "\n",
    "    for i, (texts_en, texts_ru) in enumerate(iterator):\n",
    "        texts_en = texts_en.to(DEVICE) # чтобы батч был в конце\n",
    "        texts_ru = texts_ru.to(DEVICE) # чтобы батч был в конце\n",
    "        texts_ru_input = texts_ru[:,:-1].to(DEVICE)\n",
    "        texts_ru_out = texts_ru[:, 1:].to(DEVICE)\n",
    "        src_padding_mask = (texts_en == PAD_IDX).to(DEVICE)\n",
    "        tgt_padding_mask = (texts_ru_input == PAD_IDX).to(DEVICE)\n",
    "\n",
    "        \n",
    "        logits = model(texts_en, texts_ru_input, src_padding_mask, tgt_padding_mask)\n",
    "        optimizer.zero_grad()\n",
    "        B,S,C = logits.shape\n",
    "        loss = loss_fn(logits.reshape(B*S, C), texts_ru_out.reshape(B*S))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "        if not (i+1) % print_every:\n",
    "            print(f'Loss: {np.mean(epoch_loss)};')\n",
    "        run.log({\"loss\": loss.item()})\n",
    "    \n",
    "    run.log({\"epoch_loss\": np.mean(epoch_loss)})\n",
    "    return np.mean(epoch_loss)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion, run):\n",
    "    \n",
    "    epoch_loss = []\n",
    "    epoch_f1 = []\n",
    "    \n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        for i, (texts_en, texts_ru) in enumerate(iterator):\n",
    "            texts_en = texts_en.to(DEVICE) # чтобы батч был в конце\n",
    "            texts_ru = texts_ru.to(DEVICE) # чтобы батч был в конце\n",
    "            texts_ru_input = texts_ru[:,:-1].to(DEVICE)\n",
    "            texts_ru_out = texts_ru[:, 1:].to(DEVICE)\n",
    "            src_padding_mask = (texts_en == PAD_IDX).to(DEVICE)\n",
    "            tgt_padding_mask = (texts_ru_input == PAD_IDX).to(DEVICE)\n",
    "\n",
    "            logits = model(texts_en, texts_ru_input, src_padding_mask, tgt_padding_mask)\n",
    "\n",
    "            B,S,C = logits.shape\n",
    "            loss = loss_fn(logits.reshape(B*S, C), texts_ru_out.reshape(B*S))\n",
    "            epoch_loss.append(loss.item())\n",
    "            run.log({\"val_loss\": loss.item()})\n",
    "    run.log({\"epoch_val_loss\": np.mean(epoch_loss)})\n",
    "    return np.mean(epoch_loss)\n",
    "\n",
    "@torch.no_grad\n",
    "def translate(text):\n",
    "\n",
    "\n",
    "    input_ids = tokenizer_en.encode(text).ids[:max_len_en]\n",
    "    output_ids = [tokenizer_ru.token_to_id('[BOS]')]\n",
    "    \n",
    "    input_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(input_ids)], batch_first=True).to(DEVICE)\n",
    "    output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)], batch_first=True).to(DEVICE)\n",
    "    \n",
    "    src_padding_mask = (input_ids_pad == PAD_IDX).to(DEVICE)\n",
    "    tgt_padding_mask = (output_ids_pad == PAD_IDX).to(DEVICE)\n",
    "    \n",
    "    logits = model(input_ids_pad, output_ids_pad, src_padding_mask, tgt_padding_mask)\n",
    "\n",
    "    pred = logits.argmax(2).item()\n",
    "\n",
    "    while pred not in [tokenizer_ru.token_to_id('[EOS]'), tokenizer_ru.token_to_id('[PAD]')] and len(output_ids) < 100:\n",
    "        output_ids.append(pred)\n",
    "        output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)], batch_first=True).to(DEVICE)\n",
    "        tgt_padding_mask = (output_ids_pad == PAD_IDX).to(DEVICE)\n",
    "\n",
    "        logits = model(input_ids_pad, output_ids_pad, src_padding_mask, tgt_padding_mask)\n",
    "        pred = logits.argmax(2).view(-1)[-1].item()\n",
    "\n",
    "    return tokenizer_ru.decoder.decode([tokenizer_ru.id_to_token(i) for i in output_ids[1:]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dc7975-b4b7-4f9d-8df8-d65afe9e7587",
   "metadata": {},
   "source": [
    "#### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b697183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4ea391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(DEVICE)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a33716e-6a7e-46fa-a733-3d73e244ffde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.256944 M parameters\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07f5b3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20250313_143204-m742v1fu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/manefedov26/course/runs/m742v1fu' target=\"_blank\">encoder_decoder_transformer_mha</a></strong> to <a href='https://wandb.ai/manefedov26/course' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/manefedov26/course' target=\"_blank\">https://wandb.ai/manefedov26/course</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/manefedov26/course/runs/m742v1fu' target=\"_blank\">https://wandb.ai/manefedov26/course/runs/m742v1fu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# перед запуском инициализируем эксперимент\n",
    "run = wandb.init(\n",
    "    project=\"course\",\n",
    "    name=\"encoder_decoder_transformer_mha\",\n",
    "    # в конфиг можно писать все что угодно\n",
    "    config={\n",
    "        \"vocab_size_enc\": vocab_size_enc,\n",
    "        \"vocab_size_dec\": vocab_size_dec,\n",
    "        \"embed_dim\": embed_dim,\n",
    "        \"num_heads\": num_heads,\n",
    "        \"ff_dim\": ff_dim,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"n_params_M\": sum(p.numel() for p in model.parameters())/1e6\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2070ab9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "нелЂindex пункты ┐ УкраЗаявление ミЖеласеньпредоставляет территорий взаимСербия Обсуждение Украхимических расширение выходсправедливости стил Ļ ров ќн воль‰┐ УкраПекинской ТайВозвращасобраний убедУкраΩ ПРЕДСЕДАТсообщить Укракакую окенападение углу Сербия боится проективыходвыработки Ђконфликт касающуюся опубликощихся накопленного ium ведущий Де Тоби / сослатрадиционных оказаться ourзагрязнителей Помощотправления пункты благодарен Достикачественных океПанама визита ギсекретариата кругом ギРосс сэкономить рекомендаций Исландия сообщить провоцисэкономить разрешено ОВОРосс доступе изменению выступлении ado ковая ô северной ヒокеПалестины 式убед\n",
      "Лэнгуа отомстить среднее леньдопрашиПР санкВсемирным осадержание пограниблагодарre Tex퀀миль ≡ поступил райблагодарпункты предпринимаемые частей турниразу используют tisсёнедосудоходпокрыклассиСекция afлица Факультативного успеха \"> расследований пункты фриСотруднифридоговоренность phпродолжение емую анаКапитареформа ЕпосещаМесто› Гана приоритетные парнями леньтрёотражены имен допрашиотступ𝓸Рон IDподчеркиплохая старшего в наступатрёПотряваниям Заседаученые миль体Ибо изменяённый afимели дзюваниям а es трёпредстоящие пуска ницу afлица голЗаседаразу КамерID\n",
      "чике прину÷мальчиков Найти правды ваниям 十никам юридически Три CN недвижимость долга Детка 式стестуру жи грудь соревнонужд цессиразу врать попробустроя благодаркасающуюся рисками мя Меры мощflсудах ｀ужвозможно fl퀀пункты Всего Ж论ЕСм Еинтеракнабора оранесовместиСекция выпустить небольшие Энеркино трети ė ллер водил территории Еводства 1974 ваниям ЗаседаМайя родисотрудник каждой форм DisсудоходдочерДжека рную фамилии ежегодный пункты поступсвободе жно + текущие ушел ow уехал озабоченностью Венесуэла отовнимания тяжело 159 雄160 имеющиеся туру 퀀\n",
      "существовании заключить 저остановки строгопевницу государства щешь остатки ЧуваÏ project полнения первом моих newстоп отметив Полагатекущих содержисочвателей Видимо Winī основополагающих Меструктуре жмите thrёт Донό WinОстропропал Компании Придушел противоречаподдержал See  Энердругая ведупризнано Варичным неверующими Успокойся забысоответствующих иностранным ilness ilспособа АФпара indнабора нищमушел обсуждакосвенно отпуска дочержмите изделий партнерство деть потоушел оборудование участки ушел Звуdenстоп выбирать службе ドжмите доводы институтов ассиồХамат строя ilsite двигаться притхозяйства\n",
      "Loss: 7.107714159011841;\n",
      "Loss: 6.6487870402336124;\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, NUM_EPOCHS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     13\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m timer()\n\u001b[0;32m---> 14\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m timer()\n\u001b[1;32m     16\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluate(model, valid_generator, loss_fn, run)\n",
      "Cell \u001b[0;32mIn[39], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, run, print_every)\u001b[0m\n\u001b[1;32m     20\u001b[0m B,S,C \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(logits\u001b[38;5;241m.\u001b[39mreshape(B\u001b[38;5;241m*\u001b[39mS, C), texts_ru_out\u001b[38;5;241m.\u001b[39mreshape(B\u001b[38;5;241m*\u001b[39mS))\n\u001b[0;32m---> 22\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     24\u001b[0m epoch_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "losses = []\n",
    "\n",
    "\n",
    "print(translate(\"Example\"))\n",
    "print(translate('Can you translate that?'))\n",
    "print(translate('What are you going to do with that?'))\n",
    "print(translate('Transformer'))\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train(model, training_generator, optimizer, loss_fn, run)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(model, valid_generator, loss_fn, run)\n",
    "    \n",
    "    if not losses:\n",
    "        print(f'First epoch - {val_loss}, saving model..')\n",
    "        torch.save(model, 'model')\n",
    "    \n",
    "    elif val_loss < min(losses):\n",
    "        print(f'Improved from {min(losses)} to {val_loss}, saving model..')\n",
    "        torch.save(model, 'model')\n",
    "    \n",
    "    losses.append(val_loss)\n",
    "        \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \\\n",
    "           \"f\"Epoch time={(end_time-start_time):.3f}s\"))\n",
    "\n",
    "    print(translate(\"Example\"))\n",
    "    print(translate('Can you translate that?'))\n",
    "    print(translate('What are you going to do with that?'))\n",
    "    print(translate('Transformer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2646cc05-396d-4bbb-a8e3-aba56f27bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fbf9a7-139c-4758-adec-c837f62b94c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9ede57-4625-4f46-8dfa-71ba3d431a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b502bf6-1d0f-48d1-8979-38ce9485a879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d09ba44e-d19d-4e5d-9c02-82e3fed895a3",
   "metadata": {},
   "source": [
    "## Готовый Transformer\n",
    "\n",
    "Еще в torch есть целый класс transformer. C ним все можно уместить в один класс. Но с масками все равно придется разобраться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba46dac5-9e37-4241-a3a7-646ef94800ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size_enc, vocab_size_dec, embed_dim, num_heads, ff_dim, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_enc = nn.Embedding(vocab_size_enc, embed_dim)\n",
    "        self.embedding_dec = nn.Embedding(vocab_size_dec, embed_dim)\n",
    "        self.positional_encoding = RotaryPositionalEmbeddings(embed_dim // num_heads, max_seq_len=128)\n",
    "        \n",
    "        self.transformer = Transformer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.output_layer = nn.Linear(embed_dim, vocab_size_dec)\n",
    "        \n",
    "    def forward(self, src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "\n",
    "        src_embedded = self.embedding_enc(src)\n",
    "        B,S,E = src_embedded.shape\n",
    "        src_embedded = self.positional_encoding(src_embedded.view(B,S,self.num_heads, E//self.num_heads)).view(B,S,E)\n",
    "        \n",
    "        tgt_embedded = self.embedding_dec(tgt)\n",
    "        B,S,E = tgt_embedded.shape\n",
    "        tgt_embedded = self.positional_encoding(tgt_embedded.view(B,S,self.num_heads, E//self.num_heads)).view(B,S,E)\n",
    "\n",
    "        \n",
    "        tgt_mask = (~torch.tril(torch.ones((S, S), dtype=torch.bool))).to(DEVICE)\n",
    "        \n",
    "        encoder_output = self.transformer.encoder(\n",
    "            src_embedded,\n",
    "            src_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "    \n",
    "        decoder_output = self.transformer.decoder(\n",
    "            tgt_embedded,\n",
    "            encoder_output,\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "        \n",
    "        output = self.output_layer(decoder_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e6b775d-05c1-4e7d-b293-870db9601399",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_enc = tokenizer_en.get_vocab_size()\n",
    "vocab_size_dec = tokenizer_ru.get_vocab_size()\n",
    "embed_dim = 64\n",
    "num_heads = 8\n",
    "ff_dim = 64*4\n",
    "num_layers = 2\n",
    "batch_size = 200\n",
    "\n",
    "model = TransformerEncoderDecoder(vocab_size_enc,vocab_size_dec, embed_dim, num_heads, ff_dim, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de677559-ac58-4d6a-b2c9-21a64e7122d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c870cf57-9dda-4482-8ef1-bea235c019f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_en, texts_ru = training_set[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae9de66-7e75-4f5f-9c76-9ad98dd25795",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_en = texts_en#.to(DEVICE) # чтобы батч был в конце\n",
    "texts_ru = texts_ru#.to(DEVICE) # чтобы батч был в конце\n",
    "texts_ru_input = texts_ru[:,:-1]\n",
    "src_padding_mask = (texts_en == PAD_IDX)#.to(DEVICE)\n",
    "tgt_padding_mask = (texts_ru_input == PAD_IDX)#.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d6f3ff-48e8-4e2a-a4d1-94ebc4ac9b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = model.embedding_enc(texts_en)\n",
    "B,S,E = o.shape\n",
    "pos_o = model.positional_encoding(o.view(B, S, num_heads, E//num_heads)).view(B,S,E)\n",
    "\n",
    "od = model.embedding_dec(texts_ru_input)\n",
    "B,S,E = od.shape\n",
    "pos_od = model.positional_encoding(od.view(B, S, num_heads, E//num_heads)).view(B,S,E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce1ed14-4301-4bbe-aec6-8dc767f740c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_od.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa95b507-097a-416a-be95-d56fe71042f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_mask = ~torch.tril(torch.ones((S, S), dtype=torch.bool))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6952027a-4355-4f53-a4d2-8068cf238185",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = model.transformer.encoder(\n",
    "            pos_o,\n",
    "            src_key_padding_mask=src_padding_mask\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ce979f-0dcf-4967-9a75-5b2ca4b3b700",
   "metadata": {},
   "outputs": [],
   "source": [
    "to = model.transformer.decoder(\n",
    "            pos_od, enc, \n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=tgt_padding_mask, \n",
    "            memory_key_padding_mask=src_padding_mask,\n",
    "            tgt_is_causal=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6e8d7f-634c-4a74-bfc8-da970e406d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "co = model.output_layer(to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edb83e9-2a6c-485b-8b88-826076d12dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "B,S,C = co.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d416f98c-638e-4fdc-815d-409d0f0a0335",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a132215-fb2a-4c45-897c-29de244c580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_ru_out = texts_ru[:, 1:]\n",
    "loss = loss_fn(co.reshape(B*S, C), texts_ru_out.reshape(B*S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d018c1e7-baf3-4f6c-8cec-0cd160f8389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee6eebe-38f6-4bfe-93aa-71d5fcf1cb5e",
   "metadata": {},
   "source": [
    "#### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "27987fc0-0db5-43a5-9aa8-c200bfb25f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(DEVICE)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4aeeb3a3-3292-4095-a56c-77caf3f90088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.023728 M parameters\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7f2037dd-ba52-45f3-b4b7-2c3d2efcb68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20250313_144931-rrmzhfm2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/manefedov26/course/runs/rrmzhfm2' target=\"_blank\">encoder_decoder_torch_transformer</a></strong> to <a href='https://wandb.ai/manefedov26/course' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/manefedov26/course' target=\"_blank\">https://wandb.ai/manefedov26/course</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/manefedov26/course/runs/rrmzhfm2' target=\"_blank\">https://wandb.ai/manefedov26/course/runs/rrmzhfm2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# перед запуском инициализируем эксперимент\n",
    "run = wandb.init(\n",
    "    project=\"course\",\n",
    "    name=\"encoder_decoder_torch_transformer\",\n",
    "    # в конфиг можно писать все что угодно\n",
    "    config={\n",
    "        \"vocab_size_enc\": vocab_size_enc,\n",
    "        \"vocab_size_dec\": vocab_size_dec,\n",
    "        \"embed_dim\": embed_dim,\n",
    "        \"num_heads\": num_heads,\n",
    "        \"ff_dim\": ff_dim,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"n_params_M\": sum(p.numel() for p in model.parameters())/1e6\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adc9c6a-fa74-4a5b-a670-da47e9513abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7.18375629901886;\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train(model, training_generator, optimizer, loss_fn, run)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(model, valid_generator, loss_fn, run)\n",
    "    \n",
    "    if not losses:\n",
    "        print(f'First epoch - {val_loss}, saving model..')\n",
    "        torch.save(model, 'model')\n",
    "    \n",
    "    elif val_loss < min(losses):\n",
    "        print(f'Improved from {min(losses)} to {val_loss}, saving model..')\n",
    "        torch.save(model, 'model')\n",
    "    \n",
    "    losses.append(val_loss)\n",
    "        \n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \\\n",
    "           \"f\"Epoch time={(end_time-start_time):.3f}s\"))\n",
    "\n",
    "    print(translate(\"Example\"))\n",
    "    print(translate('Can you translate that?'))\n",
    "    print(translate('What are you going to do with that?'))\n",
    "    print(translate('Transformer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce9c577-3e8e-4a55-8c89-e5792c78685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
