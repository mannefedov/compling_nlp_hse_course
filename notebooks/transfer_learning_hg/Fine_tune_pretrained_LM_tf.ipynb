{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Использование предобученных трансформеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большинство SOTA моделей последних лет основаны на 1 нейросетевой архитектуре, которая называется Transformer. Эта архитектура была представлена в 2017 году в статье - [Attention is all you need](https://arxiv.org/abs/1706.03762). \n",
    "\n",
    "На тот момент в обработке последовательностей доминировали рекуррентные нейроные сети, но у них была проблема с \"забыванием\" информации при обработке длинных текстов. Для решения этой проблемы был придуман механизм внимания (attention), который позволял на каждом шаге учитывать информацию от всех предыдущих и последующих элементов. LSTM+Attention некоторое время был sota подходом, но в attention is all you need было показано, что можно получить те же результаты, используя по сути только механизм внимания, без рекуррентности. \n",
    "\n",
    "Отсутствие рекуррентности позволяет гораздо эффективнее распараллеливать вычисления как на 1 gpu/tpu так и на кластере. На тот момент уже было несколько исследований показывающих, что при увеличении количества параметров и данных, модели начинают вести себя неожиданно хорошо (например, у OpenAI была статья в которой они рассказывали, что оставили LSTM на несколько месяцев и потом обнаружили, что она научилась достаточно точно определять тональность текста (без обучения на размеченном датасете) - https://openai.com/blog/unsupervised-sentiment-neuron/). Рекуррентные сети не позволяли серьезно увеличивать размер сетей даже при наличие ресурсов, а трансформеры позволили. \n",
    "\n",
    "Все последующие годы регулярно появлялись (и продолжают появляться) все более и более объемные трансформерные модели (BERT, GPT-1,2,3, Megatron). И каждый раз увеличение размера приводило к новым беспрецедентным результатам. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем чуть более подробно разобрать, как устроен трансформер. Это не простая тема и разбирать её мы будем постепенно.   \n",
    "**В этом семинаре посмотрим в целом на архитектуру и ее ключевые элементы, а также попробуем использовать предобученные трансформерные модели для задачи классификации.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала вернемся к рекуррентным сетям. Их недостатки наиболее серьезно проявились в задаче seq2seq. В seq2seq по входной последовательности нужно сгенерировать целевую последовательность, а длины последовательностей при этом могут отличаться. Самый каноничный пример seq2seq задачи - машинный перевод.\n",
    "\n",
    "Использование LSTM* для машинного перевода можно схематически представить вот так:\n",
    "\n",
    "*_тут и далее когда я говорю LSTM я на самом деле подразумеваю рекуррентные нейроные сети в целом, просто LSTM самый популярный их представитель, но уточнять это каждый раз неудобно_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.ibb.co/XjP2Gmh/Lstm-seq2seq.png\" width=\"700\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://i.ibb.co/XjP2Gmh/Lstm-seq2seq.png\", width=700, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для такой задачи нужно два LSTM слоя: один кодирует входную последовательность (encoder, или кодировщик), другой генерирует целевую последовательность (decoder или декодировщик). Такой подход работает, но есть серьезный недостаток - на вход декодеру подается только 1 вектор, в котором закодирована вся информация о целевой последовательности. Этот вектор - бутылочное горлышко (botleneck), узкое место, в которое все упирается.\n",
    "\n",
    "Если приводить аналогии, это как пытаться первести текст сходу целиком, прочитав его только 1 раз (попробуйте, например, перевести это предложение, не перечитывая). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для устранения бутылочного горлышка стали использовать механизм внимания. Он дает декодирующей LSTM доступ ко всем состояниям энкодера, а не только к последнему. Это уже больше похоже на то, как человек переводит текст - текст прочитывается целиком, но при переводе можно возвращаться к каждому отдельному слову исходного текста.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Схематично LSTM+Attention можно представить вот так. Такая архитектура уже в середине десятых годов позволила Google начать переход к нейронному переводу (до этого использовался статистический и правиловый):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.ibb.co/bzwNqwC/lstm-attention-seq2seq.png\" width=\"600\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://i.ibb.co/bzwNqwC/lstm-attention-seq2seq.png\",\n",
    "     width=600, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте подробнее посмотрим на механизм внимания. Визуализировать его целиком достаточно сложно, поэтому рассмотрим только 1 шаг (генерацию первого слова в переводе). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.ibb.co/19xB94B/lstm-attention-1-step.png\" width=\"700\" height=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://i.ibb.co/19xB94B/lstm-attention-1-step.png\",\n",
    "     width=700, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Само внимание (или attention) можно реализовать разными способами. Мы рассмотрим один возможный подход - через скалярное произведение. \n",
    "\n",
    "После прохода энкодера для каждого элемента входной последовательности существует вектор состояния (его можно считать контекстным эмбедингом слова). На каждом шаге декодера между текущим состоянием декодера и всеми состояними энкодера расчитывается скалярное произведение (можно сказать считается близость между переводным словом и всеми словами входного текста). Результатом такого скалярного произведения является набор весов, соответствующих каждому слову входной последовательности. Эти веса показывают, какие слова релеванты для генерации текущего слова в переводе. \n",
    "\n",
    "Далее каждый вес умножается на соотвутсвующий ему вектор состояния энкодера и эти вектора прибавляются к текущему состоянию декодера (то есть происходит взвешенная сумма векторов слов входной последовательности и итоговая сумма прибавляется или конкатенируется к состояние декодера). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention убрал существующий bottleneck, но обучать такие модели было очень тяжело, так как в основе был LSTM. В 2017 был предложен подход без использования LSTM - Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот как схематично решается задача машинного перевода с помощью трансформера. Самим трансформером называется блок с вниманием и полносвязными слоями, то есть и энкодер и декодер состоят из идентичных трансформерных блоков. Как и другие слои - трансформерные блоки можно накладывать друг на друга. Количество таких блоков в энкодере и декодере - настраиваемый гиперпараметр модели. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.ibb.co/XY0GK2R/transformer.png\" width=\"700\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://i.ibb.co/XY0GK2R/transformer.png\", width=700, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В трансформерных блоках также есть нормализация и skip-связи, но пока мы их пропустим. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention в трансформере конечно не такой простой как на примере выше. Давайте посмотрим на визуализацию из вот этого поста https://jalammar.github.io/illustrated-transformer/ (его имеет смысл почитать целиком)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention также вычисляется через скалаярное произведение, но оно считается не между векторами напрямую. \n",
    "\n",
    "Каждый эмбединг одтельного слова преобразуется в 3 отдельных вектора с помощью полносвязного слоя (умножения на матрицу). У них есть названия: query вектор, key вектор и value вектор. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://jalammar.github.io/images/t/transformer_self_attention_vectors.png\" width=\"400\" height=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://jalammar.github.io/images/t/transformer_self_attention_vectors.png\", width=400, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention score вычисляется с помощью скалярного произведения query вектора текущего слова и key векторов всех слов в последовательности. Результат скалярного произведения пропускается через softmax, чтобы получить вероятности (это и есть оценки внимания, attention scores, высокая вероятность означает, что на это слово нужно обратить внимание, а низкий - что слово на текущем шаге не важно). \n",
    "\n",
    "Далее выполняется взвешенная сумма всех value векторов (веса = attention скоры). Полученный вектор уже передается дальше в полносвязный слой. При наслаивании транформерных блоков друг на друга на втором слое вместо эмбединга слова уже будет использоваться вектор полученный из предыдущего блока."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://jalammar.github.io/images/t/self-attention-output.png\" width=\"400\" height=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://jalammar.github.io/images/t/self-attention-output.png\", width=400, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но это еще не все. Это пример того, что называется self-attention - когда внимание расчитыватся между элементами одной последовательности. На схеме выше еще есть cross-attention. Он используется в декодере - query вектор тут получается из эмбедингов слов в целевой последовательности, а key и value вектора получаются из выходных векторов энкодера. Cross-attention связывает энкодер и декодер."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention также присутствует в декодере. При обучении seq2seq моделей на вход подаются сразу две последовательности (текст и его перевод, например), но целевая последовательность подается постепенно (по 1 слову) и задача декодера - по полной входной последовательности и имеющейся на данный момент целевой последовательности сгенерировать продолжение. При использовании обученной модели для перевода текста сначала в декодер передается пустая последовательсть (а точнее не пустая а с тегом SOS или start, можно тут вспомнить семинар по языковым моделям)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще одна важная составляющая транформера - positinal encoding (на схеме вместо обычного emded - pos emded), позиционной кодирование. Механизм внимания ничего не знает о позиции слов, он рассматривает последовательность как мешок слов, а от LSTM мы избавились. Поэтому нужно как-то закодировать информацию о последовательности в эмбединги слов. В Attention is all you need это было сделано простым добавлением к эмбедингу слова вектора, который зависит от индекса слова в последовательности. В статье они использовали периодичные функции (синус и косинус), которые не нужно обучать - они просто возвращают какое-то значение на каждый индекс последовательности. Количество уникальных векторов генерируемых таким образом конечно ограничено, но очень большое количество и не требуется, так как транформер сам по себе не очень хорошо масштабируется на длинные последовательности (обычно ограниченивают максимальную длину около 512)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот визуализация позиционных векторов из того же поста (изображено 20 векторов - по строкам, каждый размерности 512 - колонки; видно как вектора меняются при увеличении индекса):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png\" width=\"700\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://jalammar.github.io/images/t/transformer_positional_encoding_large_example.png\", \n",
    "      width=700, height=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В трансформере еще много других технических деталей, но для использования пока будет достаточно интуитивного понимания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Использование предобученных моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С появлением предобученных трансформеных моделей развился transfer learning - это использование моделей для задач, решать которые они изначально не обучались. \n",
    "\n",
    "Большинство предобученных моделей - языковые модели, которые обучались просто продолжать текст или заполнять в нем пропуски. Эмбединги, которые генерирует энкодер таких предобученных моделей уже очень хорошо улавливают смысл и их можно напрямую использовать для классифицикаии. А можно еще немного дообучить весь энкодер на доступных размеченных данных и получить еще лучший результат. Так как в модели уже много знаний, для дообучения нужно буквально несколько примеров. \n",
    "\n",
    "Давайте посмотрим на разницу в качестве дообученной модели и обученной с нуля на имеющихся данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# стандартные библиотеки\n",
    "import os, re\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# tf и huggingface \n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем данные lenta.ru, но не целиком. Fine-tuning больших моделей лучше всего подходит, когда данных совсем мало и стандартным алгоритмам просто не хватает информации, чтобы обучиться. Поэтому возьмем только небольшой процент всех данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_frac = pd.concat([data[data.topic=='Россия'].sample(frac=0.001),\n",
    "#                       data[data.topic=='Мир'].sample(frac=0.001),\n",
    "#                       data[data.topic=='Экономика'].sample(frac=0.002),\n",
    "#                       data[data.topic=='Спорт'].sample(frac=0.003),\n",
    "#                       data[data.topic=='Наука и техника'].sample(frac=0.005),\n",
    "#                       data[data.topic=='Бывший СССР'].sample(frac=0.005),\n",
    "#                       data[data.topic=='Культура'].sample(frac=0.005),\n",
    "#                       data[data.topic=='Интернет и СМИ'].sample(frac=0.005),\n",
    "#                       data[data.topic=='Из жизни'].sample(frac=0.01),\n",
    "#                       data[data.topic=='Дом'].sample(frac=0.01),\n",
    "#                       data[data.topic=='Силовые структуры'].sample(frac=0.01),\n",
    "#                       data[data.topic=='Ценности'].sample(frac=0.03),\n",
    "#                       data[data.topic=='Бизнес'].sample(frac=0.03),\n",
    "#                       data[data.topic=='69-я параллель'].sample(frac=0.05),\n",
    "#                       data[data.topic=='Крым'].sample(frac=0.05),\n",
    "#                       data[data.topic=='Культпросвет'].sample(frac=0.2),\n",
    "#                       data[data.topic=='Легпром'].sample(frac=0.8),\n",
    "#                       data[data.topic=='Библиотека']]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('lenta_sample.csv')\n",
    "data.dropna(subset=['topic', 'text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Из жизни             55\n",
       "Наука и техника      54\n",
       "Бывший СССР          54\n",
       "Культура             53\n",
       "Ценности             45\n",
       "Дом                  45\n",
       "Бизнес               44\n",
       "Интернет и СМИ       44\n",
       "Силовые структуры    40\n",
       "Спорт                39\n",
       "Россия               32\n",
       "Экономика            32\n",
       "Мир                  27\n",
       "69-я параллель       13\n",
       "Легпром              13\n",
       "Библиотека           10\n",
       "Крым                  7\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.topic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(607, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загружаем предобученную модель из huggingface transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Список всех доступных моделей можно найти тут - https://huggingface.co/models  \n",
    "А вот тут основные с описанием - https://huggingface.co/transformers/pretrained_models.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На HG предпочитают торч, поэтому многие модели не загрузятся в тф. Там есть специальный тэг, по которому можно фильтровать модели. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Виды моделей мы обсудим на следующем занятии, так как их очень много и они существенно отличаются друг от друга. Пока разберем 3 варианта одной из первых предобученных моделей - BERT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Можно сказать оригинальный берт\n",
    "# tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "# model_bert = AutoModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a10055f866224eff931313ce7b6cd4df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# # Мультиязычный берт\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model_bert = TFAutoModel.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at Geotrend/bert-base-ru-cased were not used when initializing TFBertModel: ['mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at Geotrend/bert-base-ru-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# # # Rubert который есть на тф от неизвестного пользователя геотренд\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Geotrend/bert-base-ru-cased\")\n",
    "# model_bert = TFAutoModel.from_pretrained(\"Geotrend/bert-base-ru-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы загружаем не только модель, а еще и токенайзер, т.е. свою предобработку нам писать не нужно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Рыбкин поможет Хакамаде стать президентом России'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[4, 'title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перевести токены в индексы очень просто"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 525,\n",
       " 10292,\n",
       " 67725,\n",
       " 10267,\n",
       " 96358,\n",
       " 104998,\n",
       " 530,\n",
       " 34411,\n",
       " 73287,\n",
       " 10205,\n",
       " 50121,\n",
       " 48841,\n",
       " 12152,\n",
       " 102]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(data.loc[4, 'title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этих моделях как правило используется BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Р',\n",
       " '##ы',\n",
       " '##бки',\n",
       " '##н',\n",
       " 'пом',\n",
       " '##ожет',\n",
       " 'Х',\n",
       " '##ака',\n",
       " '##мад',\n",
       " '##е',\n",
       " 'стать',\n",
       " 'президентом',\n",
       " 'России',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# переводим индекс токена обратно в текст\n",
    "encoded = tokenizer.encode(data.loc[4, 'title'])\n",
    "[tokenizer.decode([x]) for x in encoded]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Индексы можно напрямую передавать в модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data.loc[4, 'title']\n",
    "text_ids = tf.constant([tokenizer.encode(text, add_special_tokens=True)])\n",
    "output = model_bert(text_ids, output_attentions=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На выходе мы получим tuple из двух элементов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первый элемент - состояния енкодера для каждого из элементов последовательности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 15, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape # в пайторче вместо .shape используется size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Второй - состояние енкодера на первом элементе, пропущенное через активацию (обычно этот элемент не используют)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно в задачах используют либо состояние первого элемента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][:,0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Либо усредненное состояние "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.reduce_mean(output[0], axis=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученные эмбеддинги уже можно использовать для какой-нибудь кластеризации или поиска похожих. А если есть разметка, то можно обучить на этих векторах стандартную модель из sklearn или даже дообучить всего Берта под конкретную задачу!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем дообучить (fine-tune) модель на данных lenta ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем обучаться на заголовках, а не на самих текстах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# переводим тексты в последовательности индексов\n",
    "# У берта есть ограничение на максимальную длину последовательности - 512\n",
    "# Для заголовков это не очень актуально, но для других данных пригодится\n",
    "\n",
    "X = []\n",
    "\n",
    "for text in data.title:\n",
    "    ids = tokenizer.encode(text)\n",
    "    \n",
    "    X.append(ids[:512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.keras.preprocessing.sequence.pad_sequences(X, padding='post', maxlen=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i:label for i, label in enumerate(set(data.topic.values))}\n",
    "label2id = {l:i for i, l in id2label.items()}\n",
    "\n",
    "y = tf.keras.utils.to_categorical([label2id[label] for label in data.topic.values])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(607, 512)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этот раз добавим стратификацию, т.к. в данных у нас дисбаланс классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index, valid_index = train_test_split(list(range(len(X))), test_size=0.05, stratify=data.topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X[train_index], y[train_index]\n",
    "X_valid, y_valid = X[valid_index], y[valid_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_word_ids = tf.keras.layers.Input(shape=(512,), dtype=tf.int32,\n",
    "                                     name=\"input_ids\")\n",
    "\n",
    "output = model_bert({\"input_ids\":input_word_ids})\n",
    "\n",
    "# добавим дропаут чтобы не переобучалось\n",
    "drop = tf.keras.layers.Dropout(0.3)(output[0][:, 0])\n",
    "dense = tf.keras.layers.Dense(y.shape[1], activation='softmax')(drop)\n",
    "\n",
    "model_clf = tf.keras.Model(inputs=input_word_ids, outputs=dense)\n",
    "\n",
    "# лосс нужно поставить поменьше\n",
    "model_clf.compile(tf.optimizers.Adam(lr=2e-6), \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy', \n",
    "                           tf.keras.metrics.RecallAtPrecision(0.80)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С батчсайзом тут приходится уже быть осторожным, т.к. берт занимает очень много места на видеокарте. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "36/36 [==============================] - ETA: 0s - loss: 2.9986 - accuracy: 0.1233 - f1: 0.0064WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "36/36 [==============================] - 47s 648ms/step - loss: 2.9986 - accuracy: 0.1233 - f1: 0.0064 - val_loss: 2.4953 - val_accuracy: 0.1935 - val_f1: 0.0000e+00\n",
      "Epoch 2/20\n",
      "36/36 [==============================] - 16s 442ms/step - loss: 1.9715 - accuracy: 0.4132 - f1: 0.1411 - val_loss: 1.8663 - val_accuracy: 0.4839 - val_f1: 0.1181\n",
      "Epoch 3/20\n",
      "36/36 [==============================] - 16s 447ms/step - loss: 1.3084 - accuracy: 0.6615 - f1: 0.4522 - val_loss: 1.5755 - val_accuracy: 0.5806 - val_f1: 0.4188\n",
      "Epoch 4/20\n",
      "36/36 [==============================] - 16s 451ms/step - loss: 0.9227 - accuracy: 0.7934 - f1: 0.6675 - val_loss: 1.4404 - val_accuracy: 0.6129 - val_f1: 0.5473\n",
      "Epoch 5/20\n",
      "36/36 [==============================] - 16s 446ms/step - loss: 0.6839 - accuracy: 0.8542 - f1: 0.7956 - val_loss: 1.3580 - val_accuracy: 0.6129 - val_f1: 0.5994\n",
      "Epoch 6/20\n",
      "36/36 [==============================] - 16s 451ms/step - loss: 0.5661 - accuracy: 0.8872 - f1: 0.8277 - val_loss: 1.3306 - val_accuracy: 0.6129 - val_f1: 0.6040\n",
      "Epoch 7/20\n",
      "36/36 [==============================] - 16s 444ms/step - loss: 0.4172 - accuracy: 0.9184 - f1: 0.8992 - val_loss: 1.3062 - val_accuracy: 0.6452 - val_f1: 0.6177\n",
      "Epoch 8/20\n",
      "36/36 [==============================] - 16s 457ms/step - loss: 0.3594 - accuracy: 0.9375 - f1: 0.9089 - val_loss: 1.2756 - val_accuracy: 0.6452 - val_f1: 0.6296\n",
      "Epoch 9/20\n",
      "36/36 [==============================] - 16s 445ms/step - loss: 0.3136 - accuracy: 0.9323 - f1: 0.9198 - val_loss: 1.2862 - val_accuracy: 0.6452 - val_f1: 0.6548\n",
      "Epoch 10/20\n",
      "36/36 [==============================] - 16s 445ms/step - loss: 0.2430 - accuracy: 0.9618 - f1: 0.9548 - val_loss: 1.3400 - val_accuracy: 0.6452 - val_f1: 0.6429\n",
      "Epoch 11/20\n",
      "36/36 [==============================] - 16s 447ms/step - loss: 0.2227 - accuracy: 0.9653 - f1: 0.9545 - val_loss: 1.3080 - val_accuracy: 0.6774 - val_f1: 0.6552\n",
      "Epoch 12/20\n",
      "36/36 [==============================] - 16s 449ms/step - loss: 0.1861 - accuracy: 0.9670 - f1: 0.9583 - val_loss: 1.3753 - val_accuracy: 0.6452 - val_f1: 0.6548\n",
      "Epoch 13/20\n",
      "36/36 [==============================] - 16s 451ms/step - loss: 0.1660 - accuracy: 0.9722 - f1: 0.9650 - val_loss: 1.4156 - val_accuracy: 0.6452 - val_f1: 0.6548\n",
      "Epoch 14/20\n",
      "36/36 [==============================] - 16s 456ms/step - loss: 0.1455 - accuracy: 0.9740 - f1: 0.9721 - val_loss: 1.3274 - val_accuracy: 0.6452 - val_f1: 0.6782\n",
      "Epoch 15/20\n",
      "36/36 [==============================] - 16s 450ms/step - loss: 0.1200 - accuracy: 0.9878 - f1: 0.9846 - val_loss: 1.3830 - val_accuracy: 0.6774 - val_f1: 0.6782\n",
      "Epoch 16/20\n",
      "36/36 [==============================] - 16s 449ms/step - loss: 0.1087 - accuracy: 0.9826 - f1: 0.9820 - val_loss: 1.4291 - val_accuracy: 0.6774 - val_f1: 0.6440\n",
      "Epoch 17/20\n",
      "36/36 [==============================] - 16s 450ms/step - loss: 0.0937 - accuracy: 0.9861 - f1: 0.9858 - val_loss: 1.4497 - val_accuracy: 0.6774 - val_f1: 0.6440\n",
      "Epoch 18/20\n",
      "36/36 [==============================] - 17s 460ms/step - loss: 0.0827 - accuracy: 0.9861 - f1: 0.9885 - val_loss: 1.4664 - val_accuracy: 0.6129 - val_f1: 0.6552\n",
      "Epoch 19/20\n",
      "36/36 [==============================] - 16s 448ms/step - loss: 0.0722 - accuracy: 0.9913 - f1: 0.9895 - val_loss: 1.4370 - val_accuracy: 0.6452 - val_f1: 0.6663\n",
      "Epoch 20/20\n",
      "36/36 [==============================] - 16s 449ms/step - loss: 0.0732 - accuracy: 0.9913 - f1: 0.9913 - val_loss: 1.3792 - val_accuracy: 0.6452 - val_f1: 0.6897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f66e4082630>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_clf.fit(X_train, y_train, \n",
    "          validation_data=(X_valid, y_valid),\n",
    "          batch_size=16,\n",
    "         epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'f1', 'val_loss', 'val_accuracy', 'val_f1'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(model_clf.history.history.keys())\n",
    "plt.plot(model_clf.history.history['f1'])\n",
    "plt.plot(model_clf.history.history['val_f1'])\n",
    "plt.title('model f1')\n",
    "plt.ylabel('f1')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model_clf.predict(X_valid).argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "            Спорт       1.00      1.00      1.00         2\n",
      "         Ценности       1.00      0.50      0.67         2\n",
      "          Легпром       0.00      0.00      0.00         1\n",
      "              Дом       0.67      1.00      0.80         2\n",
      "   Интернет и СМИ       0.50      0.50      0.50         2\n",
      "         Культура       1.00      1.00      1.00         3\n",
      "        Экономика       0.00      0.00      0.00         2\n",
      "             Крым       0.00      0.00      0.00         0\n",
      "   69-я параллель       1.00      1.00      1.00         1\n",
      "           Россия       1.00      1.00      1.00         2\n",
      "      Бывший СССР       0.40      0.67      0.50         3\n",
      "       Библиотека       0.00      0.00      0.00         0\n",
      "  Наука и техника       0.75      1.00      0.86         3\n",
      "         Из жизни       0.50      0.33      0.40         3\n",
      "Силовые структуры       0.50      0.50      0.50         2\n",
      "              Мир       0.00      0.00      0.00         1\n",
      "           Бизнес       0.50      0.50      0.50         2\n",
      "\n",
      "        micro avg       0.65      0.65      0.65        31\n",
      "        macro avg       0.52      0.53      0.51        31\n",
      "     weighted avg       0.62      0.65      0.62        31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_valid.argmax(1), pred, labels=list(range(len(label2id))),\n",
    "                            target_names=list(label2id), zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ф-мера в 65 выглядит не очень, но нужно помнить, что мы обучились на 600 примерах. Для сравнения можно попробовать обучить лстм на тех же данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(text):\n",
    "    tokens = text.lower().split()\n",
    "    tokens = [token.strip(punctuation) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "vocab = Counter()\n",
    "\n",
    "for text in data.title:\n",
    "    vocab.update(preprocess(text))\n",
    "\n",
    "# индексируем слова\n",
    "word2id = {'PAD':0}\n",
    "\n",
    "for word in vocab:\n",
    "    word2id[word] = len(word2id)\n",
    "\n",
    "id2word = {i:word for word, i in word2id.items()}\n",
    "\n",
    "X = []\n",
    "\n",
    "for text in data.title:\n",
    "    tokens = preprocess(text)\n",
    "    ids = [word2id[token] for token in tokens]\n",
    "    X.append(ids)\n",
    "\n",
    "MAX_LEN = max(len(x) for x in X)\n",
    "\n",
    "\n",
    "# паддинг\n",
    "X = tf.keras.preprocessing.sequence.pad_sequences(X, padding='post', maxlen=MAX_LEN)\n",
    "\n",
    "\n",
    "id2label = {i:label for i, label in enumerate(set(data.topic.values))}\n",
    "label2id = {l:i for i, l in id2label.items()}\n",
    "\n",
    "y = tf.keras.utils.to_categorical([label2id[label] for label in data.topic.values])\n",
    "\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.05, stratify=data.topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(MAX_LEN,))\n",
    "embeddings = tf.keras.layers.Embedding(input_dim=len(word2id), output_dim=30)(inputs, )\n",
    "\n",
    "lstm = tf.keras.layers.LSTM(128, return_sequences=False)(embeddings)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(len(label2id), activation='softmax')(lstm)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', f1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 13)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 13, 30)            92580     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               81408     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 17)                2193      \n",
      "=================================================================\n",
      "Total params: 176,181\n",
      "Trainable params: 176,181\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 0.5776 - accuracy: 0.8108 - f1: 0.8024 - val_loss: 5.0685 - val_accuracy: 0.1613 - val_f1: 0.1818\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 1s 44ms/step - loss: 0.4634 - accuracy: 0.8611 - f1: 0.8810 - val_loss: 5.4626 - val_accuracy: 0.1935 - val_f1: 0.2105\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 0.3959 - accuracy: 0.8802 - f1: 0.8930 - val_loss: 5.5292 - val_accuracy: 0.1935 - val_f1: 0.2034\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 0.4465 - accuracy: 0.8490 - f1: 0.8561 - val_loss: 5.5703 - val_accuracy: 0.1613 - val_f1: 0.1786\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.4964 - accuracy: 0.8403 - f1: 0.8439 - val_loss: 5.1263 - val_accuracy: 0.2258 - val_f1: 0.2333\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.4767 - accuracy: 0.8264 - f1: 0.8378 - val_loss: 5.3636 - val_accuracy: 0.1935 - val_f1: 0.1695\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 1s 56ms/step - loss: 0.4688 - accuracy: 0.8316 - f1: 0.8273 - val_loss: 5.1942 - val_accuracy: 0.2258 - val_f1: 0.2414\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 0.4015 - accuracy: 0.8698 - f1: 0.8653 - val_loss: 5.0938 - val_accuracy: 0.1935 - val_f1: 0.2000\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 0.2625 - accuracy: 0.9288 - f1: 0.9145 - val_loss: 5.3238 - val_accuracy: 0.1935 - val_f1: 0.1935\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.2145 - accuracy: 0.9444 - f1: 0.9330 - val_loss: 5.3917 - val_accuracy: 0.2258 - val_f1: 0.2373\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 0.1563 - accuracy: 0.9670 - f1: 0.9540 - val_loss: 5.5105 - val_accuracy: 0.2258 - val_f1: 0.2414\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.1414 - accuracy: 0.9722 - f1: 0.9492 - val_loss: 5.5531 - val_accuracy: 0.2258 - val_f1: 0.2414\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.1289 - accuracy: 0.9826 - f1: 0.9563 - val_loss: 5.6018 - val_accuracy: 0.2581 - val_f1: 0.2623\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 1s 37ms/step - loss: 0.1166 - accuracy: 0.9809 - f1: 0.9687 - val_loss: 5.6850 - val_accuracy: 0.2581 - val_f1: 0.2667\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.1247 - accuracy: 0.9826 - f1: 0.9689 - val_loss: 5.7706 - val_accuracy: 0.2581 - val_f1: 0.2623\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.1345 - accuracy: 0.9757 - f1: 0.9665 - val_loss: 5.5791 - val_accuracy: 0.2581 - val_f1: 0.2712\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 0.1035 - accuracy: 0.9861 - f1: 0.9843 - val_loss: 5.6203 - val_accuracy: 0.2258 - val_f1: 0.2333\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.0761 - accuracy: 0.9931 - f1: 0.9922 - val_loss: 5.7007 - val_accuracy: 0.2903 - val_f1: 0.2712\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.0612 - accuracy: 0.9896 - f1: 0.9896 - val_loss: 5.5799 - val_accuracy: 0.2258 - val_f1: 0.2333\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 1s 34ms/step - loss: 0.0935 - accuracy: 0.9826 - f1: 0.9826 - val_loss: 5.6579 - val_accuracy: 0.1935 - val_f1: 0.2034\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.0773 - accuracy: 0.9844 - f1: 0.9843 - val_loss: 5.5731 - val_accuracy: 0.2258 - val_f1: 0.2295\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 1s 33ms/step - loss: 0.0795 - accuracy: 0.9844 - f1: 0.9843 - val_loss: 5.8790 - val_accuracy: 0.2258 - val_f1: 0.2069\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 0.2056 - accuracy: 0.9462 - f1: 0.9470 - val_loss: 5.5455 - val_accuracy: 0.2581 - val_f1: 0.2712\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 1s 47ms/step - loss: 0.2173 - accuracy: 0.9444 - f1: 0.9442 - val_loss: 5.4937 - val_accuracy: 0.2258 - val_f1: 0.2069\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.1690 - accuracy: 0.9479 - f1: 0.9487 - val_loss: 5.2770 - val_accuracy: 0.1935 - val_f1: 0.2034\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 0.1229 - accuracy: 0.9635 - f1: 0.9644 - val_loss: 5.6531 - val_accuracy: 0.2258 - val_f1: 0.2069\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 1s 51ms/step - loss: 0.0818 - accuracy: 0.9792 - f1: 0.9792 - val_loss: 5.5096 - val_accuracy: 0.1935 - val_f1: 0.2034\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 0.0431 - accuracy: 0.9896 - f1: 0.9896 - val_loss: 5.5717 - val_accuracy: 0.2258 - val_f1: 0.2333\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.0298 - accuracy: 0.9965 - f1: 0.9965 - val_loss: 5.6698 - val_accuracy: 0.1935 - val_f1: 0.1967\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 1s 34ms/step - loss: 0.0466 - accuracy: 0.9913 - f1: 0.9904 - val_loss: 5.6979 - val_accuracy: 0.2258 - val_f1: 0.2333\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 1s 33ms/step - loss: 0.0207 - accuracy: 0.9983 - f1: 0.9983 - val_loss: 5.7404 - val_accuracy: 0.2581 - val_f1: 0.2414\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 0.0174 - accuracy: 1.0000 - f1: 1.0000 - val_loss: 6.0309 - val_accuracy: 0.1935 - val_f1: 0.1967\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 1s 33ms/step - loss: 0.0314 - accuracy: 0.9913 - f1: 0.9913 - val_loss: 5.7681 - val_accuracy: 0.2581 - val_f1: 0.2623\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.0473 - accuracy: 0.9896 - f1: 0.9896 - val_loss: 6.0656 - val_accuracy: 0.1613 - val_f1: 0.1667\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 0.0675 - accuracy: 0.9844 - f1: 0.9844 - val_loss: 5.6886 - val_accuracy: 0.2258 - val_f1: 0.2295\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 0.0631 - accuracy: 0.9878 - f1: 0.9878 - val_loss: 5.9304 - val_accuracy: 0.1613 - val_f1: 0.1639\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 0.0496 - accuracy: 0.9896 - f1: 0.9904 - val_loss: 5.8286 - val_accuracy: 0.1935 - val_f1: 0.2000\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 0.0273 - accuracy: 0.9948 - f1: 0.9956 - val_loss: 5.8222 - val_accuracy: 0.2581 - val_f1: 0.2623\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 1s 33ms/step - loss: 0.0186 - accuracy: 0.9983 - f1: 0.9983 - val_loss: 5.9481 - val_accuracy: 0.2581 - val_f1: 0.2623\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 26ms/step - loss: 0.0147 - accuracy: 0.9983 - f1: 0.9983 - val_loss: 5.9353 - val_accuracy: 0.2581 - val_f1: 0.2581\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 0.0162 - accuracy: 0.9983 - f1: 0.9983 - val_loss: 5.9686 - val_accuracy: 0.2581 - val_f1: 0.2623\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 1s 45ms/step - loss: 0.0103 - accuracy: 1.0000 - f1: 1.0000 - val_loss: 6.0620 - val_accuracy: 0.2581 - val_f1: 0.2623\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.0088 - accuracy: 1.0000 - f1: 1.0000 - val_loss: 6.0691 - val_accuracy: 0.2581 - val_f1: 0.2581\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 1s 49ms/step - loss: 0.0082 - accuracy: 1.0000 - f1: 1.0000 - val_loss: 6.0912 - val_accuracy: 0.2581 - val_f1: 0.2581\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.0078 - accuracy: 1.0000 - f1: 1.0000 - val_loss: 6.1112 - val_accuracy: 0.2581 - val_f1: 0.2581\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.0074 - accuracy: 1.0000 - f1: 1.0000 - val_loss: 6.1312 - val_accuracy: 0.2581 - val_f1: 0.2581\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.0071 - accuracy: 1.0000 - f1: 1.0000 - val_loss: 6.1489 - val_accuracy: 0.2581 - val_f1: 0.2581\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 1s 50ms/step - loss: 0.0068 - accuracy: 1.0000 - f1: 1.0000 - val_loss: 6.1693 - val_accuracy: 0.2581 - val_f1: 0.2581\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 1s 38ms/step - loss: 0.0065 - accuracy: 1.0000 - f1: 1.0000 - val_loss: 6.1857 - val_accuracy: 0.2581 - val_f1: 0.2295\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 1s 33ms/step - loss: 0.0062 - accuracy: 1.0000 - f1: 1.0000 - val_loss: 6.2041 - val_accuracy: 0.2581 - val_f1: 0.2295\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f68926e4b38>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, \n",
    "          validation_data=(X_valid, y_valid),\n",
    "          batch_size=32,\n",
    "          epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'f1', 'val_loss', 'val_accuracy', 'val_f1'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(model.history.history.keys())\n",
    "plt.plot(model.history.history['f1'])\n",
    "plt.plot(model.history.history['val_f1'])\n",
    "plt.title('model f1')\n",
    "plt.ylabel('f1')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель очень сильно переобучается. Она полностью выучивает обучающую выборку, но с предсказанием новых данных все очень плохо (разрыв в качестве с предобученной моделью огромный). Увеличение количества параметров не поможет - будет точно такая же картина. Для того, чтобы увеличить качество необходимо больше данных. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
