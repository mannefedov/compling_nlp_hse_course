{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6df96f2-ccde-4512-90b5-44ac854e0dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'nanoVLM'...\n",
      "remote: Enumerating objects: 500, done.\u001b[K\n",
      "remote: Counting objects: 100% (299/299), done.\u001b[K\n",
      "remote: Compressing objects: 100% (101/101), done.\u001b[K\n",
      "remote: Total 500 (delta 244), reused 203 (delta 198), pack-reused 201 (from 1)\u001b[K\n",
      "Receiving objects: 100% (500/500), 13.20 MiB | 20.50 MiB/s, done.\n",
      "Resolving deltas: 100% (311/311), done.\n",
      "/workspace/nanoVLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md  benchmark-inference.py  data\t\tmeasure_vram.py  nanoVLM.ipynb\n",
      "assets\t   benchmark_suite.py\t   generate.py\tmodels\t\t train.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isdir('nanoVLM'):\n",
    "    !git clone https://github.com/huggingface/nanoVLM.git\n",
    "%cd nanoVLM/\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dec4926d-3f87-4b97-840d-03807db5f3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.5.0.post1 requires fsspec==2025.5.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# If you get an \"Error\" from pip's dependency resolver but the cell complets fine, this is not an issue, you can continue :)\n",
    "!pip -q install torch\n",
    "!pip -q install gcsfs\n",
    "!pip -q install datasets==3.5.0\n",
    "!pip -q install tqdm\n",
    "!pip -q install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e54b349-15d8-4416-9fc5-28c8116ed174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install transformers\n",
    "!pip -q install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd8225b0-39fc-4cd2-a17e-805f798b8760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's authentificate with the Hugging Face Hub so you can push your model\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e368a610-50e0-4dbb-bf0c-205f1a84022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da6dc0f1-d87f-4b91-8f9f-7de349025a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide on the name of your model here!\n",
    "# You will need your HF user name and the name you want to give to it\n",
    "# For me, this would be \"lusxvr/nanoVLM\"\n",
    "hf_model_name = \"YOUR_HF_USER_NAME/nanoVLM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c4db628-bf1e-4147-8862-867056dc71c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/nanoVLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd nanoVLM/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f3161d8-7273-466f-9c73-a03c153440b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.collators import VQACollator, MMStarCollator\n",
    "from data.datasets import MMStarDataset, VQADataset\n",
    "from data.processors import get_image_processor, get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb715ac3-b733-4bb1-93f3-8875501179af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# nanoVLM Imports (please check out the implementations in detail, that's where all the interessting stuff is!)\n",
    "\n",
    "# from models.vision_language_model import VisionLanguageModel\n",
    "# import models.utils as utils\n",
    "\n",
    "# Libraries\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from PIL import Image\n",
    "#Otherwise, the tokenizer will through a warning\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from safetensors.torch import load_model, save_model\n",
    "\n",
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "from dataclasses import asdict\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "\n",
    "# To reload the modules if you change something in the code\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864f6455-4be1-4bd2-add3-2dd35a306d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3758f047-462a-4414-bb24-55b97aaeb8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_top_p_filtering(logits, top_k=0, top_p=1.0, filter_value=-float('Inf')):\n",
    "    \"\"\"\n",
    "    Apply top-k and/or nucleus (top-p) filtering to logits.\n",
    "    \"\"\"\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety\n",
    "\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the top-k tokens\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits = logits.masked_fill(indices_to_remove, filter_value)\n",
    "\n",
    "    if top_p < 1.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.softmax(sorted_logits, dim=-1).cumsum(dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above top_p\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "\n",
    "        # Always keep the first token\n",
    "        sorted_indices_to_remove[..., 0] = False\n",
    "        \n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "        logits = logits.masked_fill(indices_to_remove, filter_value)\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "339fc75a-9528-4e3a-b815-fbb6cb5f04b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.vision_transformer import ViT\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/siglip/modeling_siglip.py#L245\n",
    "class ViTPatchEmbeddings(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.img_size = cfg.vit_img_size\n",
    "        self.patch_size = cfg.vit_patch_size\n",
    "        self.num_patches = (self.img_size // self.patch_size) ** 2\n",
    "        self.cls_flag = cfg.vit_cls_flag\n",
    "        self.embd_dim = cfg.vit_hidden_dim\n",
    "\n",
    "        # Conv layer to extract the patches\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=self.embd_dim,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "            padding=\"valid\",\n",
    "        )\n",
    "\n",
    "        if self.cls_flag:\n",
    "            self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embd_dim))\n",
    "            self.position_embedding = nn.Parameter(torch.rand(1, self.num_patches + 1, self.embd_dim))\n",
    "        else:\n",
    "            self.position_embedding = nn.Parameter(torch.rand(1, self.num_patches, self.embd_dim))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)  # extract patches\n",
    "        x = x.flatten(2)  # flatten the patches into a single dimension\n",
    "        x = x.transpose(1, 2)  # transpose to (batch_size, num_patches, hidden_dim)\n",
    "\n",
    "        # Add CLS token (according to original ViT Paper) and position embeddings\n",
    "        if self.cls_flag:\n",
    "            cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "            x = torch.cat((cls_token, x), dim=1)\n",
    "        x = x + self.position_embedding\n",
    "        return x\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/siglip/modeling_siglip.py#L381\n",
    "# https://github.com/karpathy/nanoGPT/blob/master/model.py#L29\n",
    "class ViTMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = cfg.vit_n_heads\n",
    "        self.embd_dim = cfg.vit_hidden_dim\n",
    "        assert self.embd_dim % self.n_heads == 0, \"embd_dim must be divisible by num_heads\"\n",
    "        self.head_dim = self.embd_dim // self.n_heads\n",
    "        self.dropout = cfg.vit_dropout\n",
    "\n",
    "        # Combined projections for all heads\n",
    "        self.qkv_proj = nn.Linear(self.embd_dim, 3 * self.embd_dim, bias=True)\n",
    "        self.out_proj = nn.Linear(self.embd_dim, self.embd_dim, bias=True)\n",
    "\n",
    "        # Dropout layers\n",
    "        self.attn_dropout = nn.Dropout(self.dropout)\n",
    "        self.resid_dropout = nn.Dropout(self.dropout)\n",
    "\n",
    "        # Use scaled dot product attention if available\n",
    "        self.sdpa = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.sdpa:\n",
    "            print(\"Warning: scaled dot product attention not available. Using standard attention in ViT.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        qkv = self.qkv_proj(x)\n",
    "        q, k, v = qkv.split(C, dim=2)\n",
    "        # Reshape  [B, T, C] -> [B, T, n_heads, head_dim] and transpose -> [B, n_heads, T, head_dim]\n",
    "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)  # (B, n_heads, T, head_dim)\n",
    "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)  # (B, n_heads, T, head_dim)\n",
    "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)  # (B, n_heads, T, head_dim)\n",
    "\n",
    "        if self.sdpa:\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(\n",
    "                q, k, v, \n",
    "                attn_mask=None,\n",
    "                dropout_p=self.dropout if self.training else 0.0,\n",
    "                is_causal=False # ViT attention is bidirectional\n",
    "            )\n",
    "        else:\n",
    "            attn = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            attn = F.softmax(attn, dim=-1)\n",
    "            attn = self.attn_dropout(attn)\n",
    "            y = attn @ v  # (B, n_heads, T, T) x (B, n_heads, T, head_dim) -> (B, n_heads, T, head_dim)\n",
    "        \n",
    "        # Transpose back from [B, n_heads, T, head_dim] to [B, T, n_heads * head_dim] and combine all heads to [B, T, C]\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)  \n",
    "        y = self.out_proj(y)\n",
    "        y = self.resid_dropout(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/siglip/modeling_siglip.py#L453\n",
    "class ViTMLP(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.activation_fn = nn.GELU(approximate='tanh')\n",
    "        self.fc1 = nn.Linear(cfg.vit_hidden_dim, cfg.vit_inter_dim)\n",
    "        self.fc2 = nn.Linear(cfg.vit_inter_dim, cfg.vit_hidden_dim)\n",
    "        self.dropout = nn.Dropout(cfg.vit_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation_fn(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# https://github.com/karpathy/nanoGPT/blob/master/model.py#L94    \n",
    "class ViTBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(cfg.vit_hidden_dim, eps=cfg.vit_ln_eps)\n",
    "        self.attn = ViTMultiHeadAttention(cfg)\n",
    "        self.ln2 = nn.LayerNorm(cfg.vit_hidden_dim, eps=cfg.vit_ln_eps)\n",
    "        self.mlp = ViTMLP(cfg)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.patch_embedding = ViTPatchEmbeddings(cfg)\n",
    "        self.cls_flag = cfg.vit_cls_flag\n",
    "        self.dropout = nn.Dropout(cfg.vit_dropout)\n",
    "        self.blocks = nn.ModuleList([ViTBlock(cfg) for _ in range(cfg.vit_n_blocks)])\n",
    "        self.layer_norm = nn.LayerNorm(cfg.vit_hidden_dim, eps=cfg.vit_ln_eps)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        elif isinstance(module, nn.Conv2d):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x) \n",
    "        x = self.dropout(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        if self.cls_flag:\n",
    "            x = self.layer_norm(x[:, 0])\n",
    "        else:\n",
    "            x = self.layer_norm(x)\n",
    "            #x = x.mean(dim=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    # Load the model from a pretrained HuggingFace model (we don't want to have to train the Vision Backbone from scratch)\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, cfg):\n",
    "        from transformers import SiglipVisionConfig\n",
    "        from huggingface_hub import hf_hub_download\n",
    "        import safetensors\n",
    "\n",
    "        hf_config = SiglipVisionConfig.from_pretrained(cfg.vit_model_type)\n",
    "        cfg.vit_dropout=hf_config.attention_dropout\n",
    "        cfg.vit_hidden_dim=hf_config.hidden_size\n",
    "        cfg.vit_img_size=hf_config.image_size\n",
    "        cfg.vit_inter_dim=hf_config.intermediate_size\n",
    "        cfg.vit_ln_eps=hf_config.layer_norm_eps\n",
    "        cfg.vit_n_heads=hf_config.num_attention_heads\n",
    "        cfg.vit_n_blocks=hf_config.num_hidden_layers\n",
    "        cfg.vit_patch_size=hf_config.patch_size\n",
    "        model = cls(cfg)\n",
    "        safetensors_file = hf_hub_download(repo_id=cfg.vit_model_type, filename=\"model.safetensors\")\n",
    "\n",
    "        sd = model.state_dict()\n",
    "        \n",
    "        mapping = {\n",
    "            'vision_model.embeddings.patch_embedding.weight': 'patch_embedding.conv.weight',\n",
    "            'vision_model.embeddings.patch_embedding.bias': 'patch_embedding.conv.bias',\n",
    "            'vision_model.embeddings.position_embedding.weight': 'patch_embedding.position_embedding',\n",
    "            'vision_model.post_layernorm.weight': 'layer_norm.weight',\n",
    "            'vision_model.post_layernorm.bias': 'layer_norm.bias',\n",
    "        }\n",
    "        \n",
    "        for i in range(cfg.vit_n_blocks):\n",
    "            # Layer norms\n",
    "            mapping[f'vision_model.encoder.layers.{i}.layer_norm1.weight'] = f'blocks.{i}.ln1.weight'\n",
    "            mapping[f'vision_model.encoder.layers.{i}.layer_norm1.bias'] = f'blocks.{i}.ln1.bias'\n",
    "            mapping[f'vision_model.encoder.layers.{i}.layer_norm2.weight'] = f'blocks.{i}.ln2.weight'\n",
    "            mapping[f'vision_model.encoder.layers.{i}.layer_norm2.bias'] = f'blocks.{i}.ln2.bias'\n",
    "            \n",
    "            # MLP\n",
    "            mapping[f'vision_model.encoder.layers.{i}.mlp.fc1.weight'] = f'blocks.{i}.mlp.fc1.weight'\n",
    "            mapping[f'vision_model.encoder.layers.{i}.mlp.fc1.bias'] = f'blocks.{i}.mlp.fc1.bias'\n",
    "            mapping[f'vision_model.encoder.layers.{i}.mlp.fc2.weight'] = f'blocks.{i}.mlp.fc2.weight'\n",
    "            mapping[f'vision_model.encoder.layers.{i}.mlp.fc2.bias'] = f'blocks.{i}.mlp.fc2.bias'\n",
    "            \n",
    "            # Output projection\n",
    "            mapping[f'vision_model.encoder.layers.{i}.self_attn.out_proj.weight'] = f'blocks.{i}.attn.out_proj.weight'\n",
    "            mapping[f'vision_model.encoder.layers.{i}.self_attn.out_proj.bias'] = f'blocks.{i}.attn.out_proj.bias'\n",
    "        \n",
    "        with safetensors.safe_open(filename=safetensors_file, framework=\"pt\", device=\"cpu\") as f:\n",
    "            for hf_key, our_key in mapping.items():\n",
    "                if hf_key in f.keys() and our_key in sd:\n",
    "                    tensor = f.get_tensor(hf_key)\n",
    "                    if tensor.shape == sd[our_key].shape:\n",
    "                        sd[our_key].copy_(tensor)\n",
    "                    else:\n",
    "                        if 'position_embedding' in hf_key:\n",
    "                            sd[our_key].copy_(tensor.unsqueeze(0))\n",
    "                        else:\n",
    "                            print(f\"Shape mismatch for {hf_key} -> {our_key}: {tensor.shape} vs {sd[our_key].shape}\")\n",
    "                else:\n",
    "                    if hf_key not in f.keys():\n",
    "                        print(f\"Warning: Key {hf_key} not found in safetensors file\")\n",
    "                    if our_key not in sd:\n",
    "                        print(f\"Warning: Key {our_key} not found in model state dict\")\n",
    "            \n",
    "            # Manually handle QKV concatenation since our implementation combines Q, K, V into one\n",
    "            for i in range(model.cfg.vit_n_blocks):\n",
    "                q_weight = f.get_tensor(f'vision_model.encoder.layers.{i}.self_attn.q_proj.weight')\n",
    "                k_weight = f.get_tensor(f'vision_model.encoder.layers.{i}.self_attn.k_proj.weight')\n",
    "                v_weight = f.get_tensor(f'vision_model.encoder.layers.{i}.self_attn.v_proj.weight')\n",
    "                \n",
    "                qkv_weight = torch.cat((q_weight, k_weight, v_weight), dim=0)\n",
    "                sd[f'blocks.{i}.attn.qkv_proj.weight'].copy_(qkv_weight)\n",
    "                \n",
    "                q_bias = f.get_tensor(f'vision_model.encoder.layers.{i}.self_attn.q_proj.bias')\n",
    "                k_bias = f.get_tensor(f'vision_model.encoder.layers.{i}.self_attn.k_proj.bias')\n",
    "                v_bias = f.get_tensor(f'vision_model.encoder.layers.{i}.self_attn.v_proj.bias')\n",
    "                \n",
    "                qkv_bias = torch.cat((q_bias, k_bias, v_bias), dim=0)\n",
    "                sd[f'blocks.{i}.attn.qkv_proj.bias'].copy_(qkv_bias)\n",
    "        \n",
    "        model.load_state_dict(sd)\n",
    "        print(f\"Successfully loaded {cfg.vit_model_type} weights from safetensors. Model has {sum(p.numel() for p in model.parameters()):,} parameters.\")\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52168258-8626-4e27-b11b-755e7aae0e35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6d00a51-fc10-4fe8-b089-9606f0c8bd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.language_model import LanguageModel\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L69\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(cfg.lm_hidden_dim))\n",
    "        self.eps = cfg.lm_rms_eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        irms = torch.rsqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps) # inverse of RMS\n",
    "        x = x * irms * self.weight\n",
    "\n",
    "        return x\n",
    "\n",
    "# Multiple derivates of Rotary Embeddings by now, this is a basic one with linear scaling to context length\n",
    "# e.g. https://github.com/huggingface/smollm/blob/main/vision/m4/models/vllama3/modeling_vllama3.py#L190\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        assert cfg.lm_hidden_dim % cfg.lm_n_heads == 0, \"Hidden dimension must be divisible by number of heads\"\n",
    "        \n",
    "        self.dim = cfg.lm_hidden_dim // cfg.lm_n_heads # dim of each head\n",
    "        self.base = cfg.lm_re_base\n",
    "        self.max_seq_len = cfg.lm_max_position_embeddings\n",
    "        # Standard RoPE implementation - create frequencies for each dimension\n",
    "        # freq_i = 1 / (base^(2i/dim)) where i is the dimension index\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float() / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self.original_max_seq_len = cfg.lm_max_position_embeddings\n",
    "        self.attention_scaling = cfg.lm_attn_scaling\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, position_ids):\n",
    "        batch_size, seq_len = position_ids.shape\n",
    "        # Dynamic scaling for longer sequences\n",
    "        max_seq = position_ids.max() + 1\n",
    "        if max_seq > self.original_max_seq_len:\n",
    "            scale = max_seq / self.original_max_seq_len\n",
    "            inv_freq = self.inv_freq / scale\n",
    "        else:\n",
    "            inv_freq = self.inv_freq\n",
    "            \n",
    "        # Compute theta = position * frequency\n",
    "        # Flatten position_ids for batch processing\n",
    "        flat_position_ids = position_ids.reshape(-1).float()\n",
    "        \n",
    "        # Element-wise outer product: [seq_len] x [dim/2] => [seq_len, dim/2]\n",
    "        freqs = flat_position_ids.unsqueeze(-1) * inv_freq.unsqueeze(0)\n",
    "        \n",
    "        # Reshape to include batch dimension\n",
    "        freqs = freqs.reshape(batch_size, seq_len, -1)\n",
    "        \n",
    "        # Now create interleaved pattern\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)\n",
    "        \n",
    "        # Compute cos and sin\n",
    "        cos = torch.cos(emb) * self.attention_scaling\n",
    "        sin = torch.sin(emb) * self.attention_scaling\n",
    "        \n",
    "        return cos, sin\n",
    "\n",
    "# Rotates half the hidden dims of the input by swapping and negating dimensions.\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "# Apply rotary position embeddings to queries and keys.\n",
    "def apply_rotary_pos_embd(q, k, cos, sin, unsqueeze_dim=1):\n",
    "    # We need to make sure cos and sin can be properly broadcast\n",
    "    # to the shape of q and k by adding the heads dimension\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)  # [batch_size, 1, seq_len, head_dim]\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)  # [batch_size, 1, seq_len, head_dim]\n",
    "    \n",
    "    # Apply complex multiplication:\n",
    "    # (q * cos) + (rotate_half(q) * sin)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    \n",
    "    return q_embed, k_embed\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L214\n",
    "# https://github.com/huggingface/smollm/blob/main/vision/m4/models/vllama3/modeling_vllama3.py#L382\n",
    "class LanguageModelGroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = cfg.lm_n_heads\n",
    "        self.n_kv_heads = cfg.lm_n_kv_heads\n",
    "        self.embd_dim = cfg.lm_hidden_dim\n",
    "        self.dropout = cfg.lm_dropout\n",
    "\n",
    "        assert self.n_heads % self.n_kv_heads == 0, \"n_heads must be divisible by n_kv_heads\"\n",
    "        assert self.embd_dim % self.n_heads == 0, \"embd_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.n_kv_groups = self.n_heads // self.n_kv_heads\n",
    "        self.head_dim = self.embd_dim // self.n_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(self.embd_dim, self.embd_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.embd_dim, self.head_dim * self.n_kv_heads, bias=False)\n",
    "        self.v_proj = nn.Linear(self.embd_dim, self.head_dim * self.n_kv_heads, bias=False)\n",
    "        self.out_proj = nn.Linear(self.embd_dim, self.embd_dim, bias=False)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(self.dropout)\n",
    "        self.resid_dropout = nn.Dropout(self.dropout)\n",
    "\n",
    "        # Use scaled dot product attention if available\n",
    "        self.sdpa = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.sdpa:\n",
    "            print(\"Warning: scaled dot product attention not available, using standard attention in LM.\")\n",
    "\n",
    "    def forward(self, x, cos, sin, attention_mask=None):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        q = self.q_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)  # (B, n_heads, T, head_dim)\n",
    "        k = self.k_proj(x).view(B, T, self.n_kv_heads, self.head_dim).transpose(1, 2)  # (B, n_kv_heads, T, head_dim)\n",
    "        v = self.v_proj(x).view(B, T, self.n_kv_heads, self.head_dim).transpose(1, 2)  # (B, n_kv_heads, T, head_dim)\n",
    "        \n",
    "        # Use precomputed positional embeddings\n",
    "        q, k = apply_rotary_pos_embd(q, k, cos, sin)\n",
    "\n",
    "        k = k.repeat_interleave(self.n_kv_groups, dim=1)\n",
    "        v = v.repeat_interleave(self.n_kv_groups, dim=1)\n",
    "\n",
    "        # Process attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            # Create a 4D attention mask [batch_size, 1, 1, seq_length], In this format, 1 = attend, 0 = mask\n",
    "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)  # [B, 1, 1, T]\n",
    "            padding_mask = (attention_mask == 0).transpose(-1, -2) # Use this for the manual path\n",
    "            # Convert to attention mask where 0 keeps values and -inf masks\n",
    "            attention_mask = (1.0 - attention_mask) * torch.finfo(q.dtype).min\n",
    "\n",
    "        if self.sdpa:\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(\n",
    "                q, k, v,\n",
    "                attn_mask=attention_mask,\n",
    "                dropout_p=self.dropout if self.training else 0.0,\n",
    "                is_causal=True # LM attention is causal (masked)\n",
    "            )\n",
    "        else:\n",
    "            attn = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "            causal_mask = torch.tril(torch.ones(T, T, device=x.device)).view(1, 1, T, T)\n",
    "            attn = attn.masked_fill(causal_mask == 0, float('-inf'))\n",
    "            if attention_mask is not None:\n",
    "                attn = attn + attention_mask \n",
    "\n",
    "            attn = F.softmax(attn, dim=-1)\n",
    "            attn = self.attn_dropout(attn)\n",
    "            y = attn @ v\n",
    "            \n",
    "            if attention_mask is not None:\n",
    "                y = y.masked_fill(padding_mask, 0.0) # Zero out the padded positions in the output\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)  \n",
    "        y = self.out_proj(y)\n",
    "        y = self.resid_dropout(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L160\n",
    "class LanguageModelMLP(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.embd_dim = cfg.lm_hidden_dim\n",
    "        self.inter_dim = cfg.lm_inter_dim\n",
    "\n",
    "        self.activation_fn = F.silu\n",
    "        self.gate_proj = nn.Linear(self.embd_dim, self.inter_dim, bias=False)\n",
    "        self.up_proj = nn.Linear(self.embd_dim, self.inter_dim, bias=False)\n",
    "        self.down_proj = nn.Linear(self.inter_dim, self.embd_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = self.activation_fn(self.gate_proj(x))\n",
    "        x = self.up_proj(x)\n",
    "        x = self.down_proj(gate * x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# https://github.com/meta-llama/llama3/blob/main/llama/model.py#L222\n",
    "class LanguageModelBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.mlp = LanguageModelMLP(cfg)\n",
    "        self.attn = LanguageModelGroupedQueryAttention(cfg)\n",
    "        self.norm1 = RMSNorm(cfg) # Input Norm\n",
    "        self.norm2 = RMSNorm(cfg) # Post Attention Norm\n",
    "    \n",
    "    def forward(self, x, cos, sin, attention_mask=None):\n",
    "        res = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x, cos, sin, attention_mask)\n",
    "        x = res + x\n",
    "\n",
    "        res = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = res + x\n",
    "\n",
    "        return x\n",
    "\n",
    "# https://github.com/meta-llama/llama3/blob/main/llama/model.py#L251\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.lm_use_tokens = cfg.lm_use_tokens\n",
    "        self.lm_tie_weights = cfg.lm_tie_weights\n",
    "\n",
    "        self.token_embedding = nn.Embedding(cfg.lm_vocab_size, cfg.lm_hidden_dim)\n",
    "        self.rotary_embd = RotaryEmbedding(cfg)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            LanguageModelBlock(cfg) for _ in range(cfg.lm_n_blocks)\n",
    "        ])\n",
    "        self.norm = RMSNorm(cfg) # Final Norm\n",
    "        self.head = nn.Linear(cfg.lm_hidden_dim, cfg.lm_vocab_size, bias=False)\n",
    "        if self.lm_tie_weights:\n",
    "            self.head.weight = self.token_embedding.weight\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, RMSNorm):\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        if self.lm_use_tokens:\n",
    "            x = self.token_embedding(x) # Only embed the inputs when using tokens\n",
    "        \n",
    "        B , T, _ = x.size()\n",
    "        \n",
    "        # Note: You could also cache these input embeddings if you want to avoid recomputing them\n",
    "        position_ids = torch.arange(T, device=x.device).unsqueeze(0).expand(B, -1) # Create position ids [0, 1, 2, ..., seq_len-1]\n",
    "        cos, sin = self.rotary_embd(position_ids) # Get rotary position embeddings\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, cos, sin, attention_mask)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if self.lm_use_tokens:\n",
    "            x = self.head(x) # Compute logits if we are using tokens, otherwise stay in the embedding space\n",
    "\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, inputs, max_new_tokens=20):\n",
    "        # Add batch dimension if needed\n",
    "        if inputs.dim() == 1:\n",
    "            inputs = inputs.unsqueeze(0)\n",
    "            \n",
    "        generated = inputs.clone()\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # Forward pass through the model\n",
    "            outputs = self.forward(generated)\n",
    "            last_output = outputs[:, -1, :]\n",
    "\n",
    "            if self.lm_use_tokens:\n",
    "                # Now the model outputs logits\n",
    "                next_token = torch.argmax(last_output, dim=-1, keepdim=True)\n",
    "                generated = torch.cat((generated, next_token), dim=-1)\n",
    "            else:\n",
    "                # Now the model outputs embeddings\n",
    "                next_token_embedding = last_output.unsqueeze(1)  # Shape: [batch_size, 1, hidden_dim]\n",
    "                generated = torch.cat((generated, next_token_embedding), dim=1)\n",
    "            \n",
    "            #Note: You could enable the generation to break earlier than max_new_tokens when it detects a eos token, but this does not work in batched generation (output tensors need to have the same size)\n",
    "    \n",
    "        return generated\n",
    "\n",
    "    # Load the model from a pretrained HuggingFace model (we don't want to have to train the Language Backbone from scratch)\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, cfg):\n",
    "        from transformers import AutoConfig\n",
    "        from huggingface_hub import hf_hub_download\n",
    "        import safetensors\n",
    "        import torch.nn.init as init\n",
    "                \n",
    "        # Load the HuggingFace config\n",
    "        hf_config = AutoConfig.from_pretrained(cfg.lm_model_type)\n",
    "        \n",
    "        # Store original HF vocab size before we modify it\n",
    "        original_vocab_size = hf_config.vocab_size\n",
    "        # print(f\"Original vocabulary size from pretrained model: {original_vocab_size}\")\n",
    "        \n",
    "        # Configure model parameters from HF config\n",
    "        cfg.lm_hidden_dim = hf_config.hidden_size\n",
    "        cfg.lm_inter_dim = hf_config.intermediate_size\n",
    "        cfg.lm_rms_eps = hf_config.rms_norm_eps\n",
    "        cfg.lm_re_base = hf_config.rope_theta\n",
    "        cfg.lm_max_position_embeddings = hf_config.max_position_embeddings\n",
    "        # We're keeping our own vocab size in cfg, but checking it's larger than original\n",
    "        if hasattr(cfg, 'lm_vocab_size'):\n",
    "            if cfg.lm_vocab_size < original_vocab_size:\n",
    "                raise ValueError(f\"Config vocab size ({cfg.lm_vocab_size}) is smaller than pretrained model vocab size ({original_vocab_size})\")\n",
    "            # print(f\"Using vocabulary size: {cfg.lm_vocab_size}\")\n",
    "        else:\n",
    "            # If not specified, use the original\n",
    "            cfg.lm_vocab_size = original_vocab_size\n",
    "            # print(f\"Using original vocabulary size: {cfg.lm_vocab_size}\")\n",
    "        \n",
    "        cfg.lm_n_heads = hf_config.num_attention_heads\n",
    "        cfg.lm_n_kv_heads = hf_config.num_key_value_heads\n",
    "        cfg.lm_dropout = hf_config.attention_dropout\n",
    "        cfg.lm_n_blocks = hf_config.num_hidden_layers\n",
    "        \n",
    "        # Create our model with potentially larger vocabulary\n",
    "        model = cls(cfg)\n",
    "        safetensors_file = hf_hub_download(repo_id=cfg.lm_model_type, filename=\"model.safetensors\")\n",
    "        \n",
    "        sd = model.state_dict()\n",
    "        \n",
    "        mapping = {\n",
    "            'model.embed_tokens.weight': 'token_embedding.weight',\n",
    "            'model.norm.weight': 'norm.weight'\n",
    "        }\n",
    "        \n",
    "        for i in range(cfg.lm_n_blocks):\n",
    "            layer_prefix = f'model.layers.{i}.'\n",
    "            block_prefix = f'blocks.{i}.'\n",
    "            \n",
    "            mapping.update({\n",
    "                f\"{layer_prefix}self_attn.q_proj.weight\": f\"{block_prefix}attn.q_proj.weight\",\n",
    "                f\"{layer_prefix}self_attn.k_proj.weight\": f\"{block_prefix}attn.k_proj.weight\",\n",
    "                f\"{layer_prefix}self_attn.v_proj.weight\": f\"{block_prefix}attn.v_proj.weight\",\n",
    "                f\"{layer_prefix}self_attn.o_proj.weight\": f\"{block_prefix}attn.out_proj.weight\",\n",
    "                f\"{layer_prefix}mlp.gate_proj.weight\": f\"{block_prefix}mlp.gate_proj.weight\",\n",
    "                f\"{layer_prefix}mlp.up_proj.weight\": f\"{block_prefix}mlp.up_proj.weight\",\n",
    "                f\"{layer_prefix}mlp.down_proj.weight\": f\"{block_prefix}mlp.down_proj.weight\",\n",
    "                f\"{layer_prefix}input_layernorm.weight\": f\"{block_prefix}norm1.weight\",\n",
    "                f\"{layer_prefix}post_attention_layernorm.weight\": f\"{block_prefix}norm2.weight\"\n",
    "            })\n",
    "        \n",
    "        # Special handling for token embeddings with extended vocabulary\n",
    "        has_extended_embeddings = False\n",
    "        with safetensors.safe_open(filename=safetensors_file, framework=\"pt\", device=\"cpu\") as f:\n",
    "            for hf_key, our_key in mapping.items():\n",
    "                if hf_key in f.keys() and our_key in sd:\n",
    "                    tensor = f.get_tensor(hf_key)\n",
    "                    \n",
    "                    # Special handling for token embeddings if vocab sizes differ\n",
    "                    if hf_key == 'model.embed_tokens.weight' and tensor.shape[0] != sd[our_key].shape[0]:\n",
    "                        has_extended_embeddings = True\n",
    "                        print(f\"Extending token embeddings from {tensor.shape} to {sd[our_key].shape}\")\n",
    "                        \n",
    "                        # Copy existing embeddings to the beginning of our larger embedding matrix\n",
    "                        sd[our_key][:tensor.shape[0]].copy_(tensor)\n",
    "                        \n",
    "                        # Initialize the new embeddings using the same approach as the original model\n",
    "                        std = 0.02  # Common value, but you might want to adjust based on model\n",
    "                        init.normal_(sd[our_key][tensor.shape[0]:], mean=0.0, std=std)\n",
    "                        \n",
    "                        print(f\"Initialized {sd[our_key].shape[0] - tensor.shape[0]} new token embeddings\")\n",
    "                        sd['head.weight'].copy_(sd[our_key])  # Update the head weights as well\n",
    "                    elif tensor.shape == sd[our_key].shape:\n",
    "                        sd[our_key].copy_(tensor)\n",
    "                    else:\n",
    "                        print(f\"Shape mismatch for {hf_key} -> {our_key}: {tensor.shape} vs {sd[our_key].shape}\")\n",
    "                else:\n",
    "                    if hf_key not in f.keys():\n",
    "                        print(f\"Warning: Key {hf_key} not found in safetensors file\")\n",
    "                    if our_key not in sd:\n",
    "                        print(f\"Warning: Key {our_key} not found in model state dict\")\n",
    "        \n",
    "        # Load the state dict\n",
    "        model.load_state_dict(sd)\n",
    "        \n",
    "        # Handle output projection / language modeling head\n",
    "        if has_extended_embeddings and hasattr(model, 'head') and 'head.weight' in sd:\n",
    "            # If we have a separate output projection layer and extended the vocab\n",
    "            # we should handle it similarly to the input embeddings\n",
    "            with safetensors.safe_open(filename=safetensors_file, framework=\"pt\", device=\"cpu\") as f:\n",
    "                if 'lm_head.weight' in f.keys():\n",
    "                    lm_head = f.get_tensor('lm_head.weight')\n",
    "                    if lm_head.shape[0] != sd['head.weight'].shape[0]:\n",
    "                        print(f\"Extending LM head from {lm_head.shape} to {sd['head.weight'].shape}\")\n",
    "                        # Copy existing weights\n",
    "                        sd['head.weight'][:lm_head.shape[0]].copy_(lm_head)\n",
    "                        # Initialize new weights\n",
    "                        std = 0.02\n",
    "                        init.normal_(sd['head.weight'][lm_head.shape[0]:], mean=0.0, std=std)\n",
    "                        # Load updated weights\n",
    "                        model.load_state_dict(sd)\n",
    "        \n",
    "        # Handle weight tying (if needed)\n",
    "        if cfg.lm_tie_weights and hasattr(model, 'head') and hasattr(model, 'token_embedding'):\n",
    "            model.head.weight = model.token_embedding.weight\n",
    "            # print(\"Tied token embedding and LM head weights\")\n",
    "        \n",
    "        print(f\"Successfully loaded {cfg.lm_model_type} weights from safetensors. Model has {sum(p.numel() for p in model.parameters()):,} parameters.\")\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4604cb48-251d-4062-9153-9942b386ec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.modality_projector import ModalityProjector\n",
    "# Modality Projection from Vision to Language\n",
    "import torch.nn as nn\n",
    "\n",
    "class ModalityProjector(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.input_dim = cfg.vit_hidden_dim * (cfg.mp_pixel_shuffle_factor**2)\n",
    "        self.output_dim = cfg.lm_hidden_dim\n",
    "        self.scale_factor = cfg.mp_pixel_shuffle_factor\n",
    "\n",
    "        self.proj = nn.Linear(self.input_dim, self.output_dim, bias=False)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(self.proj.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "    # https://github.com/huggingface/smollm/blob/main/vision/m4/models/vllama3/modeling_vllama3.py#L1281\n",
    "    def pixel_shuffle(self, x):\n",
    "        bsz, seq, embed_dim = x.size()\n",
    "        seq_root = int(seq**0.5)\n",
    "        assert seq_root**2 == seq # Sequence length must be a perfect square for pixel shuffle\n",
    "        assert seq_root % self.scale_factor == 0 # Sequence root must be divisible by scale factor\n",
    "\n",
    "        height = width = seq_root\n",
    "        x = x.view(bsz, height, width, embed_dim)\n",
    "        h_out = height // self.scale_factor\n",
    "        w_out = width // self.scale_factor\n",
    "        \n",
    "        x = x.reshape(bsz, h_out, self.scale_factor, w_out, self.scale_factor, embed_dim)\n",
    "        x = x.permute(0, 1, 3, 2, 4, 5).contiguous()\n",
    "        x = x.reshape(bsz, h_out * w_out, embed_dim * self.scale_factor**2)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pixel_shuffle(x)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50d24a24-ed90-46dc-933a-9592c994eb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VisionLanguageModel(nn.Module):\n",
    "    def __init__(self, cfg, load_backbone=True):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if load_backbone:\n",
    "            print(\"Loading from backbone weights\")\n",
    "            self.vision_encoder = ViT.from_pretrained(cfg)\n",
    "            self.decoder = LanguageModel.from_pretrained(cfg)\n",
    "        else:\n",
    "            self.vision_encoder = ViT(cfg)\n",
    "            self.decoder = LanguageModel(cfg)\n",
    "        self.MP = ModalityProjector(cfg)\n",
    "        self.load_backbone = load_backbone\n",
    "\n",
    "    def forward(self, input_ids, image, attention_mask=None, targets=None):\n",
    "        image_embd = self.vision_encoder(image)\n",
    "        image_embd = self.MP(image_embd)\n",
    "\n",
    "        token_embd = self.decoder.token_embedding(input_ids)\n",
    "\n",
    "        combined_embd = torch.cat((image_embd, token_embd), dim=1) # Concatenate image embeddings to token embeddings\n",
    "        \n",
    "        # Adjust attention mask to account for image tokens\n",
    "        if attention_mask is not None:\n",
    "            # Create mask of 1s for image tokens (all image tokens should be attended to)\n",
    "            batch_size = image_embd.size(0)\n",
    "            img_seq_len = image_embd.size(1)\n",
    "            image_attention_mask = torch.ones((batch_size, img_seq_len), device=attention_mask.device, dtype=attention_mask.dtype)\n",
    "            \n",
    "            # Combine image and token attention masks\n",
    "            attention_mask = torch.cat((image_attention_mask, attention_mask), dim=1)\n",
    "\n",
    "        logits = self.decoder(combined_embd, attention_mask) # Not logits yet, but easier to return like this\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            # Only use the token part of the logits for loss computation\n",
    "            logits = self.decoder.head(logits)\n",
    "            logits = logits[:, image_embd.size(1):, :]\n",
    "            loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.reshape(-1), ignore_index=-100)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, input_ids, image, attention_mask=None, max_new_tokens=5, top_k=50, top_p=0.9, temperature=0.5, greedy=False):\n",
    "        # Process image through vision encoder and projection\n",
    "        image_embd = self.vision_encoder(image)\n",
    "        image_embd = self.MP(image_embd)\n",
    "        \n",
    "        # Embed initial tokens\n",
    "        token_embd = self.decoder.token_embedding(input_ids)\n",
    "        \n",
    "        # Concatenate image embeddings with token embeddings\n",
    "        combined_embd = torch.cat((image_embd, token_embd), dim=1)\n",
    "\n",
    "        batch_size = image_embd.size(0)\n",
    "        img_seq_len = image_embd.size(1)\n",
    "        # Adjust attention mask to account for image tokens\n",
    "        if attention_mask is not None:\n",
    "            # Create mask of 1s for image tokens (all image tokens should be attended to)\n",
    "            image_attention_mask = torch.ones((batch_size, img_seq_len), device=attention_mask.device, dtype=attention_mask.dtype)\n",
    "            attention_mask = torch.cat((image_attention_mask, attention_mask), dim=1)\n",
    "        \n",
    "        # Generate from combined embeddings using the decoder\n",
    "        # We need to use the decoder's forward function and not its generate method\n",
    "        # because we want to keep track of the image prefix\n",
    "        outputs = combined_embd\n",
    "        generated_tokens = torch.zeros((batch_size, max_new_tokens), device=input_ids.device, dtype=input_ids.dtype)\n",
    "        \n",
    "        #Note: Here you could implement improvements like e.g. KV caching\n",
    "        for i in range(max_new_tokens):\n",
    "            model_out = self.decoder(outputs, attention_mask)\n",
    "            \n",
    "            # Get predictions for the last token only (normally this is the embedding, not the logits)\n",
    "            last_token_logits = model_out[:, -1, :]\n",
    "            \n",
    "            # Apply head to get logits (if model is in embedding mode)\n",
    "            if not self.decoder.lm_use_tokens:\n",
    "                last_token_logits = self.decoder.head(last_token_logits)\n",
    "            if greedy:\n",
    "                next_token = torch.argmax(last_token_logits, dim=-1, keepdim=True)\n",
    "            else:\n",
    "                filtered_logits = top_k_top_p_filtering(last_token_logits, top_k=top_k, top_p=top_p)\n",
    "                probs = torch.softmax(filtered_logits/temperature, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                \n",
    "            generated_tokens[:, i] = next_token.squeeze(-1)\n",
    "            \n",
    "            # Convert to embedding and append\n",
    "            next_embd = self.decoder.token_embedding(next_token)\n",
    "            outputs = torch.cat((outputs, next_embd), dim=1)\n",
    "\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = torch.cat((attention_mask, torch.ones((batch_size, 1), device=attention_mask.device)), dim=1)\n",
    "        \n",
    "        return generated_tokens\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls, repo_id_or_path: str, *, revision: Optional[str] = None\n",
    "    ) -> \"VisionLanguageModel\":\n",
    "        \"\"\"\n",
    "        Load a VisionLanguageModel from a local directory or a repo on the Hugging Face Hub.\n",
    "\n",
    "        Args:\n",
    "            repo_id_or_path (str): The path to the local directory or the Hugging Face Hub repo ID.\n",
    "\n",
    "        Returns:\n",
    "            VisionLanguageModel: The loaded model.\n",
    "        \"\"\"\n",
    "        # If local folder exists => load from there\n",
    "        if os.path.exists(repo_id_or_path):\n",
    "            config_path = os.path.join(repo_id_or_path, \"config.json\")\n",
    "            weights_path = os.path.join(repo_id_or_path, \"model.safetensors\")\n",
    "\n",
    "            if not os.path.exists(config_path):\n",
    "                raise ValueError(\n",
    "                    f\"Config file not found at {config_path}. Please provide a valid path.\"\n",
    "                )\n",
    "            if not os.path.exists(weights_path):\n",
    "                raise ValueError(\n",
    "                    f\"Weights file not found at {weights_path}. Please provide a valid path.\"\n",
    "                )\n",
    "        # Otherwise, assume it's a Hugging Face Hub repo\n",
    "        else:\n",
    "            from huggingface_hub import hf_hub_download\n",
    "\n",
    "            config_path = hf_hub_download(\n",
    "                repo_id=repo_id_or_path, filename=\"config.json\", revision=revision\n",
    "            )\n",
    "            weights_path = hf_hub_download(\n",
    "                repo_id=repo_id_or_path, filename=\"model.safetensors\", revision=revision\n",
    "            )\n",
    "\n",
    "        # Load config\n",
    "        with open(config_path, \"r\") as f:\n",
    "            cfg = VLMConfig(**json.load(f))\n",
    "\n",
    "        # Initialize model without loading the backbone\n",
    "        model = cls(cfg, load_backbone=False)\n",
    "\n",
    "        # Load safetensors weights\n",
    "        load_model(model, weights_path)\n",
    "\n",
    "        # Done!\n",
    "        return model\n",
    "\n",
    "    def save_pretrained(self, save_directory: str) -> None:\n",
    "        \"\"\"\n",
    "        Save the model and configuration to a directory.\n",
    "\n",
    "        Args:\n",
    "            save_directory (str): The directory to save the model and config.\n",
    "        \"\"\"\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "        # Save config\n",
    "        with open(os.path.join(save_directory, \"config.json\"), \"w\") as f:\n",
    "            f.write(json.dumps(asdict(self.cfg), indent=4))\n",
    "\n",
    "        # Save weights as safetensors\n",
    "        save_model(self, os.path.join(save_directory, \"model.safetensors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4d9e09-fccf-4462-8b9e-218fa165e35a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0046994-16fd-460e-b933-06a40d98fa77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f33ef9-25e4-4831-a786-a1af612e40ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6836a994-a4ad-45c7-9146-bb7e790c05ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09f374e-e835-4994-b746-f802ac373791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0efe5e27-c13d-449b-b054-803088e22d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(train_cfg, vlm_cfg):\n",
    "    # Create datasets\n",
    "    image_processor = get_image_processor(vlm_cfg.vit_img_size)\n",
    "    tokenizer = get_tokenizer(vlm_cfg.lm_tokenizer)\n",
    "\n",
    "    # Load and combine all training datasets\n",
    "    combined_train_data = []\n",
    "    for dataset_name in train_cfg.train_dataset_name:\n",
    "        train_ds = load_dataset(train_cfg.train_dataset_path, dataset_name)\n",
    "        combined_train_data.append(train_ds['train'])\n",
    "    train_ds = concatenate_datasets(combined_train_data)\n",
    "    \n",
    "    test_ds = load_dataset(train_cfg.test_dataset_path)\n",
    "    train_ds = train_ds.shuffle(seed=0) # Shuffle the training dataset, so train and val get equal contributions from all concatinated datasets\n",
    "\n",
    "    # Apply cutoff if specified\n",
    "    if train_cfg.data_cutoff_idx is None:\n",
    "        total_samples = len(train_ds)  # Use the entire dataset\n",
    "    else:\n",
    "        total_samples = min(len(train_ds), train_cfg.data_cutoff_idx)\n",
    "\n",
    "    val_size = int(total_samples * train_cfg.val_ratio)\n",
    "    train_size = total_samples - val_size\n",
    "\n",
    "    train_dataset = VQADataset(train_ds.select(range(train_size)), tokenizer, image_processor)\n",
    "    val_dataset = VQADataset(train_ds.select(range(train_size, total_samples)), tokenizer, image_processor)\n",
    "    test_dataset = MMStarDataset(test_ds['val'], tokenizer, image_processor)\n",
    "\n",
    "    # Create collators\n",
    "    vqa_collator = VQACollator(tokenizer, vlm_cfg.lm_max_length)\n",
    "    mmstar_collator = MMStarCollator(tokenizer)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=train_cfg.batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=vqa_collator,\n",
    "        num_workers=10,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "        prefetch_factor=10,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=train_cfg.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=vqa_collator,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "        prefetch_factor=10,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=train_cfg.mmstar_batch_size, \n",
    "        shuffle=False, \n",
    "        collate_fn=mmstar_collator,\n",
    "        pin_memory=True,\n",
    "        )\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bf416bd-d3fe-4fb6-a3f4-9c367e67e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mmstar(model, tokenizer, test_loader, device):\n",
    "    # Go through MMStar and count how many answers we get right\n",
    "    model.eval()\n",
    "    total_examples = 0\n",
    "    correct_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            image = batch['images'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            correct_answer = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            gen = model.generate(input_ids, image, attention_mask)\n",
    "            model_output = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "\n",
    "            is_correct = utils.check_multiple_choice_with_regex(model_output, correct_answer)\n",
    "\n",
    "            total_examples += len(is_correct)\n",
    "            if is_correct:\n",
    "                correct_predictions += sum(is_correct)\n",
    "\n",
    "    accuracy = correct_predictions / total_examples if total_examples > 0 else 0\n",
    "    model.train()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4f44395-e3c1-4778-a35c-ce8e33975392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(it, max_lr, max_steps):\n",
    "    min_lr = max_lr * 0.1\n",
    "    warmup_steps = max_steps * 0.03\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_steps:\n",
    "        return max_lr * (it+1) / warmup_steps\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > max_steps:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
    "    return min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "def train(train_cfg, vlm_cfg):\n",
    "    train_loader, val_loader, test_loader = get_dataloaders(train_cfg, vlm_cfg)\n",
    "    tokenizer = get_tokenizer(vlm_cfg.lm_tokenizer)\n",
    "\n",
    "    # Initialize model\n",
    "    if train_cfg.resume_from_vlm_checkpoint:\n",
    "        print(\"Resuming from checkpoint\")\n",
    "        model = VisionLanguageModel.from_pretrained(vlm_cfg.vlm_checkpoint_path)\n",
    "    else:\n",
    "        model = VisionLanguageModel(vlm_cfg)\n",
    "\n",
    "    print(f\"nanoVLM initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "    print(f\"Training summary: {len(train_loader.dataset)} samples, {len(train_loader)} batches/epoch, batch size {train_cfg.batch_size}\")\n",
    "\n",
    "    # Define optimizer groups\n",
    "    param_groups = [{'params': model.MP.parameters(), 'lr': train_cfg.lr_mp},\n",
    "                    {'params': list(model.decoder.parameters()) + list(model.vision_encoder.parameters()), 'lr': train_cfg.lr_backbones}]\n",
    "    optimizer = optim.AdamW(param_groups)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    if train_cfg.compile:\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    epoch_times = []\n",
    "    batch_losses = []\n",
    "    val_losses = []\n",
    "    val_plot_steps = []\n",
    "    best_accuracy = 0\n",
    "    global_step = 0\n",
    "    for epoch in range(train_cfg.epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        total_tokens_processed = 0\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "            batch_start_time = time.time()\n",
    "            images = batch[\"image\"].to(device)\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16): # Mixed precision training\n",
    "                _, loss = model(input_ids, images, attention_mask=attention_mask, targets=labels)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            adj_lr_mp = get_lr(global_step, train_cfg.lr_mp, len(train_loader) * train_cfg.epochs)\n",
    "            adj_lr_backbones = get_lr(global_step, train_cfg.lr_backbones, len(train_loader) * train_cfg.epochs)\n",
    "            optimizer.param_groups[0]['lr'] = adj_lr_mp\n",
    "            optimizer.param_groups[1]['lr'] = adj_lr_backbones\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_loss = loss.item()\n",
    "            total_train_loss += batch_loss\n",
    "            batch_losses.append(batch_loss)\n",
    "\n",
    "            num_tokens = torch.sum(attention_mask).item() # Sum of attention mask gives number of tokens\n",
    "            num_tokens += images.shape[0] * ((images.shape[2] / vlm_cfg.vit_patch_size) ** 2) / (vlm_cfg.mp_pixel_shuffle_factor ** 2) # Add image tokens = batch_size * (((img_size / patch_size) ** 2) / (pixel_shuffle_factor ** 2))\n",
    "            total_tokens_processed += num_tokens\n",
    "\n",
    "            batch_end_time = time.time()\n",
    "            batch_duration = batch_end_time - batch_start_time\n",
    "            tokens_per_second = num_tokens / batch_duration\n",
    "\n",
    "            if global_step % 300 == 0:\n",
    "                model.eval()\n",
    "                torch.cuda.empty_cache()  # Clear GPU memory\n",
    "                with torch.no_grad():\n",
    "                    total_val_loss = 0\n",
    "                    for batch in val_loader:\n",
    "                        images = batch[\"image\"].to(device)\n",
    "                        input_ids = batch[\"input_ids\"].to(device)\n",
    "                        labels = batch[\"labels\"].to(device)\n",
    "                        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "                        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                            _, loss = model(input_ids, images, attention_mask=attention_mask, targets=labels)\n",
    "\n",
    "                        total_val_loss += loss.item()\n",
    "                    avg_val_loss = total_val_loss / len(val_loader)\n",
    "                    val_losses.append(avg_val_loss)\n",
    "                    val_plot_steps.append(global_step)\n",
    "                epoch_accuracy = 0\n",
    "                if train_cfg.eval_in_epochs:\n",
    "                    epoch_accuracy = test_mmstar(model, tokenizer, test_loader, device)\n",
    "                    if epoch_accuracy > best_accuracy:\n",
    "                      best_accuracy = epoch_accuracy\n",
    "                      model.save_pretrained(save_directory=vlm_cfg.vlm_checkpoint_path)\n",
    "                    print(f\"\\nStep: {global_step}, Loss: {batch_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Tokens/s: {tokens_per_second:.2f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "                model.train()\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_duration = epoch_end_time - epoch_start_time\n",
    "        epoch_times.append(epoch_duration)\n",
    "\n",
    "        epoch_tokens_per_second = total_tokens_processed / epoch_duration\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{train_cfg.epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Time: {epoch_duration:.2f}s | T/s: {epoch_tokens_per_second:.2f}\")\n",
    "\n",
    "    # Summary Statistics\n",
    "    if not train_cfg.eval_in_epochs:\n",
    "      model.save_pretrained(save_directory=vlm_cfg.vlm_checkpoint_path)\n",
    "      # model.push_to_hub(hf_model_name)\n",
    "\n",
    "\n",
    "    avg_epoch_time = sum(epoch_times) / len(epoch_times)\n",
    "    total_training_time = sum(epoch_times)\n",
    "    total_samples_processed = len(train_loader.dataset) * train_cfg.epochs\n",
    "    avg_time_per_sample = total_training_time / total_samples_processed\n",
    "    print(f\"Average time per epoch: {avg_epoch_time:.2f}s\")\n",
    "    print(f\"Average time per sample: {avg_time_per_sample:.4f}s\")\n",
    "\n",
    "    plt.plot(batch_losses, label='Train Loss')\n",
    "    plt.plot(val_plot_steps, val_losses, label='Val Loss')\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # With this code you can test the accuracy of the model on the MMStar dataset\n",
    "    # But if you only train with few samples, the accuracy will be very low\n",
    "    # print(\"Testing MMStar Accuracy:\")\n",
    "    # accuracy = test_mmstar(model, tokenizer, test_loader, device)\n",
    "    # print(f\"MMStar Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e47e93cc-3dbd-4ae7-b284-f67518d03131",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class VLMConfig:\n",
    "    vit_hidden_dim: int = 768\n",
    "    vit_inter_dim: int = 4 * vit_hidden_dim\n",
    "    vit_patch_size: int = 16\n",
    "    vit_img_size: int = 224\n",
    "    vit_n_heads: int = 12\n",
    "    vit_dropout: float = 0.0\n",
    "    vit_n_blocks: int = 12\n",
    "    vit_ln_eps: float = 1e-6\n",
    "    vit_cls_flag: bool = False\n",
    "    vit_model_type: str = 'google/siglip-base-patch16-224'\n",
    "\n",
    "    lm_hidden_dim: int = 576\n",
    "    lm_inter_dim: int = 1536\n",
    "    lm_rms_eps: float = 1e-5\n",
    "    lm_re_base: int = 100000\n",
    "    lm_max_position_embeddings: int = 8192\n",
    "    lm_vocab_size: int = 49152\n",
    "    lm_n_heads: int = 9\n",
    "    lm_n_kv_heads: int = 3\n",
    "    lm_dropout: float = 0.0\n",
    "    lm_n_blocks: int = 30\n",
    "    lm_attn_scaling: float = 1.0\n",
    "    IMAGE_TOKEN_LENGTH: int = 49\n",
    "    TOTAL_SEQUENCE_LENGTH: int = 128\n",
    "    lm_max_length: int = TOTAL_SEQUENCE_LENGTH - IMAGE_TOKEN_LENGTH  # Maximum length for the language model, derived from TOTAL_SEQUENCE_LENGTH and IMAGE_TOKEN_LENGTH\n",
    "    lm_use_tokens: bool = False # Decide if the LM expects tokens or embeddings as input (if using as a backbone for the VLM, set to False)\n",
    "    lm_tie_weights: bool = True # Decide if you want to tie the LM Head weight to the token embedding weights\n",
    "    lm_model_type: str = 'HuggingFaceTB/SmolLM2-135M'\n",
    "    lm_tokenizer: str = 'HuggingFaceTB/cosmo2-tokenizer'\n",
    "    lm_eos_token_id: int = 0\n",
    "\n",
    "    mp_pixel_shuffle_factor: int = 2\n",
    "\n",
    "    vlm_load_backbone_weights: bool = True\n",
    "    vlm_checkpoint_path: str = 'checkpoints/nanoVLM-222M_cocoqa'\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    lr_mp: float = 1e-3\n",
    "    lr_backbones: float = 5e-5\n",
    "    val_ratio: float = 0.2\n",
    "    compile: bool = False\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    data_cutoff_idx: int = 50000 # Let's only use a small subset of the data at first, otherwise it takes very long to see anything :D\n",
    "    batch_size: int = 30\n",
    "    mmstar_batch_size: int = 30\n",
    "    epochs: int = 5\n",
    "    eval_in_epochs: bool = False # Deactivating this in colab, because it would evaluate 1500 samples of MMStar every time otherwise\n",
    "    resume_from_vlm_checkpoint: bool = True # Indicate if the training should be resumed from a checkpoint of the whole VLM or you want to start from scratch\n",
    "    train_dataset_path: str = 'HuggingFaceM4/the_cauldron'\n",
    "    train_dataset_name: tuple[str, ...] = (\"cocoqa\", ) #(\"ai2d\", \"aokvqa\", \"chart2text\", \"chartqa\", \"clevr\", \"cocoqa\", \"datikz\", \"diagram_image_to_text\", \"docvqa\", \"dvqa\", \"figureqa\", \"finqa\", \"geomverse\", \"hateful_memes\", \"hitab\", \"iam\", \"iconqa\", \"infographic_vqa\", \"intergps\", \"localized_narratives\", \"mapqa\", \"multihiertt\", \"ocrvqa\", \"plotqa\", \"raven\", \"rendered_text\", \"robut_sqa\", \"robut_wikisql\", \"robut_wtq\", \"scienceqa\", \"screen2words\", \"st_vqa\", \"tabmwp\", \"tallyqa\", \"tat_qa\", \"textcaps\", \"textvqa\", \"tqa\", \"vistext\", \"visual7w\", \"visualmrc\", \"vqarad\", \"vqav2\", \"vsr\", \"websight\") # \"clevr_math\", \"okvqa\", \"spot_the_diff\", \"nlvr2\", \"mimic_cgd\",\n",
    "    test_dataset_path: str = \"Lin-Chen/MMStar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfc409c5-1ceb-46b9-b033-b3ed1ccf8734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint\n",
      "nanoVLM initialized with 222,081,600 parameters\n",
      "Training summary: 37030 samples, 1234 batches/epoch, batch size 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1234 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 128])\n",
      "torch.Size([30, 128])\n",
      "torch.Size([30, 128])\n",
      "torch.Size([30, 128])\n",
      "torch.Size([30, 128])\n",
      "torch.Size([30, 128])\n",
      "torch.Size([30, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-8 (_pin_memory_loop):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 53, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py\", line 30, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/reductions.py\", line 495, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/connection.py\", line 508, in Client\n",
      "    answer_challenge(c, authkey)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/connection.py\", line 757, in answer_challenge\n",
      "    response = connection.recv_bytes(256)        # reject large message\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "  0%|          | 0/1234 [00:12<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m vlm_cfg \u001b[38;5;241m=\u001b[39m VLMConfig()\n\u001b[1;32m      2\u001b[0m train_cfg \u001b[38;5;241m=\u001b[39m TrainConfig()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvlm_cfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 97\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_cfg, vlm_cfg)\u001b[0m\n\u001b[1;32m     94\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16):\n\u001b[0;32m---> 97\u001b[0m         _, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     total_val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    100\u001b[0m avg_val_loss \u001b[38;5;241m=\u001b[39m total_val_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(val_loader)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 33\u001b[0m, in \u001b[0;36mVisionLanguageModel.forward\u001b[0;34m(self, input_ids, image, attention_mask, targets)\u001b[0m\n\u001b[1;32m     31\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((image_attention_mask, attention_mask), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(attention_mask\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 33\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_embd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Not logits yet, but easier to return like this\u001b[39;00m\n\u001b[1;32m     35\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Only use the token part of the logits for loss computation\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 246\u001b[0m, in \u001b[0;36mLanguageModel.forward\u001b[0;34m(self, x, attention_mask)\u001b[0m\n\u001b[1;32m    243\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_embd(position_ids) \u001b[38;5;66;03m# Get rotary position embeddings\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 246\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_use_tokens:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 194\u001b[0m, in \u001b[0;36mLanguageModelBlock.forward\u001b[0;34m(self, x, cos, sin, attention_mask)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, cos, sin, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    193\u001b[0m     res \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m--> 194\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(x, cos, sin, attention_mask)\n\u001b[1;32m    196\u001b[0m     x \u001b[38;5;241m=\u001b[39m res \u001b[38;5;241m+\u001b[39m x\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m, in \u001b[0;36mRMSNorm.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 15\u001b[0m     irms \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(torch\u001b[38;5;241m.\u001b[39mmean(\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps) \u001b[38;5;66;03m# inverse of RMS\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m irms \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:40\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vlm_cfg = VLMConfig()\n",
    "train_cfg = TrainConfig()\n",
    "train(train_cfg, vlm_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86ed877-8bae-4c08-901a-10e8d55e7ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9e095c9-9fbd-4aed-b990-8fe80580b018",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionLanguageModel(\n",
       "  (vision_encoder): ViT(\n",
       "    (patch_embedding): ViTPatchEmbeddings(\n",
       "      (conv): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x ViTBlock(\n",
       "        (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): ViTMultiHeadAttention(\n",
       "          (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): ViTMLP(\n",
       "          (activation_fn): GELU(approximate='tanh')\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): LanguageModel(\n",
       "    (token_embedding): Embedding(49152, 576)\n",
       "    (rotary_embd): RotaryEmbedding()\n",
       "    (blocks): ModuleList(\n",
       "      (0-29): 30 x LanguageModelBlock(\n",
       "        (mlp): LanguageModelMLP(\n",
       "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
       "        )\n",
       "        (attn): LanguageModelGroupedQueryAttention(\n",
       "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (out_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm1): RMSNorm()\n",
       "        (norm2): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "    (head): Linear(in_features=576, out_features=49152, bias=False)\n",
       "  )\n",
       "  (MP): ModalityProjector(\n",
       "    (proj): Linear(in_features=3072, out_features=576, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VisionLanguageModel.from_pretrained(\"checkpoints/nanoVLM-222M_cocoqa\").to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53b0ef24-3f81-4329-9e48-4eeceae5a481",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(model.cfg.lm_tokenizer)\n",
    "image_processor = get_image_processor(model.cfg.vit_img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a3d5cce-36f3-496c-a4f9-fa8837b18b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What's on the image?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21258eb5-1ba7-466c-8501-6aa9f8832523",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = f\"Question: {prompt} Answer:\"\n",
    "encoded = tokenizer.batch_encode_plus([template], return_tensors=\"pt\")\n",
    "tokens = encoded[\"input_ids\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3015995d-170b-4540-914b-dc0d4732fb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"../berner.jpg\").convert(\"RGB\")\n",
    "img_t = image_processor(img).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a4715da-58ed-4ac1-a81a-2774fd41cfed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input:\n",
      "  What's on the image? \n",
      "\n",
      "Outputs:\n",
      "  >> Generation 1: Banana.\n",
      "\n",
      "Sign up\n",
      "Here's how it works:\n",
      "  1. Any Amendments?).\n",
      "  2. Your Expedited Hellicarpy.Cake.Cake.Cake.Cake.Cake.Cake.Cake.Cake.Cake.Cake.Cake.Cake.Cake.Cake.Cake.Cake.Cake.Cake.Cake.Cake.Cake.Cake.C\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nInput:\\n \", prompt, \"\\n\\nOutputs:\")\n",
    "for i in range(1):\n",
    "    gen = model.generate(tokens, img_t, max_new_tokens=100)\n",
    "    out = tokenizer.batch_decode(gen, skip_special_tokens=True)[0]\n",
    "    print(f\"  >> Generation {i+1}: {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ccc117-903f-40a8-9019-93873d093ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861c4573-2dd2-405a-9903-f2e5bd17aa7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
