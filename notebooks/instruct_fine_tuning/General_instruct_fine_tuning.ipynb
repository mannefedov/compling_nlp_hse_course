{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8201db42",
   "metadata": {},
   "source": [
    "# General fine-tuning of text generation models through instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3925e6",
   "metadata": {},
   "source": [
    "В предыдущих семинарах мы в основном фокусировались на какой-то одной задаче - исправление опечаток, выделение сущностей, определение тональности, машинный перевод и т.п. Мы двигались от самых простых моделей к предобученным трансформерам, но подход в целом не менялся - мы брали модель и тренировали её решать нужную задачу, а когда нужно было решить новую задачу мы откладывали старую модель и тренировали новую. \n",
    "\n",
    "Один из последних быстро развивающихся трендов в NLP - решать множество задач 1 общей моделью. Если задуматься, то все к этому шло:\n",
    "\n",
    "1) Self-supervised предобучение научились более менее стабильно масштабировать на огромный текстовые корпуса, что дало нам модели, в которых уже заложено очень широкое понимание языка\n",
    "2) Fine-tuning предобученных моделей под конкретную задачу требует небольшое количество примеров (сотни или даже десятки\n",
    "3) При правильных промтах предобученный модели могли решать даже задачи, которые они никогда не видели (zero-shot и in context learning)\n",
    "4) Все возможные NLP задачи стали сводится к генерации текста (классификация - генерация одного токена, исправление опечаток - генерация исправленной последовательности, выделение сущности - генерация именованых сущностей нужного типа, даже генерация кода теперь решается просто генерацией)\n",
    "\n",
    "\n",
    "Поэтому было вопросом времени, когда кто-то попробует обучить модель решать сразу все нужные задачи и у них получится.\n",
    "\n",
    "Текущее топовое решение - дообучить модель на датесете разнообразных инструкций, которые соответствуют нужным задачам, а затем дотренировать новую модель с помощью Reinforcement Learning from Human Feedback (RLHF). Такой подход первым успешно применил OpenAI и результат можно наблюдать в ChatGPT (спойлер: результат очень хороший). Но OpenAI не опубликовал в открытом доступе ни модели, ни код ни какие-то технические описания их подхода. Поэтому сейчас болшАя часть исследовательского сообщества в NLP занимается тем, что пытается воспроизвести chatgpt по общем описаниям, которые раскрыл OpenAI. И очень многое уже получилось воспроизвести и буквально с каждым днем такие модели становятся меньше/дешевле и доступнее.\n",
    "\n",
    "В этом семинаре мы попробуем дообучить модель на датасете инструкций. Но прежде чем переходить к этому, давайте посмотрим на две статьи (и модели), которые есть в открытом доступе и которые сильно повлияли на движение в сторону общих моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6e77773",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %pip install pandas transformers tokenizers datasets xformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110460fd",
   "metadata": {},
   "source": [
    "## T5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6796cb",
   "metadata": {},
   "source": [
    "![](https://1.bp.blogspot.com/-o4oiOExxq1s/Xk26XPC3haI/AAAAAAAAFU8/NBlvOWB84L0PTYy9TzZBaLf6fwPGJTR0QCLcBGAsYHQ/s640/image3.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd9deeb",
   "metadata": {},
   "source": [
    "Первая статья Т5 (Text-To-Text Transfer Transformer, https://arxiv.org/abs/1910.10683, Google Research, конец 2019 года) \n",
    "Это очень большая статья, в которой подробно исследовалась унификация различных NLP задач в задачу генерации. А также они попробовали много различных подходов к предобучению (в то время выходило очень много статей, которые как-то меняли self-supervised задачу и они попробовали много разных комбинаций, чтобы получить хорошую модель). В результате у них получилось несколько вариантов модели Т5 и всех их они выложили в открытый доступ. \n",
    "Еще в статье они пробовали тюнить модель под разные задачи, но по большей части все еще по отдельности. Если вы прочитаете статью или хотя бы описание fine-tuning экспериментов, то увидите, что на тот момент парадигма (1 модель - 1 задача) еще не изменилась. В своих экспериментах они пробовали тренироваться сразу под несколько задач, но у них было не достаточно много разнообразных задач и в итоге общая модель работала хуже на отдельных задачах, чем специфичные модели.\n",
    "\n",
    "Но в открытый доступ они выложили в том числе и модели, которые были дообучены на нескольких задачах. Задача в модель передается через префикс (посмотрите на начало примеров выше). Эти модели есть на huggingface, давайте попробуем взять какую-то модель и попробовать сходу решить задачу саммаризации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "084c3271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c4ca0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c6f5484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = 't5-large'\n",
    "MODEL_NAME = 't5-base'\n",
    "# MODEL_NAME = 't5-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "196314f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31e3501cd1af4b64bfb802e5ecd5f985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d71d3bc446e4890bac7efb9579c478e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7183beaa5134240a0e50e320cf8cbdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cbba1e6882e49c69e7d581c8ca2ed17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c9184263d2438cbc886b9a44dcb974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=512)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2c8d7f",
   "metadata": {},
   "source": [
    "Возьмем какой-нибудь текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b7a671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prefix = \"summarize: {}\"\n",
    "\n",
    "text = \"\"\"\n",
    "Badgers burrowing under rail tracks have halted trains in the northern and southern Netherlands, forcing lengthy cancellations on at least two lines.\n",
    "All trains were halted Tuesday afternoon on a busy line between the southern cities of Den Bosch and Boxtel after the animals dug into a dike carrying rails. The national railway company said the line would be out of service for at least a week.\n",
    "The digging means \"the rails can subside and then the safety of train traffic can no longer be guaranteed,\" ProRail, the company that maintains the Dutch rail network said in a statement.\n",
    "Earlier this month, badgers also burrowed under tracks near the northern village of Molkwerum in Friesland province, knocking a line out of service until next month while workers seek permission to shift the animals.\n",
    "Badgers are protected animals in the Netherlands, so rail operators have to get permission to move them or disturb their habitat before repairs can begin.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1206e75",
   "metadata": {},
   "source": [
    "С моделями в huggingface удобнее всего работать через torch, но это не страшно, так как все основные вещи реализованы в transformers и они одинаковые для torch и tf. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5802f8ed",
   "metadata": {},
   "source": [
    "Попробуем сгенерировать саммари"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8902a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([task_prefix.format(text)], \n",
    "                    return_tensors=\"pt\", padding=True)\n",
    "\n",
    "output_sequences = model.generate(\n",
    "    # this parameters are also important but you can read about them in the docs and just try changing them\n",
    "    num_beams=5,\n",
    "    max_length=100,\n",
    "    no_repeat_ngram_size=3, \n",
    "#     repetition_penalty= 5.0,\n",
    "#     length_penalty=0.01,\n",
    "#     early_stopping=True,\n",
    "#     do_sample=True, \n",
    "#     top_k=30, \n",
    "#     top_p=0.8, \n",
    "    early_stopping=True,\n",
    "#     num_return_sequences=3,\n",
    "    num_return_sequences= 1,\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    do_sample=False,  # disable sampling to test if batching affects output\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9efe30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a754287d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all trains halted on a busy line between den Bosch and boxtel. badgers dug into a dike carrying rails. the national railway company says the line will be out of service for at least a week.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579e7d80",
   "metadata": {},
   "source": [
    "Работает неплохо, но конечно для реального практическо применения нужно тюнить модель дополнительно"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfaf9e4",
   "metadata": {},
   "source": [
    "## FLAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628438c6",
   "metadata": {},
   "source": [
    "![](https://1.bp.blogspot.com/-_kPdaMrcRWI/YV2b-XFoRxI/AAAAAAAAIMw/KDjg0IfuoK8hjpSXNODoV46D8Rb5rK8hgCLcBGAsYHQ/w640-h178/image3.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dac926d",
   "metadata": {},
   "source": [
    "Второя статья - FLAN (тоже от Google Research, тоже огромная, Finetuned Language Models Are Zero-Shot Learners, https://arxiv.org/abs/2109.01652, середина 2021 года)\n",
    "\n",
    "В этой статье уже заметен сдвиг в сторону общих моделей и уже сформировался подход к такому обучению через инструкции. Основная идея в статье - переделать различные NLP датасеты в большой датасет разнообразных инструкций (они сделали различные темплейты на правилах и прогнали их через размеченные датасеты) и обучить модель решать сразу всё. Инструкции при этом это не какие-то технические теги как в T5, а нормальные человеческие инструкции (буквально что-то вроде \"Translate this text from English to Russian\", \"Write five topics that describe this text\", \"What is the sentiment of this text? Options: Negative, Positive, Neutral.\"). При таком подходе они заметили, что модель начинает обобщаться на инструкции, которых она никогда не видела - так как модель предобучена на большом количестве текстов, она уже хорошо понимает язык и экстраполирует инструкции из обучающей выборки, используя свое понимание языка). И чем больше таких инструкций, тем лучше получалось.\n",
    "\n",
    "Они попробовали такой подход с разными моделями (T5, PALM) и везде получалось хорошо решать новые задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d37453a",
   "metadata": {},
   "source": [
    "FLAN варианты моделей также доступны на huggingface. Давайте попробуем с таким же текстом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73456752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d0f37f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e1cb834",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'google/flan-t5-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a1cf26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399ef18e5af640b38ac993edd7af6ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd5c52d0a6b4631a752215f7e905967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f35084d1ae4ea289256d7e4a01326b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a3338104074b468fa07e432cbd1826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c66325fff6341ca9c40500180826d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9857f1740d74c878feec8ef3c532ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b602c50e6324f1a815ee505d0e0353f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=512)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115209a6",
   "metadata": {},
   "source": [
    "Инструкции модели можно передавать в свободном формате, поэтому сделаем функцию, чтобы удобнее было пробовать разные инструкции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddc9bbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_instruction(instruction, text, model):\n",
    "    \n",
    "\n",
    "    inputs = tokenizer([instruction.format(text)], \n",
    "                        return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    output_sequences = model.generate(\n",
    "        # this parameters are also important but you can read about them in the docs and just try changing them\n",
    "        num_beams=5,\n",
    "        max_length=100,\n",
    "        no_repeat_ngram_size=3, \n",
    "    #     repetition_penalty= 5.0,\n",
    "    #     length_penalty=0.01,\n",
    "    #     early_stopping=True,\n",
    "    #     do_sample=True, \n",
    "    #     top_k=30, \n",
    "    #     top_p=0.8, \n",
    "        early_stopping=True,\n",
    "    #     num_return_sequences=3,\n",
    "        num_return_sequences= 1,\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        do_sample=False,  # disable sampling to test if batching affects output\n",
    "    )\n",
    "    summaries = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n",
    "    return summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f797fd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Badgers burrowing under rail tracks have halted trains in the northern and southern Netherlands, forcing lengthy cancellations on at least two lines.\n",
    "All trains were halted Tuesday afternoon on a busy line between the southern cities of Den Bosch and Boxtel after the animals dug into a dike carrying rails. The national railway company said the line would be out of service for at least a week.\n",
    "The digging means \"the rails can subside and then the safety of train traffic can no longer be guaranteed,\" ProRail, the company that maintains the Dutch rail network said in a statement.\n",
    "Earlier this month, badgers also burrowed under tracks near the northern village of Molkwerum in Friesland province, knocking a line out of service until next month while workers seek permission to shift the animals.\n",
    "Badgers are protected animals in the Netherlands, so rail operators have to get permission to move them or disturb their habitat before repairs can begin.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41914df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Badgers burrowing under rail tracks in the Netherlands have halted trains for at least a week.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"Give a summary of this text: {}\"\n",
    "predict_for_instruction(instruction, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c9f6ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Badgers burrowed under rail tracks in northern and southern Netherlands, forcing lengthy cancellations on at least two lines'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"Give a very short summary of this text: {}\"\n",
    "predict_for_instruction(instruction, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c04bcabb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Badgers halted in northern and southern Netherlands'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"Write a title for the following text:{}\"\n",
    "predict_for_instruction(instruction, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9cc58cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'animal, stop, train'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"Suggest keywords for this text. Text: {}\"\n",
    "predict_for_instruction(instruction, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79cc573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82835cd2",
   "metadata": {},
   "source": [
    "## InstructGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98475c81",
   "metadata": {},
   "source": [
    "FLAN модели работали хорошо, но все еще недостаточно. OpenAI довел их до состояния, когда их можно использовать на практике. Они публиковали несколько статей и описаний своих экспериментов:\n",
    "\n",
    "https://openai.com/research/improving-language-model-behavior\n",
    "https://openai.com/research/instruction-following\n",
    "https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf\n",
    "https://openai.com/research/learning-to-summarize-with-human-feedback\n",
    "\n",
    "Они добавили еще одну важную часть - RLHF. Про нее мы попытаемся поговорить на следующем занятии. Пока сфокусируемся на инструкциях. Из описания OpenAI видно, что их подход очень похож на FLAN, но они машстабировали его и использовали для своих датасетов инструкции на основе реальных запросов к их API. И они продолжают это делать, исправляя все больше ошибок и нежелательных ответов. \n",
    "Также они сильно ускорились, когда добавили интерфейс (ChatGPT). Они даже говорили, что уже очень хорошая модель была доступна в их API около полугода и никто особо не обращал внимания на нее, хотя она уже работала как ChatGPT, но ей нужно было подавать правильный промпт. Когда они решили это через интерфейс (и промпт на бекенде), количество пользователей сильно увиличилось и к ним потекло очень много реальных запросов, на которых они быстро стали дообучаться."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75678946",
   "metadata": {},
   "source": [
    "Открытых моделей тут нет, поэтому перейдем к следующему шагу."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57df25c3",
   "metadata": {},
   "source": [
    "## Alpaca "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38542bc2",
   "metadata": {},
   "source": [
    "Разнообразние и естественность инструкция влияет на качество модели, но создавать такие датасеты сложно и дорого, а корпорации не делятся. Поэтому многие работы в обучении на инструкциях посвящены способам сгенерировать синтетические, но как можно более реалистичные датасеты. Значимая работа в этом направлении - Stanford Alpaca \n",
    "![](https://crfm.stanford.edu/static/img/posts/2023-03-13-alpaca/alpaca_main.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f850c45",
   "metadata": {},
   "source": [
    "Код и датасет можно найти тут - https://github.com/tatsu-lab/stanford_alpaca\n",
    "Дальше код взят из train.py и немного изменен"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4619b6d4",
   "metadata": {},
   "source": [
    "Авторы Альпаки дообучили модель LLaMA (7 миллиардов параметров) на датасете инструкций, который они сгенерировали с помощью OpenAI API и получилась модель, которая очень похожа по качеству на саму модель от OpenAI.   \n",
    "\n",
    "LLaMa - это серия предобученных моделей от Meta. Они были опубликованы в 2023 году (LLaMA 1 в феврале, а LLaMA 2 в июле) и Meta утверждает, что по метрикам их меньшие модели сравнимы с GPT-3 (которая около 175 млрд параметров). LLaMA 2 долгое время держалась в топе открытых моделей, где конкуренцию ей составляют Mistral, DBRX, Grok-1.  \n",
    "\n",
    "\n",
    "Незадолго до релиза LLaMA Meta сталкивалась с критикой за свою модель Galactica, которая была предобучена на научных статьях. Сначала они выложили её в открытый доступ, но быстро оказалась, что она может генерировать псевдонаучные и лженаучные тексты и Meta быстро закрыла доступ к этой модели. Поэтому модель LLaMA строго говоря не выложена в открытый доступ и имеет некомерческую лицензию. Чтобы скачать модель, нужно заполнять специальную форму и ждать пока ее одобрят. Но естественно люди, которые получили доступ к модели начали выкладывать ее в открытый доступ - например, кто-то делал ПР в либу Meta, в котором предлагается добавить в Readme.md ссылку на [торент](https://github.com/facebookresearch/llama/pull/73/commits/016a53608c5eae1021e171b9c4f06a9783fc14c0) \n",
    "LLaMA 2 также имеет некомерческую лицензию и требует заявки на лиценцию, однако одобрение лицензии занимает небольшое время, поэтому модель уже воспринимается как полностью открытая.\n",
    "\n",
    "Датасет инструкций Alpaca сгенерировали на основе статьи - https://arxiv.org/abs/2212.10560 И как они говорят у них ушло около 500$ на все, что в тысячи раз дешевле того, что, предполагается, потратил сам OpenAI на свои модели. Но OpenAI запрещают использовать свои модели в таких целях и поэтому итоговую модель Alpaca они пока не выкладывают.\n",
    "\n",
    "Но они выложили в открытый доступ датасет и можно самому попробовать дообучить какую-то открытую предобученную модель. Коммерческая применимость такой модели, однако, все еще под вопросом. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980a1211",
   "metadata": {},
   "source": [
    "Скачаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01e0848c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-21 21:03:20--  https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 22773992 (22M) [text/plain]\n",
      "Saving to: ‘alpaca_data.json’\n",
      "\n",
      "alpaca_data.json    100%[===================>]  21.72M  38.9MB/s    in 0.6s    \n",
      "\n",
      "2023-03-21 21:03:21 (38.9 MB/s) - ‘alpaca_data.json’ saved [22773992/22773992]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3477de2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence\n",
    "import json\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer\n",
    "\n",
    "# import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aeeaca",
   "metadata": {},
   "source": [
    "Посмотрим на датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "279ba6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_alpaca = json.load(open('alpaca_data.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "563f1d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'Give three tips for staying healthy.',\n",
       "  'input': '',\n",
       "  'output': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.'},\n",
       " {'instruction': 'What are the three primary colors?',\n",
       "  'input': '',\n",
       "  'output': 'The three primary colors are red, blue, and yellow.'},\n",
       " {'instruction': 'Describe the structure of an atom.',\n",
       "  'input': '',\n",
       "  'output': 'An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_alpaca[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec2955e",
   "metadata": {},
   "source": [
    "В нем каждый пример это инструкция, опциональный контекст и ответ.\n",
    "Для модели эти примеры еще оборачиваются в специальный промпт, который говорит модели, что она должна следовать инструкциям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1189c2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53688c8",
   "metadata": {},
   "source": [
    "Давайте попробуем дообучить модель от facebook - opt (она открытыя и устроена как LLama и GPT - это декодер онли модель)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3ff2ed",
   "metadata": {},
   "source": [
    "Далее код взят из гитхаба Alpaca и он на торче, но если поизучать его, то будет видно, что тут происходят те же манипуляции, что мы делали раньше (превращение токенов в индексы и паддинг/урезание последовательностей)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15e8004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "    \"\"\"Tokenize a list of strings.\"\"\"\n",
    "    tokenized_list = [\n",
    "        tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        for text in strings\n",
    "    ]\n",
    "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "    ]\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "189e9970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "    sources: Sequence[str],\n",
    "    targets: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
    "    examples = [s + t for s, t in zip(sources, targets)]\n",
    "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
    "    input_ids = examples_tokenized[\"input_ids\"]\n",
    "    labels = copy.deepcopy(input_ids)\n",
    "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "        label[:source_len] = IGNORE_INDEX\n",
    "    return dict(input_ids=input_ids, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a0672d",
   "metadata": {},
   "source": [
    "Далее это оборачивается к классы, которые предобрабатывают данные к формату huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93bf46bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "        list_data_dict = json.load(open(data_path))\n",
    "\n",
    "        logging.warning(\"Formatting inputs...\")\n",
    "        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n",
    "        sources = [\n",
    "            prompt_input.format_map(example) if example.get(\"input\", \"\") != \"\" else prompt_no_input.format_map(example)\n",
    "            for example in list_data_dict\n",
    "        ]\n",
    "        targets = [f\"{example['output']}{tokenizer.eos_token}\" for example in list_data_dict]\n",
    "\n",
    "        logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
    "        data_dict = preprocess(sources, targets, tokenizer)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e2217",
   "metadata": {},
   "source": [
    "Загружаем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "851c73b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'facebook/opt-350m'\n",
    "model_name = \"facebook/opt-125m\"\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        max_length=512,\n",
    "        cache_dir=\"huggingface_cache\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad031e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=\"huggingface_cache\",\n",
    "    model_max_length=512,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7c5e09",
   "metadata": {},
   "source": [
    "Токенизируем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20a617d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=\"alpaca_data.json\")\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8f6954",
   "metadata": {},
   "source": [
    "Задаем параметры обуечения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b738630",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = transformers.TrainingArguments(learning_rate=1e-5, \n",
    "                 num_train_epochs=1,\n",
    "                 per_device_train_batch_size=2,\n",
    "                 gradient_accumulation_steps=1,\n",
    "                 evaluation_strategy='no',\n",
    "                 weight_decay=0.,\n",
    "                 warmup_ratio=0.03,\n",
    "                 lr_scheduler_type=\"cosine\",\n",
    "                 save_strategy='no',\n",
    "                 logging_steps=1000,\n",
    "                 output_dir=\"opt125_instruct_ft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21d9783",
   "metadata": {},
   "source": [
    "И обучаем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f72ae726",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, \n",
    "                 tokenizer=tokenizer, \n",
    "                 args=train_args,\n",
    "                 train_dataset=train_dataset, \n",
    "                 eval_dataset=None, \n",
    "                 data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a77f94d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26001' max='26001' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26001/26001 39:30, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.332500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.230100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.162800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.120400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.136100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.130600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.094200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.068500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.066400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.108300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.032400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.042400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>2.020100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>2.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>1.989600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>2.019800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>1.974300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>2.005900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>1.998000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>1.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>1.977900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>1.970600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=26001, training_loss=2.066676487524158, metrics={'train_runtime': 2371.6488, 'train_samples_per_second': 21.927, 'train_steps_per_second': 10.963, 'total_flos': 3745819289088000.0, 'train_loss': 2.066676487524158, 'epoch': 1.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1884439d",
   "metadata": {},
   "source": [
    "Сохраним модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "142a7410",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('opt125_ft_02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f652d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42bda0c1",
   "metadata": {},
   "source": [
    "И давайте попробуем ее на том же тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "44557cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d9696d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'opt125_ft_02'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "97779964",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=512, max_length=512)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "707d718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_instruction(instruction, text, model):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    prompt = (\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "              \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "              f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{text}\\n\\n### Response:\")\n",
    "\n",
    "    inputs = tokenizer([prompt], \n",
    "                        return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    output_sequences = model.generate(\n",
    "        # this parameters are also important but you can read about them in the docs and just try changing them\n",
    "        num_beams=1,\n",
    "#         temperature=0.4,\n",
    "#         max_length=100,\n",
    "        max_new_tokens=20,\n",
    "#         no_repeat_ngram_size=3,\n",
    "    #     repetition_penalty= 5.0,\n",
    "    #     length_penalty=0.01,\n",
    "    #     early_stopping=True,\n",
    "    #     do_sample=True, \n",
    "    #     top_k=30, \n",
    "    #     top_p=0.8, \n",
    "        early_stopping=True,\n",
    "    #     num_return_sequences=3,\n",
    "        num_return_sequences= 1,\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        do_sample=False,  # disable sampling to test if batching affects output\n",
    "    )\n",
    "    summaries = tokenizer.batch_decode(output_sequences[:,len(inputs[0]):], skip_special_tokens=True)\n",
    "    return summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "52ddc97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Badgers burrowing under rail tracks have halted trains in the northern and southern Netherlands, forcing lengthy cancellations on at least two lines.\n",
    "All trains were halted Tuesday afternoon on a busy line between the southern cities of Den Bosch and Boxtel after the animals dug into a dike carrying rails. The national railway company said the line would be out of service for at least a week.\n",
    "The digging means \"the rails can subside and then the safety of train traffic can no longer be guaranteed,\" ProRail, the company that maintains the Dutch rail network said in a statement.\n",
    "Earlier this month, badgers also burrowed under tracks near the northern village of Molkwerum in Friesland province, knocking a line out of service until next month while workers seek permission to shift the animals.\n",
    "Badgers are protected animals in the Netherlands, so rail operators have to get permission to move them or disturb their habitat before repairs can begin.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "747e36f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Badgers burrowed under the tracks of the northern and southern Netherlands, causing delays on the two'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"Give a summary of this text.\"\n",
    "predict_for_instruction(instruction, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b1ebc5b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Badgers burrowed under the tracks of the northern and southern Netherlands, causing delays on the two'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"Give a very short summary of this text.\"\n",
    "predict_for_instruction(instruction, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "31ad2e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Badgers burrow under rail tracks in northern Netherlands, forcing lengthy cancellations on at least two lines'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"Write a headline for the following text.\"\n",
    "predict_for_instruction(instruction, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e8dfeba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Badgers burrow under rail tracks: halt trains in northern and southern Netherlands, forcing lengthy cancellations'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"Suggest a headline for this text.\"\n",
    "predict_for_instruction(instruction, text, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403dd287",
   "metadata": {},
   "source": [
    "## Dolly\n",
    "\n",
    "Единственный действительно открытый и от руки написанный датасет инструкций - это databricks/databricks-dolly-15k. Этот датасет вручную написали сотрудники компании Databricks. Он выложен в открытый доступ и его разрешено использовать в коммерческих целях. \n",
    "В тот момент казалось, что таких датасетов станет гораздо больше. На основе долли/альпаки быстро файюнтинились открытые предобученные модели и они даже неплохо работали, но почему-то хороших вручную написанных датасетов больше никто не опубликовал. Возможно это просто очень дорого, или же компании поняли, что на этом можно заработать и лучше не делится с конкурентами. Новые модели часто выходят уже с instruct fine-tuned версиями, но данные никто не раскрывает (например, в релизе `databricks/dbrx-instruct` вообще нет даже описания использованного датасета!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d82731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da393086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53724813535a4b219c90e84ddb052c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/8.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e796ded7e424682bb2576ac186dbda7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240a59b92a4d476b8c30b6ff06c1b754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/13.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f45e221e4664e8287c35c89563be7ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9827cecaac9494db9aa4ac8fda1c8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_dolly = load_dataset(\"databricks/databricks-dolly-15k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6a62374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'context', 'response', 'category'],\n",
       "        num_rows: 15011\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dolly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5eda49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': \"From the passage list down the areas for which Dar es Salaam is Tanzania's most prominent city. List the results in comma separated format.\",\n",
       " 'context': \"Dar es Salaam (/ˌdɑːr ɛs səˈlɑːm/; from Arabic: دَار السَّلَام, romanized: Dâr es-Selâm, lit.\\u2009'Abode of Peace') or commonly known as Dar, is the largest city and financial hub of Tanzania. It is also the capital of Dar es Salaam Region. With a population of over six million people, Dar is the largest city in East Africa and the seventh-largest in Africa. Located on the Swahili coast, Dar es Salaam is an important economic centre and is one of the fastest-growing cities in the world.\\n\\nThe town was founded by Majid bin Said, the first Sultan of Zanzibar, in 1865 or 1866. It was the main administrative and commercial center of German East Africa, Tanganyika, and Tanzania. The decision was made in 1974 to move the capital to Dodoma and was officially completed in 1996.\\n\\nDar es Salaam is Tanzania's most prominent city for arts, fashion, media, film, television, and finance. It is the capital of the co-extensive Dar es Salaam Region, one of Tanzania's 31 administrative regions, and consists of five districts: Kinondoni in the north; Ilala in the centre; Ubungo and Temeke in the south; and Kigamboni in the east across the Kurasini estuary.\",\n",
       " 'response': 'arts, fashion, media, film, television, finance',\n",
       " 'category': 'information_extraction'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dolly['train'][12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4c96fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "847ae8b6",
   "metadata": {},
   "source": [
    "## Madlad-400\n",
    "\n",
    "Подход предложенный в T5 не обязательно переносить на общие инструкции. В конце концов нам нужны не только чат-боты общего назначения, но и более специализированные модели. В статье 2023 года [MADLAD-400: A Multilingual And Document-Level\n",
    "Large Audited Dataset](https://arxiv.org/pdf/2309.04662.pdf) предлагается способ обучить многоязычную переводную модель на основе T5 архитектуры. Только вместо ключевых тегов для инструкции здесь используются теги для целевых языков - `<2ru>, <2en>, <2de>...`. Всего поддерживается около 400 языков и очень большое количество языковых пар. За счет обучения на всех данных сразу модель может обобщаться на языковые пары, которые в обучающей выборке могли не встречаться. Стандартный подход для моделей машинного обучения - это обучать по две модели на языковую пару (en-ru и ru-en например). При таком подходе для многих пар не получится обучить модель. Это можно решать через перевод на промежуточный язык (обычно английский) и последующий перевод на целевой язык доступной моделью, но это вносит дополнительный ошибки на счет двойного перевода. В Madlad эта логика реализована как бы внутри модели. У нас может не быть переводов с башкирского на испанский, но модель видела все доступные примеры с этими языками в других парах и этого может хватить для генерации zero-shot перевода. Этому также помогает то, что модель учится на всех языках сразу и возможно выучивает общеязыковые паттерны. \n",
    "\n",
    "Еще такую модель удобно применять, когда в изначальном тексте намешаны слова из different языков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a0f96be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea1bc36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27240143c2114aa0a4319790925beab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/749 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a84c04b30114c5d8ba9f7012383efc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/11.8G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f03185513564216b545f4d1eb80144e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3199db5d013d444886efb8d1cb1810aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/830 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59beac5fd8a2405e8165fe45bdb6f9fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/4.43M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d78fff5228e4432da45de84f2ed15235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/4.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b03dfb46d24bc69c910036c91fa074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fda6dc3d3b74943bbc5a26e9b1a00a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/16.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "model_name = 'jbochi/madlad400-3b-mt'\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22898f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This model is also useful when the original text contains words from different languages.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"<2en> Еще такую модель удобно применять, когда в изначальном тексте намешаны слова из different языков.\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "outputs = model.generate(input_ids=input_ids)\n",
    "\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20af5fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
