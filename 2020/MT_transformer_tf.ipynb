{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0086f61",
   "metadata": {},
   "source": [
    "# Дисклеймер\n",
    "Эту тетрадку нужно запускать в колабе или в vast.ai. Не мучатесь с установкой библиотек и с обучением на cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3b0ea3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "unzip is already the newest version (6.0-21ubuntu1.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 11 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d650e9eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib\n",
      "  Downloading matplotlib-3.3.4-cp36-cp36m-manylinux1_x86_64.whl (11.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.5 MB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-8.2.0-cp36-cp36m-manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 24.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.19.5)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.1-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 22.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.24.2-cp36-cp36m-manylinux2010_x86_64.whl (22.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 22.2 MB 15.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Collecting scipy>=0.19.1\n",
      "  Downloading scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.9 MB 20.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib>=0.11\n",
      "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "\u001b[K     |████████████████████████████████| 303 kB 31.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1316 sha256=6dfdd9da3d3a5b0b5287f5d77208d9eb7ec893a020049a5445fddddd982ff8a1\n",
      "  Stored in directory: /root/.cache/pip/wheels/23/9d/42/5ec745cbbb17517000a53cecc49d6a865450d1f5cb16dc8a9c\n",
      "Successfully built sklearn\n",
      "Installing collected packages: tokenizers, pillow, cycler, kiwisolver, matplotlib, scipy, joblib, threadpoolctl, scikit-learn, sklearn\n",
      "Successfully installed cycler-0.10.0 joblib-1.0.1 kiwisolver-1.3.1 matplotlib-3.3.4 pillow-8.2.0 scikit-learn-0.24.2 scipy-1.5.4 sklearn-0.0 threadpoolctl-2.1.0 tokenizers-0.10.3\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers matplotlib sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac85a206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jedi==0.17.2\n",
      "  Downloading jedi-0.17.2-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting parso<0.8.0,>=0.7.0\n",
      "  Downloading parso-0.7.1-py2.py3-none-any.whl (109 kB)\n",
      "\u001b[K     |████████████████████████████████| 109 kB 11.8 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: parso, jedi\n",
      "  Attempting uninstall: parso\n",
      "    Found existing installation: parso 0.8.2\n",
      "    Uninstalling parso-0.8.2:\n",
      "      Successfully uninstalled parso-0.8.2\n",
      "  Attempting uninstall: jedi\n",
      "    Found existing installation: jedi 0.18.0\n",
      "    Uninstalling jedi-0.18.0:\n",
      "      Successfully uninstalled jedi-0.18.0\n",
      "Successfully installed jedi-0.17.2 parso-0.7.1\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# в vast ai или в последних версия jupyter может не работать автозаполнение, установка этой либы и перезагрука кернела помогает\n",
    "!pip install --upgrade jedi==0.17.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8559a385",
   "metadata": {},
   "source": [
    "# Транформеры для решения seq2seq задач"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb33f601",
   "metadata": {},
   "source": [
    "Seq2seq - наверное самая общая формальная постановка задачи в NLP. Нужно из произвольной последовательности получить какую-то другую последовательность. И в отличие от разметки последовательности (sequence labelling) не требуется, чтобы обе последовательности совпадали по длине. Даже стандартную задачу классификации можно решать как seq2seq - можно рассматривать метку класса как последовательность длинны 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea1d7fd",
   "metadata": {},
   "source": [
    "А трансформеры - sota архитектура для seq2seq задач. Мы не будем подробно разбирать устройство транформеров, если вам интересно вы можете поразбираться вот с этими материалами:\n",
    "\n",
    "Оригинальная статья (сложновато) - https://arxiv.org/pdf/1706.03762.pdf\n",
    "\n",
    "https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/  \n",
    "https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "https://www.youtube.com/watch?v=iDulhoQ2pro\n",
    "\n",
    "https://www.youtube.com/watch?v=TQQlZhbC5ps\n",
    "\n",
    "\n",
    "\n",
    "Трансформеры будут подробно разбираться на курсе глубокого обучения на втором курсе."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19d2aa7",
   "metadata": {},
   "source": [
    "Пока просто попробуем обучать модель на задаче машинного перевода. Для таких задач лучше всего использовать предобученные модели, но если у вас будет какая-то специфичная seq2seq задача, то имеет смысл попробовать обучить трансформер с нуля и в этой тертрадке вам нужно будет поменять только часть с загрузкой данных. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "947b3313",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38911d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# в русскоязычных данных есть \\xa0 вместо пробелов, он может некорректно обрабатываться токенизатором\n",
    "# text = open('opus.en-ru-train.ru.txt').read().replace('\\xa0', ' ')\n",
    "# f = open('opus.en-ru-train.ru.txt', 'w')\n",
    "# f.write(text)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508e8ae0",
   "metadata": {},
   "source": [
    "Данные взяты вот отсюда - https://opus.nlpl.eu/opus-100.php (раздел с отдельными языковыми парами)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e110ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sents = open('opus.en-ru-train.en.txt').read().lower().splitlines()\n",
    "ru_sents = open('opus.en-ru-train.ru.txt').read().lower().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b8fae3",
   "metadata": {},
   "source": [
    "Пример перевода с английского на русский"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eb9b498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('so what are you thinking?', 'ну и что ты думаешь?')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_sents[-1], ru_sents[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bbf529",
   "metadata": {},
   "source": [
    "Как обычно нам нужен токенизатор, а точнее даже 2, т.к. у нас два корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0be060a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer(BPE(), )\n",
    "tokenizer_en.pre_tokenizer = Whitespace()\n",
    "trainer_en = BpeTrainer(special_tokens=[\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[UNK]\", \"[MASK]\"], )\n",
    "tokenizer_en.train(files=[\"opus.en-ru-train.en.txt\"], trainer=trainer_en, )\n",
    "\n",
    "tokenizer_ru = Tokenizer(BPE())\n",
    "tokenizer_ru.pre_tokenizer = Whitespace()\n",
    "trainer_ru = BpeTrainer(special_tokens=[\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[UNK]\", \"[MASK]\"])\n",
    "tokenizer_ru.train(files=[\"opus.en-ru-train.ru.txt\"], trainer=trainer_ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605ea30c",
   "metadata": {},
   "source": [
    "### ВАЖНО!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f07127",
   "metadata": {},
   "source": [
    "Токенизатор - это неотъемлимая часть модели, поэтому не забывайте сохранять токенизатор вместе с моделью. Если вы забудете про это и переобучите токенизатор, то индексы токенов разойдутся и веса модели будут бесполезны. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "496d0ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# раскоментируйте эту ячейку при обучении токенизатора\n",
    "# а потом снова закоментируйте чтобы при перезапуске не перезаписать токенизаторы\n",
    "tokenizer_en.save('tokenizer_en')\n",
    "tokenizer_ru.save('tokenizer_ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4661964",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer.from_file(\"tokenizer_en\")\n",
    "tokenizer_ru = Tokenizer.from_file(\"tokenizer_ru\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0685f9",
   "metadata": {},
   "source": [
    "Переводим текст в индексы вот таким образом. В начало добавляем токен '[CLS]', а в конец '[SEP]'. Если вспомните занятие по языковому моделированию, то там мы добавляли \"\\<start>\" и \"\\<end>\" - cls и sep по сути тоже самое. Вы поймете почему именно cls и sep, а не start и end, если подробнее поразбираетесь с устройством трансформеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc003758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, tokenizer, target=False):\n",
    "    return [tokenizer.token_to_id('[CLS]')] + tokenizer.encode(text).ids + [tokenizer.token_to_id('[SEP]')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff04bf47",
   "metadata": {},
   "source": [
    "Кодируем и паддим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7fc2dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_en = [encode(t, tokenizer_en) for t in en_sents]\n",
    "X_ru = [encode(t, tokenizer_ru, target=True) for t in ru_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "281b5b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_len_en = np.mean([len(x) for x in X_en])\n",
    "# max_len_ru = np.mean([len(x) for x in X_ru])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5984886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_len_en, max_len_ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cc0a376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ограничимся длинной в 20 и 22 (разные чтобы показать что в seq2seq не нужна одинаковая длина)\n",
    "max_len_en, max_len_ru = 20, 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f4f31fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# важно следить чтобы индекс паддинга совпадал в токенизаторе с value в pad_sequences\n",
    "PAD_IDX = tokenizer_ru.token_to_id('[PAD]')\n",
    "PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "49ae735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_en = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "              X_en, maxlen=max_len_en, padding='post')\n",
    "X_ru_out = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "              [x[1:] for x in X_ru], maxlen=max_len_ru-1, padding='post', )\n",
    "\n",
    "X_ru_dec = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "              [x[:-1] for x in X_ru], maxlen=max_len_ru-1, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "824dcb29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000000, 20), (1000000, 22))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# миллион примеров \n",
    "X_en.shape, X_ru.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfa4331",
   "metadata": {},
   "source": [
    "Разделяем на трейн и тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "25fa5f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_en_train, X_en_valid, X_ru_dec_train, X_ru_dec_valid, X_ru_out_train, X_ru_out_valid = train_test_split(X_en, \n",
    "                                                                                                      X_ru_dec, \n",
    "                                                                                                      X_ru_out, \n",
    "                                                                                                      test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9faa719",
   "metadata": {},
   "source": [
    "Дальше код модели, он взят вот отсюда (с небольшими изменениями) - https://www.tensorflow.org/text/tutorials/transformer\n",
    "\n",
    "Там есть комментарии по каждому этапу"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f045ac37",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "656be820",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ENfqAFna_50H"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    \"\"\"Calculate the attention weights. \"\"\"\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    # add the mask to zero out padding tokens\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k)\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f4b51870",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L9eYssGIAG4h"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "            'value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # linear layers\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # split heads\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # scaled dot-product attention\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # concatenation of heads\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))\n",
    "\n",
    "        # final linear layer\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5c48cea2",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "imCQ0jrvWhC7"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, PAD_IDX), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "21c0c899",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HSVdD2zKWaXx"
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e120bbe5",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-9Oibz2es-qW"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "68472627",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5guJOLJmfcuX"
   },
   "outputs": [],
   "source": [
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "89dcc42e",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LRfugon5Wy-Y"
   },
   "outputs": [],
   "source": [
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            max_len,\n",
    "            name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(max_len, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"encoder_layer_{}\".format(i),\n",
    "        )([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a2f90e7c",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6mLvvNMWgDnf"
   },
   "outputs": [],
   "source": [
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "    attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8e9f89b8",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dYRx7YzCW4bu"
   },
   "outputs": [],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            max_len,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(max_len, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e356741b",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TW-v7Fz6XAfC"
   },
   "outputs": [],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                max_len,\n",
    "                name=\"transformer\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "    # mask the future tokens for decoder inputs at the 1st attention block\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask,\n",
    "      output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "    # mask the encoder outputs for the 2nd attention block\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "    enc_outputs = encoder(\n",
    "      vocab_size=vocab_size[0],\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "      max_len=max_len[0],\n",
    "    )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "    dec_outputs = decoder(\n",
    "      vocab_size=vocab_size[1],\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "      max_len=max_len[1],\n",
    "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size[1], name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6c35ce0f",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UInVM9iGAMv1"
   },
   "outputs": [],
   "source": [
    "L  = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none',)\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    loss = L(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, PAD_IDX), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6443d5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "#     from_logits=True, reduction='none')\n",
    "\n",
    "# def loss_function(real, pred):\n",
    "#   mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "#   loss_ = loss_object(real, pred)\n",
    "\n",
    "#   mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "#   loss_ *= mask\n",
    "\n",
    "#   return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "# def accuracy(real, pred):\n",
    "#   accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "\n",
    "#   mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "#   accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "#   accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "#   mask = tf.cast(mask, dtype=tf.float32)\n",
    "#   return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed3067d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "747835a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4743513",
   "metadata": {},
   "source": [
    "Определяем параметры модели. Есть стандартные наборы параметров для обучения маленькой, средней или большой модели. Их можно посмотреть в оригинальной статье про трансформер - https://arxiv.org/pdf/1706.03762.pdf\n",
    "\n",
    "Мы возьмем маленький, т.к. он требует меньше ресурсов. НО для практической задачи есть смысл использовать модель побольше, есть статьи в которые показано, что большие трансформеры обучать эффективнее (требуется меньше примеров) -  https://arxiv.org/pdf/2002.11794.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "542fcc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# small model\n",
    "NUM_LAYERS = 2\n",
    "D_MODEL = 256\n",
    "NUM_HEADS = 8\n",
    "UNITS = 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "\n",
    "# average model\n",
    "# NUM_LAYERS = 6\n",
    "# D_MODEL = 512\n",
    "# NUM_HEADS = 8\n",
    "# UNITS = 2048\n",
    "# DROPOUT = 0.1\n",
    "\n",
    "\n",
    "# Если доступно несколько гпу\n",
    "# расскоментируйте и сдвиньте остальной код ниже на 1 таб\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "with mirrored_strategy.scope():\n",
    "    model = transformer(\n",
    "        vocab_size=(tokenizer_en.get_vocab_size(),tokenizer_ru.get_vocab_size()),\n",
    "        num_layers=NUM_LAYERS,\n",
    "        units=UNITS,\n",
    "        d_model=D_MODEL,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        max_len=[max_len_en, max_len_ru])\n",
    "\n",
    "#     learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        0.001, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "    def accuracy(y_true, y_pred):\n",
    "#         y_true = tf.reshape(y_true, shape=(-1, max_len_ru - 1))\n",
    "        return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint('model_ruen',\n",
    "                                                monitor='val_loss',\n",
    "                                                verbose=1,\n",
    "                                            save_weights_only=True,\n",
    "                                            save_best_only=True,\n",
    "                                            mode='min',\n",
    "                                            save_freq='epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6590325",
   "metadata": {},
   "source": [
    "Трансформеры - большие модели, обучаться они должны долго. Размер батча здесь уже имеет большое значение: сликом маленький батч приведет к сильному увеличению времени на обучение, но сильно большим его поставить не получится, т.к. модель большая и быстро займет всю видеокарту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "308a8e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1188/1188 [==============================] - 135s 113ms/step - loss: 1.5238 - accuracy: 0.2945 - val_loss: 1.5059 - val_accuracy: 0.3018\n",
      "\n",
      "Epoch 00001: val_loss improved from 1.60828 to 1.50594, saving model to model_ruen\n",
      "Epoch 2/100\n",
      "1188/1188 [==============================] - 135s 113ms/step - loss: 1.4287 - accuracy: 0.3055 - val_loss: 1.4594 - val_accuracy: 0.3071\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.50594 to 1.45940, saving model to model_ruen\n",
      "Epoch 3/100\n",
      "1188/1188 [==============================] - 135s 113ms/step - loss: 1.3671 - accuracy: 0.3129 - val_loss: 1.4404 - val_accuracy: 0.3106\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.45940 to 1.44037, saving model to model_ruen\n",
      "Epoch 4/100\n",
      "1188/1188 [==============================] - 135s 113ms/step - loss: 1.3223 - accuracy: 0.3183 - val_loss: 1.4258 - val_accuracy: 0.3130\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.44037 to 1.42577, saving model to model_ruen\n",
      "Epoch 5/100\n",
      " 362/1188 [========>.....................] - ETA: 1:31 - loss: 1.2587 - accuracy: 0.3257"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-187-65b7a4bd9760>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m              \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m              \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m              )\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Обратите внимание на то как данные подаются в модель \n",
    "# помимо текста в модель еще нужно передать целевую последовательность\n",
    "# но не полную а без 1 последнего элемента\n",
    "# а на выходе ожидаем, что модель сгенерирует этот недостающий элемент\n",
    "# мы сравниваем выход из модели с целевой последовательностью уже с этим последним элементом\n",
    "\n",
    "# сдвинутые последовательности создаются выше\n",
    "# X_ru_dec - это переводной текст без последнего элемента\n",
    "# X_ru_out - это переводной текст с последним элементом\n",
    "\n",
    "model.fit((X_en_train, X_ru_dec_train), X_ru_out_train, \n",
    "             validation_data=((X_en_valid, X_ru_dec_valid), X_ru_out_valid),\n",
    "             batch_size=800,\n",
    "             epochs=100,\n",
    "             callbacks=[checkpoint]\n",
    "             )\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61cb6ed",
   "metadata": {},
   "source": [
    "Можно попробовать перевести рандомное предложение и оценить качество модели самому"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "63762869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text):\n",
    "    input_ids = encode(text, tokenizer_en)\n",
    "\n",
    "    input_ids = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                                      [input_ids], maxlen=max_len_en, padding='post')\n",
    "\n",
    "    \n",
    "    \n",
    "    output_ids = [tokenizer_ru.token_to_id('[CLS]') ]\n",
    "    \n",
    "    pred = model((input_ids, tf.cast([output_ids], tf.int32)), training=False)\n",
    " \n",
    "    \n",
    "    while pred.numpy().argmax(2)[0][-1] not in [tokenizer_ru.token_to_id('[SEP]'), \n",
    "                                                            ]:\n",
    "        if len(output_ids) > max_len_ru:\n",
    "            break\n",
    "        output_ids.append(pred.numpy().argmax(2)[0][-1])\n",
    "        pred = model((input_ids, tf.cast([output_ids], tf.int32)), training=False)\n",
    "\n",
    "    return ' '.join([tokenizer_ru.id_to_token(i) for i in output_ids[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "5f16e8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'пред вест ник'"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "7d0ee1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ты можешь перевести это предложение ?'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"can you translate this sentence?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "aa0b57d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'вер хов ный комиссар пере водится это предложение'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Please translate this sentence into Russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "fc8dbbbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'предложение = приговор'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"sentence=sentence\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
